{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config option `kernel_spec_manager_class` not recognized by `EnableNBExtensionApp`.\r\n",
      "Enabling notebook extension jupyter-js-widgets/extension...\r\n",
      "      - Validating: \u001b[32mOK\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 20111980/1000000000 [00:05<04:23, 3711908.92it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m output \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000000000\u001b[39m)):\n\u001b[0;32m----> 4\u001b[0m     \u001b[43moutput\u001b[49m\u001b[38;5;241m.\u001b[39mappend(i)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "output = []\n",
    "for i in tqdm.tqdm(range(1000000000)):\n",
    "    output.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5KSSto1yK2AJ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import re\n",
    "import pickle\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "bz8nJse7KXjB",
    "outputId": "32494629-6529-469d-910e-228eb7c492a5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>আমি জানি আমার এই লেখা,টির জন্য আমাকে অনেক গালম...</td>\n",
       "      <td>বাংলাদেশে কোচিং বানিজ্য বন্ধ এখন সময়ের দাবি</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>একটা ভাষায় তুলনামূলক ভাবে অনেক বেশি মানুষ কথা ...</td>\n",
       "      <td>বাংলা ভাষার প্রযুক্তি নিয়ে আমাদের আরো অনেক বেশ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>আমাদের ফেব্রুয়ারি মাসটি ভাষার মাস। এর বাইরেও ত...</td>\n",
       "      <td>যদি শিশুরা বই পড়ার অভ্যাস করে তাহলে সারা জীবনে...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>আমাকে যদি কেউ কখনো জিজ্ঞেস করে বাংলাদেশের সবচে...</td>\n",
       "      <td>বাংলাদেশে সব স্তরে নারীর ক্ষমতায়নের জন্য আরও অ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>মানুষের মুখ খুব শক্তিশালী এক জিনিস। মানুষ যেটা...</td>\n",
       "      <td>ভালো কথা বল, নয়ত চুপ থাকো</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  আমি জানি আমার এই লেখা,টির জন্য আমাকে অনেক গালম...   \n",
       "1  একটা ভাষায় তুলনামূলক ভাবে অনেক বেশি মানুষ কথা ...   \n",
       "2  আমাদের ফেব্রুয়ারি মাসটি ভাষার মাস। এর বাইরেও ত...   \n",
       "3  আমাকে যদি কেউ কখনো জিজ্ঞেস করে বাংলাদেশের সবচে...   \n",
       "4  মানুষের মুখ খুব শক্তিশালী এক জিনিস। মানুষ যেটা...   \n",
       "\n",
       "                                             Summary  \n",
       "0        বাংলাদেশে কোচিং বানিজ্য বন্ধ এখন সময়ের দাবি  \n",
       "1  বাংলা ভাষার প্রযুক্তি নিয়ে আমাদের আরো অনেক বেশ...  \n",
       "2  যদি শিশুরা বই পড়ার অভ্যাস করে তাহলে সারা জীবনে...  \n",
       "3  বাংলাদেশে সব স্তরে নারীর ক্ষমতায়নের জন্য আরও অ...  \n",
       "4                          ভালো কথা বল, নয়ত চুপ থাকো  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_excel(\"text-summarization.xlsx\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0_ScD3LTK0a_"
   },
   "outputs": [],
   "source": [
    "document = df['Text']\n",
    "summary = df['Summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L0NXAlEKLRwW",
    "outputId": "efaabe5d-7e53-429a-ca52-dffaee7b8642"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"গতরাতে আব্বু তার ফোন নিয়ে আমার কাছে এসে বললো, \"বাবা, ফেইসবুকে কোন একটা পোস্টে কিভাবে প্রাইভেসি সেট করে, একটু দেখায় দেও তো\"আমি তাকে একবার দেখালাম প্রসেসটা ... কিন্তু বয়সের কারণে খালি চোখে আব্বু ডান পাশের ছোট ডট দেখতে পাচ্ছিলো না ... বারবার বলতেছিলো, \"কই? কই ক্লিক করবো?\"প্রায় ৫ বার দেখানোর পরেও যখন আব্বু উল্টাপাল্টা ক্লিক করতেসিলো, প্রচণ্ড মেজাজ খারাপ হইলো ... আমি বেখেয়ালে একটু রাগের সুরে বললাম, \"এই সিম্পল জিনিস পারতেছো না? ধুরর\"এই কথাটা বলার সাথে সাথে আব্বুর মুখটা শুকনা হয়ে গেলো ... আমি সাথে সাথে বুঝতে পারলাম, মানুষটাকে আমি ছোট্ট একটা কথা দিয়ে অনেক বড় একটা কষ্ট দিয়ে ফেলসি ... \\'সরি\\' বলে আবার ভালোভাবে প্রসেসটা বুঝায় বললাম !!আমার ধারণা, নিজের অজান্তেই খুব ছোট ছোট কথা দিয়ে আমরা মানুষকে আঘাত করে ফেলি ... পৃথিবীতে সবাই সবকিছু পারে না ... কারো না পারা নিয়ে তার সাথে অপমানের সুরে কথা বললে সে নিজের ভেতর খুব ছোট বোধ করে ... আমার কোন অধিকার নেই কাউকে অপমান করার, কাউকে ছোট করার !!যে বাবার প্রতি আমি বিরক্ত হলাম কারণ সে সামান্য ফোন চালাতে পারছে না, সেই বাবাই ছোট বেলায় কখনো বিরক্ত হয় নি যখন আমি সামান্য হাঁটতেও পারতাম না ... দিনের পর দিন হাতে ধরে ধরে ক্লান্তিহীনভাবে শিখিয়ে গেছে ... তাকে ৫ বার বা ১০ বার ফোন চালানো শিখাতে আমার বিরক্তি আসবে কেন?নিজেকে অন্য মানুষটার জায়গায় কল্পনা করলে বুঝা যায়, কেউ আমার অপারগতা নিয়ে তাচ্ছিল্যের সুরে কথা বললে কতটা কষ্ট লাগে ... হয়তো কোন একটা ব্যাপার আমার আসলেই পারা উচিত, জানা উচিত, কিন্তু আমি পারি না অথবা জানি না ... এই ব্যর্থতা বা অপারগতার জন্য বোধহয় আমি অপমানিত হওয়া ডিজার্ভ করি না !!এই ক্ষুদ্র জীবনে ইচ্ছাকৃত কিংবা অনিচ্ছাকৃতভাবে আমার বলা কথায় যারা কষ্ট পেয়েছেন, তাদের কাছে আমি ক্ষমাপ্রার্থী ... আমি চেষ্টা করবো কখনোই কাউকে তার অজ্ঞতার জন্য ছোট না করতে !!কোন একটা বিষয় না জানাটা কোন অপরাধ না ... কিন্তু সেটা না জানার জন্য কাউকে তাচ্ছিল্য করাটা অবশ্যই অপরাধ !!আমার ধারণা, আমাদের আশেপাশে এমন কিছু মানুষ আছে যারা কথায় কথায় আমাদের ছোট করে, এই মানুষগুলোর আচরণের জন্য আমরা নিজের ভেতর ডিপ্রেসন তৈরি করে ফেলি ... যে মানুষগুলো প্রতিনিয়ত আমার অজ্ঞতা কিংবা অপারগতা নিয়ে অপমানের সুরে কথা বলে, তাদের থেকে দূরে থাকাই ভালো ... অন্যের কথার কারণে নিজের ভেতরে অপমানবোধের যে অনুভূতিটা জন্মায়, তার চেয়ে ভয়ংকর তীব্র অনুভূতি আর একটাওনেই !!',\n",
       " 'সবকিছুর সাথে আপোষ করা যায়,কিন্তু নিজের আত্মসম্মানকে যে আঘাত করে,তার সাথে আপোষ করার প্রশ্নই আসেনা ')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document[30], summary[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vanMVt32tQ1y",
    "outputId": "df82dbc9-4957-47da-9b34-f2723777d2a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1026"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "SN34pX5Xtnl6"
   },
   "outputs": [],
   "source": [
    "c1 = dict(df.Text.str.split(expand=True).stack().value_counts())\n",
    "c1 = dict(sorted(c1.items(), key=lambda x: x[1], reverse=True))\n",
    "c2 = dict(df.Summary.str.split(expand=True).stack().value_counts())\n",
    "c2 = dict(sorted(c2.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6FagwZYt5yI",
    "outputId": "4ab54d5d-77b2-4468-9884-0d7cb9c14c02"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'না': 154,\n",
       " 'মানুষ': 70,\n",
       " 'জন্য': 63,\n",
       " 'করে': 62,\n",
       " 'ভালোবাসা': 62,\n",
       " 'না।': 56,\n",
       " 'অনেক': 54,\n",
       " 'করা': 48,\n",
       " 'মানুষের': 45,\n",
       " 'করতে': 45,\n",
       " 'আর': 45,\n",
       " 'হবে': 44,\n",
       " 'সব': 43,\n",
       " 'ভালো': 42,\n",
       " 'হয়': 39,\n",
       " 'কিছু': 39,\n",
       " 'যে': 38,\n",
       " 'সাথে': 34,\n",
       " 'এই': 33,\n",
       " 'কষ্ট': 32,\n",
       " 'এর': 32,\n",
       " 'থেকে': 32,\n",
       " 'নিজের': 32,\n",
       " 'ভালোবাসার': 32,\n",
       " 'কোনো': 31,\n",
       " 'সময়': 30,\n",
       " 'ও': 30,\n",
       " 'করার': 28,\n",
       " 'কথা': 28,\n",
       " 'হতে': 28,\n",
       " 'যায়': 28,\n",
       " ',': 28,\n",
       " 'তার': 26,\n",
       " 'নেই': 26,\n",
       " 'একটি': 25,\n",
       " 'উচিত': 25,\n",
       " 'থাকে': 25,\n",
       " 'হয়ে': 24,\n",
       " 'আমাদের': 24,\n",
       " 'কেউ': 24,\n",
       " 'কারো': 24,\n",
       " 'প্রতি': 24,\n",
       " 'পারে': 23,\n",
       " 'প্রেম': 21,\n",
       " '।': 21,\n",
       " 'আমরা': 21,\n",
       " 'মানুষকে': 21,\n",
       " 'উপর': 20,\n",
       " 'একজন': 20,\n",
       " 'যায়।': 20,\n",
       " 'জীবন': 20,\n",
       " 'এক': 20,\n",
       " 'হবে।': 19,\n",
       " 'হয়।': 19,\n",
       " 'নিজেকে': 19,\n",
       " 'কাউকে': 19,\n",
       " 'মন': 19,\n",
       " 'থাকা': 18,\n",
       " 'একটা': 18,\n",
       " 'কখনো': 18,\n",
       " 'তাকে': 18,\n",
       " 'এবং': 18,\n",
       " 'নিয়ে': 18,\n",
       " 'এখন': 18,\n",
       " 'করোনা': 17,\n",
       " 'এমন': 17,\n",
       " 'তা': 16,\n",
       " 'চলে': 16,\n",
       " 'সে': 16,\n",
       " 'জীবনে': 16,\n",
       " 'মধ্যে': 16,\n",
       " 'মতো': 16,\n",
       " 'শুধু': 15,\n",
       " 'যাবে': 15,\n",
       " 'সবার': 15,\n",
       " 'হচ্ছে': 15,\n",
       " 'মানে': 15,\n",
       " 'গুলো': 15,\n",
       " 'বেশি': 15,\n",
       " 'উচিত।': 14,\n",
       " 'সম্মান': 14,\n",
       " 'নেই।': 14,\n",
       " 'ভালোবাসতে': 14,\n",
       " 'খারাপ': 14,\n",
       " 'টাকা': 14,\n",
       " 'আছে': 14,\n",
       " 'দিতে': 13,\n",
       " 'জীবনের': 13,\n",
       " 'আগে': 13,\n",
       " 'হলে': 13,\n",
       " 'বলে': 13,\n",
       " 'মাঝে': 13,\n",
       " 'থাকতে': 13,\n",
       " 'কাছে': 13,\n",
       " 'যা': 13,\n",
       " 'না,': 13,\n",
       " 'দরকার': 12,\n",
       " 'নয়': 12,\n",
       " 'বলতে': 12,\n",
       " 'কিছুই': 12,\n",
       " 'সেই': 12,\n",
       " 'দিয়ে': 12,\n",
       " 'দিয়ে': 12,\n",
       " 'নারীর': 12,\n",
       " 'বা': 12,\n",
       " 'রাখতে': 12,\n",
       " 'মনে': 11,\n",
       " 'আপনি': 11,\n",
       " 'করে।': 11,\n",
       " 'কে': 11,\n",
       " 'কারণ': 11,\n",
       " 'রাখা': 11,\n",
       " 'কাজ': 11,\n",
       " 'হওয়া': 11,\n",
       " 'মেনে': 11,\n",
       " 'পাওয়া': 11,\n",
       " 'মানেই': 11,\n",
       " 'সম্পর্ক': 11,\n",
       " 'কি': 10,\n",
       " 'নিতে': 10,\n",
       " 'শেষ': 10,\n",
       " 'চেষ্টা': 10,\n",
       " 'মনের': 10,\n",
       " 'থাকলে': 10,\n",
       " 'আমি': 10,\n",
       " 'নাই।': 10,\n",
       " 'আমার': 10,\n",
       " 'থাকার': 10,\n",
       " 'সবাই': 10,\n",
       " 'দরকার।': 10,\n",
       " 'খুব': 10,\n",
       " 'নিজে': 9,\n",
       " 'চায়': 9,\n",
       " 'সময়ের': 9,\n",
       " 'প্রয়োজন': 9,\n",
       " 'সম্পর্কে': 9,\n",
       " 'যার': 9,\n",
       " 'নারীরা': 9,\n",
       " 'পেতে': 9,\n",
       " 'কষ্টের': 9,\n",
       " 'ইচ্ছা': 9,\n",
       " 'পর': 9,\n",
       " 'বন্ধু': 9,\n",
       " 'দেয়া': 9,\n",
       " 'গুরুত্ব': 9,\n",
       " 'যত': 9,\n",
       " 'বড়': 9,\n",
       " 'থাকবে': 9,\n",
       " 'মূল্য': 9,\n",
       " 'বদলে': 8,\n",
       " 'সবচেয়ে': 8,\n",
       " 'সত্যিকারের': 8,\n",
       " 'ভালো।': 8,\n",
       " 'মেয়েদের': 8,\n",
       " 'নারী': 8,\n",
       " 'অন্যের': 8,\n",
       " 'কোন': 8,\n",
       " 'নতুন': 8,\n",
       " 'দিন': 8,\n",
       " 'তাই': 8,\n",
       " 'পাশে': 8,\n",
       " 'বাংলাদেশে': 8,\n",
       " 'তাদের': 8,\n",
       " 'নষ্ট': 8,\n",
       " 'চেয়ে': 8,\n",
       " 'বই': 8,\n",
       " 'ভাইরাস': 8,\n",
       " 'যদি': 7,\n",
       " 'ঠিক': 7,\n",
       " 'নিয়ন্ত্রণ': 7,\n",
       " 'বড়': 7,\n",
       " 'সত্যি': 7,\n",
       " 'দিনশেষে': 7,\n",
       " 'প্রকাশ': 7,\n",
       " 'বলা': 7,\n",
       " 'দেশের': 7,\n",
       " 'কিন্তু': 7,\n",
       " 'লাগে': 7,\n",
       " 'দেশে': 7,\n",
       " 'দুই': 7,\n",
       " 'নাম': 7,\n",
       " 'সৌন্দর্য': 7,\n",
       " 'খুশি': 7,\n",
       " 'লেখক': 7,\n",
       " 'গেলে': 7,\n",
       " 'সুখ': 7,\n",
       " 'হয়।': 7,\n",
       " 'ভারত': 7,\n",
       " 'ছাড়া': 7,\n",
       " 'ছেলে': 7,\n",
       " 'কঠিন।': 7,\n",
       " 'বাংলাদেশ': 6,\n",
       " 'মেয়েরা': 6,\n",
       " 'একই': 6,\n",
       " 'চাওয়া': 6,\n",
       " 'আরো': 6,\n",
       " 'চেয়ে': 6,\n",
       " 'নয়।': 6,\n",
       " 'কিছুর': 6,\n",
       " 'পরিবর্তন': 6,\n",
       " 'প্রকৃত': 6,\n",
       " 'দেয়': 6,\n",
       " 'দেখা': 6,\n",
       " 'মিথ্যা': 6,\n",
       " 'আছে।': 6,\n",
       " 'করো': 6,\n",
       " 'ছেড়ে': 6,\n",
       " 'জোর': 6,\n",
       " 'আপনার': 6,\n",
       " 'দেখে': 6,\n",
       " 'প্রতিটি': 6,\n",
       " 'অল্পতেই': 6,\n",
       " 'তো': 6,\n",
       " 'পারে।': 6,\n",
       " 'লেখকের': 6,\n",
       " 'আসবে': 6,\n",
       " 'আত্মসম্মান': 6,\n",
       " 'কখনও': 6,\n",
       " 'থাকাটা': 6,\n",
       " 'যেমন': 6,\n",
       " 'ধর্ম': 6,\n",
       " 'ভুল': 6,\n",
       " 'তুমি': 6,\n",
       " 'আপনাকে': 6,\n",
       " 'দেশ': 6,\n",
       " 'খুঁজে': 6,\n",
       " 'চাইতে': 6,\n",
       " 'জিনিস': 6,\n",
       " 'বিশ্বাস': 6,\n",
       " 'পাওয়ার': 5,\n",
       " 'খরচ': 5,\n",
       " 'ভালোবেসে': 5,\n",
       " 'জরুরী।': 5,\n",
       " 'কবি': 5,\n",
       " 'শিক্ষা': 5,\n",
       " 'যাবে।': 5,\n",
       " 'যারা': 5,\n",
       " 'মায়ের': 5,\n",
       " 'পুরুষের': 5,\n",
       " 'সমাজে': 5,\n",
       " 'হলো': 5,\n",
       " 'আটকে': 5,\n",
       " 'বছরের': 5,\n",
       " 'বলার': 5,\n",
       " 'মাধ্যমে': 5,\n",
       " 'দোষ': 5,\n",
       " 'চাই।': 5,\n",
       " 'আসল': 5,\n",
       " 'দায়িত্ব': 5,\n",
       " 'দেওয়ার': 5,\n",
       " 'বুঝে': 5,\n",
       " 'একদিন': 5,\n",
       " 'যাওয়ার': 5,\n",
       " 'পৃথিবীতে': 5,\n",
       " 'তখন': 5,\n",
       " 'চোখে': 5,\n",
       " 'বেঁচে': 5,\n",
       " 'ভালোবাসায়': 5,\n",
       " 'যখন': 5,\n",
       " 'সুন্দর': 5,\n",
       " 'হয়ে': 5,\n",
       " 'নিজেদের': 5,\n",
       " 'কঠিন': 5,\n",
       " 'বাংলা': 5,\n",
       " 'ফেলে': 5,\n",
       " 'দাম': 5,\n",
       " 'হওয়ার': 5,\n",
       " 'আসে': 5,\n",
       " 'প্রশ্ন': 5,\n",
       " 'ছেলেদের': 5,\n",
       " 'ভালোবাসা।': 5,\n",
       " 'যথেষ্ট': 5,\n",
       " 'থাকে।': 5,\n",
       " 'সুখী': 5,\n",
       " 'ক্ষতি': 5,\n",
       " 'সবারই': 4,\n",
       " 'গিয়ে': 4,\n",
       " 'পাবে': 4,\n",
       " 'লুকিয়ে': 4,\n",
       " 'সন্তানের': 4,\n",
       " 'চোখের': 4,\n",
       " 'দেওয়া': 4,\n",
       " 'স্মৃতি': 4,\n",
       " 'তোমাকে': 4,\n",
       " 'হলেও': 4,\n",
       " 'আলো': 4,\n",
       " 'মজুদ': 4,\n",
       " 'আমাকে': 4,\n",
       " 'পিছনে': 4,\n",
       " 'চিন্তা': 4,\n",
       " 'অন্যকে': 4,\n",
       " 'দৃষ্টিভঙ্গি': 4,\n",
       " 'এ': 4,\n",
       " 'সকল': 4,\n",
       " 'গুলা': 4,\n",
       " 'করুন': 4,\n",
       " 'মানা': 4,\n",
       " 'সারাজীবন': 4,\n",
       " 'হার': 4,\n",
       " 'ইচ্ছে': 4,\n",
       " 'বয়স': 4,\n",
       " 'ভুলের': 4,\n",
       " 'সারারাত': 4,\n",
       " 'বেস্ট': 4,\n",
       " 'পায়': 4,\n",
       " 'নারীকে': 4,\n",
       " 'জেগে': 4,\n",
       " 'ছোটখাটো': 4,\n",
       " 'অধিকার': 4,\n",
       " 'ভালোবাসে': 4,\n",
       " 'পড়া': 4,\n",
       " 'মেয়েদের': 4,\n",
       " 'জীবনকে': 4,\n",
       " 'বড়ই': 4,\n",
       " 'গ্রহ': 4,\n",
       " 'দিয়েছে।': 4,\n",
       " 'রাখার': 4,\n",
       " 'ভাষার': 4,\n",
       " 'বসে': 4,\n",
       " 'সেটা': 4,\n",
       " 'পৃথিবীর': 4,\n",
       " 'ছোট': 4,\n",
       " 'তেমনি': 4,\n",
       " 'অপেক্ষা': 4,\n",
       " 'সকলের': 4,\n",
       " 'বিভিন্ন': 4,\n",
       " 'যাই': 4,\n",
       " 'আবেগ': 4,\n",
       " 'প্রকৃতির': 4,\n",
       " 'যাই।': 4,\n",
       " 'পরিচয়': 4,\n",
       " 'ধরে': 4,\n",
       " 'আক্রান্ত': 4,\n",
       " 'পবিত্র': 4,\n",
       " 'সঠিক': 4,\n",
       " 'থেমে': 4,\n",
       " 'আবার': 4,\n",
       " 'দূরে': 4,\n",
       " 'এখনো': 4,\n",
       " 'অতি': 4,\n",
       " 'যুদ্ধ': 4,\n",
       " 'করেছে': 4,\n",
       " 'ভুলে': 4,\n",
       " 'ভাইরাসের': 4,\n",
       " 'করতেন।': 4,\n",
       " 'যেতে': 4,\n",
       " 'পেয়ে': 4,\n",
       " 'গল্প': 4,\n",
       " 'স্কুল': 4,\n",
       " 'অবস্থায়': 4,\n",
       " 'সমাজের': 4,\n",
       " 'মহৎ': 4,\n",
       " 'যায়।': 4,\n",
       " 'সময়': 4,\n",
       " 'কাজের': 4,\n",
       " 'পড়ে': 4,\n",
       " 'স্বাধীনতা': 4,\n",
       " 'আছে,': 4,\n",
       " 'সচেতন': 4,\n",
       " 'সামনে': 4,\n",
       " 'লোক': 4,\n",
       " 'তোমার': 4,\n",
       " 'দেয়ার': 4,\n",
       " 'করি।': 4,\n",
       " 'সুযোগ': 4,\n",
       " 'দেখানো': 4,\n",
       " 'নেয়া': 4,\n",
       " 'সবচাইতে': 4,\n",
       " 'পাল্টায়': 4,\n",
       " 'চুরি': 4,\n",
       " 'ভাল': 4,\n",
       " 'দেয়।': 3,\n",
       " 'বাবা': 3,\n",
       " 'অনুভূতি': 3,\n",
       " 'যায়,': 3,\n",
       " 'তবে': 3,\n",
       " 'মর্ম': 3,\n",
       " 'কবিকে': 3,\n",
       " 'উপভোগ': 3,\n",
       " 'ঘুম': 3,\n",
       " 'মেয়ের': 3,\n",
       " 'হয়েছে।': 3,\n",
       " 'কিশোর': 3,\n",
       " 'সত্য': 3,\n",
       " 'বাংলাদেশের': 3,\n",
       " 'পথে': 3,\n",
       " 'কখন': 3,\n",
       " 'এটি': 3,\n",
       " 'স্বপ্ন': 3,\n",
       " 'মেয়ে': 3,\n",
       " 'কথা।': 3,\n",
       " 'মিষ্টি': 3,\n",
       " 'আগামী': 3,\n",
       " 'গেছে': 3,\n",
       " 'করলে': 3,\n",
       " 'মিস': 3,\n",
       " 'অভাব': 3,\n",
       " 'পাই': 3,\n",
       " 'শক্ত': 3,\n",
       " 'স্বাধীনতার': 3,\n",
       " 'টা': 3,\n",
       " 'আত্মবিশ্বাস': 3,\n",
       " 'রাখা।': 3,\n",
       " 'হোক।': 3,\n",
       " 'ই': 3,\n",
       " 'ভরসা': 3,\n",
       " 'যায়': 3,\n",
       " 'পাওয়া': 3,\n",
       " 'কথার': 3,\n",
       " 'পূরণ': 3,\n",
       " 'এখনও': 3,\n",
       " 'উপহার': 3,\n",
       " 'শক্তিশালী': 3,\n",
       " 'পার্থক্য': 3,\n",
       " 'সংসার': 3,\n",
       " 'ধরনের': 3,\n",
       " 'বঙ্গবন্ধুর': 3,\n",
       " 'মধ্যবিত্ত': 3,\n",
       " 'লাগে।': 3,\n",
       " 'বিচার': 3,\n",
       " 'কাহিনী।': 3,\n",
       " 'অনেক।': 3,\n",
       " 'নয়।': 3,\n",
       " 'জানানো': 3,\n",
       " 'নেওয়া': 3,\n",
       " 'পুরাতন': 3,\n",
       " 'আলাদা।': 3,\n",
       " 'করবেন': 3,\n",
       " 'ঘৃণা': 3,\n",
       " 'সম্ভব': 3,\n",
       " 'প্রাপ্য': 3,\n",
       " 'আল্লাহ': 3,\n",
       " 'জানতে': 3,\n",
       " 'যাকে': 3,\n",
       " 'করা।': 3,\n",
       " 'যাওয়া': 3,\n",
       " 'অতিরিক্ত': 3,\n",
       " 'প্রয়োজন।': 3,\n",
       " 'উচিৎ।': 3,\n",
       " 'লজ্জা': 3,\n",
       " 'মানুষেরই': 3,\n",
       " 'কার': 3,\n",
       " 'যথেষ্ট।': 3,\n",
       " 'মুক্তি': 3,\n",
       " 'জানে।': 3,\n",
       " 'আসল।': 3,\n",
       " 'সাহায্য': 3,\n",
       " 'একা': 3,\n",
       " 'জিনিসের': 3,\n",
       " 'বিরুদ্ধে': 3,\n",
       " 'চেনা': 3,\n",
       " 'বেপারে': 3,\n",
       " 'করেই': 3,\n",
       " 'হবার': 3,\n",
       " 'বাস্তবতা': 3,\n",
       " 'হবে,': 3,\n",
       " 'মিথ্যে': 3,\n",
       " 'হাত': 3,\n",
       " 'দুঃখ': 3,\n",
       " 'পারফেক্ট': 3,\n",
       " 'খুবই': 3,\n",
       " 'উপর।': 3,\n",
       " 'মেয়েরা': 3,\n",
       " 'করুন।': 3,\n",
       " 'লাভ': 3,\n",
       " 'কাল': 3,\n",
       " 'মানুষগুলো': 3,\n",
       " 'সারাদিন': 3,\n",
       " 'তুচ্ছ': 3,\n",
       " 'পুরুষদের': 3,\n",
       " 'বোকা': 3,\n",
       " 'পরে': 3,\n",
       " 'মাথায়': 3,\n",
       " 'মত': 3,\n",
       " 'বিয়া': 3,\n",
       " 'এটা': 3,\n",
       " 'ব্ল্যাক': 3,\n",
       " 'বানানো': 3,\n",
       " 'সিনেমা': 3,\n",
       " 'স্ট্যাটাস': 3,\n",
       " 'টেস্ট': 3,\n",
       " 'ধারনা': 3,\n",
       " 'ফল': 3,\n",
       " 'তেমন': 3,\n",
       " 'হয়েছে': 3,\n",
       " 'দুনিয়াতে': 3,\n",
       " 'পারলে': 3,\n",
       " 'পড়াশোনা': 3,\n",
       " 'ক্ষমতা': 3,\n",
       " 'যুদ্ধে': 3,\n",
       " 'বিশ্বের': 3,\n",
       " 'হাতে': 3,\n",
       " 'তে': 3,\n",
       " 'শাস্তি': 3,\n",
       " 'মেধা': 3,\n",
       " 'বাক': 3,\n",
       " 'সবাইকে': 3,\n",
       " 'ক্রিকেট': 3,\n",
       " 'ঢাকা': 3,\n",
       " 'কাজে': 3,\n",
       " 'পেলে': 3,\n",
       " 'ভীষণ': 3,\n",
       " 'বিয়ে': 3,\n",
       " 'সমান': 3,\n",
       " 'উপায়': 3,\n",
       " 'পানি': 3,\n",
       " 'চাই': 3,\n",
       " 'হিসাব': 3,\n",
       " 'লাগার': 3,\n",
       " 'আসার': 3,\n",
       " 'যাওয়া': 3,\n",
       " 'সফলতা': 3,\n",
       " 'তত': 3,\n",
       " 'মাস্ক': 3,\n",
       " 'এসে': 3,\n",
       " 'স্ট্রিং': 3,\n",
       " 'বিজ্ঞান': 3,\n",
       " 'নির্ভর': 3,\n",
       " 'চুপচাপ': 3,\n",
       " 'হল': 3,\n",
       " 'গুলোর': 3,\n",
       " 'পড়ার': 3,\n",
       " 'সৃষ্টি': 3,\n",
       " 'থাকুক': 3,\n",
       " 'কেন্দ্র': 3,\n",
       " 'অবহেলা': 3,\n",
       " 'প্রস্তুতি': 3,\n",
       " 'কিট': 3,\n",
       " 'অর্থের': 3,\n",
       " 'বলে,': 3,\n",
       " 'মূল': 3,\n",
       " 'বোঝাতে': 3,\n",
       " 'হিসাবে': 3,\n",
       " 'ভাইরাসে': 3,\n",
       " 'কাছের': 3,\n",
       " 'থেকেই': 3,\n",
       " 'বন্ধ': 3,\n",
       " 'আলোর': 3,\n",
       " 'একে': 3,\n",
       " 'শক্তি': 3,\n",
       " 'আগলে': 2,\n",
       " 'নাটকের': 2,\n",
       " 'অন্তরে': 2,\n",
       " '৩': 2,\n",
       " 'যুগে': 2,\n",
       " ',আমি': 2,\n",
       " 'প্রজন্ম': 2,\n",
       " 'বুদ্ধিজীবী': 2,\n",
       " 'ভাষা': 2,\n",
       " 'ইংরেজি': 2,\n",
       " 'প্রযুক্তির': 2,\n",
       " 'কমে': 2,\n",
       " 'মার্চ': 2,\n",
       " 'আকাশ': 2,\n",
       " 'দিবে': 2,\n",
       " 'শ্রদ্ধা।': 2,\n",
       " 'বাসনা।': 2,\n",
       " 'রেখেই': 2,\n",
       " 'খবর': 2,\n",
       " ',সে': 2,\n",
       " 'যায়.': 2,\n",
       " 'ভাঙ্গার': 2,\n",
       " 'কুসংস্কার': 2,\n",
       " 'তাৎপর্য': 2,\n",
       " 'থেকেও': 2,\n",
       " 'ভোলা': 2,\n",
       " 'একসাথে': 2,\n",
       " 'চাওয়া।': 2,\n",
       " 'চেয়েও': 2,\n",
       " 'পারা': 2,\n",
       " 'বয়সে': 2,\n",
       " 'ইমোশন': 2,\n",
       " 'বৃদ্ধ': 2,\n",
       " 'ভালোবাসি': 2,\n",
       " 'কোঠাটান': 2,\n",
       " ',এর': 2,\n",
       " 'বোঝা': 2,\n",
       " 'ছাড়াও': 2,\n",
       " 'চাবে': 2,\n",
       " 'শত': 2,\n",
       " 'আকুল': 2,\n",
       " 'সেরা': 2,\n",
       " 'অভিমানের': 2,\n",
       " 'নির্ভরশীল': 2,\n",
       " 'দেখেই': 2,\n",
       " 'সনাক্তকরনের': 2,\n",
       " 'পাবনা': 2,\n",
       " 'অবস্থান': 2,\n",
       " 'ঘরে': 2,\n",
       " 'বোঝে': 2,\n",
       " 'আবেগের': 2,\n",
       " 'বৃষ্টির': 2,\n",
       " 'কুলখানি': 2,\n",
       " 'সূর্য': 2,\n",
       " 'কেন': 2,\n",
       " 'প্রতিষ্ঠিত': 2,\n",
       " 'রাগ': 2,\n",
       " 'শক্তির': 2,\n",
       " 'মধ্য': 2,\n",
       " 'যোগ': 2,\n",
       " 'বহুবচন': 2,\n",
       " 'টেলিস্কোপের': 2,\n",
       " 'পরিবারের': 2,\n",
       " 'মাধ্যমের': 2,\n",
       " 'জেনেও': 2,\n",
       " 'আসতে': 2,\n",
       " 'ছিল': 2,\n",
       " 'গুলোকে': 2,\n",
       " 'কণিকা': 2,\n",
       " 'বুঝতে': 2,\n",
       " 'জেদ': 2,\n",
       " 'প্রোটন': 2,\n",
       " 'এনে': 2,\n",
       " 'ব্যবস্থা': 2,\n",
       " 'কম্পনের': 2,\n",
       " 'হিসেবে': 2,\n",
       " 'পদার্থের': 2,\n",
       " 'দ্বিতীয়': 2,\n",
       " 'মেশিন': 2,\n",
       " 'রাখে।': 2,\n",
       " 'ব্যাবহার': 2,\n",
       " 'মায়ায়': 2,\n",
       " 'অসাধারণ': 2,\n",
       " 'সম্মানী': 2,\n",
       " 'চিনে।': 2,\n",
       " 'মোকাবিলায়': 2,\n",
       " 'তুলে।': 2,\n",
       " 'একদিন।': 2,\n",
       " 'ছেলের': 2,\n",
       " 'দেখতে': 2,\n",
       " 'প্রেমে': 2,\n",
       " 'ভালোবাসলে': 2,\n",
       " 'বিচারিক': 2,\n",
       " 'ম্যাজিস্ট্রেটদের': 2,\n",
       " 'প্রয়োজনে': 2,\n",
       " 'কোরিয়া': 2,\n",
       " 'মৃত্যুর': 2,\n",
       " 'যাওয়া।': 2,\n",
       " 'বাঙালি': 2,\n",
       " 'শরীর': 2,\n",
       " 'জাতির': 2,\n",
       " 'দেহ': 2,\n",
       " 'করো।': 2,\n",
       " 'ছায়াপথ।': 2,\n",
       " 'দিনের': 2,\n",
       " 'কখনোই': 2,\n",
       " 'মূলত': 2,\n",
       " 'ঘুমাতে': 2,\n",
       " 'রাষ্ট্র': 2,\n",
       " 'বেশি।': 2,\n",
       " 'বললে': 2,\n",
       " 'অসুস্থ': 2,\n",
       " 'নির্দেশ': 2,\n",
       " 'ইভেন': 2,\n",
       " 'অর্থ': 2,\n",
       " 'জন্মতোই': 2,\n",
       " 'পাশের': 2,\n",
       " 'কাকে': 2,\n",
       " 'কালের': 2,\n",
       " 'সময়টা': 2,\n",
       " 'রাখি।': 2,\n",
       " 'নারীরাই': 2,\n",
       " 'জাগরণের': 2,\n",
       " 'প্রধান': 2,\n",
       " 'অন্তরায়': 2,\n",
       " 'জন্মদিন': 2,\n",
       " 'বোকামি': 2,\n",
       " '\"প্রাক্তন\"': 2,\n",
       " 'কত': 2,\n",
       " 'এক্সদের': 2,\n",
       " 'ব্যাপার।': 2,\n",
       " 'দিক': 2,\n",
       " 'বোঝার': 2,\n",
       " 'বিশ্ববিদ্যালয়ের': 2,\n",
       " 'বরাদ্দ': 2,\n",
       " 'গেছে।': 2,\n",
       " 'ছাড়াই': 2,\n",
       " 'ফেরানো': 2,\n",
       " 'স্যানিটাইজার': 2,\n",
       " 'ফেসবুকে': 2,\n",
       " 'নারীদের': 2,\n",
       " 'পরিষ্কার': 2,\n",
       " 'পরিপূর্ণভাবে': 2,\n",
       " 'খেলায়': 2,\n",
       " 'সাবান': 2,\n",
       " 'ছুটে': 2,\n",
       " 'হ্যান্ড': 2,\n",
       " 'করিনি': 2,\n",
       " 'অতিমুনাফার': 2,\n",
       " 'মৃত্যু': 2,\n",
       " 'যাবেন।': 2,\n",
       " 'এমনেই': 2,\n",
       " 'ভুলার': 2,\n",
       " 'ভালোবেসেছি।': 2,\n",
       " 'মানিয়ে': 2,\n",
       " 'সংখ্যা': 2,\n",
       " 'ঘৃনা': 2,\n",
       " 'ক্রিকেটের': 2,\n",
       " 'চাঞ্চল্য': 2,\n",
       " 'শিক্ষার্থীরা': 2,\n",
       " 'আব্দুল': 2,\n",
       " 'হামিদ': 2,\n",
       " 'করি': 2,\n",
       " 'ভেজাল': 2,\n",
       " 'পাশাপাশি': 2,\n",
       " 'বয়সের': 2,\n",
       " 'কিছুকে': 2,\n",
       " 'পড়াশুনা': 2,\n",
       " 'মানায়': 2,\n",
       " 'অনেকেই': 2,\n",
       " 'জাতীয়': 2,\n",
       " 'কোচিং': 2,\n",
       " 'উপসর্গ': 2,\n",
       " 'শব্দটা': 2,\n",
       " 'ভেতরে': 2,\n",
       " 'নানা': 2,\n",
       " 'মুখ': 2,\n",
       " 'লিখে': 2,\n",
       " 'কষ্টের।': 2,\n",
       " 'চাইলে': 2,\n",
       " 'অল্প': 2,\n",
       " 'লেখা': 2,\n",
       " 'ইতিহাস': 2,\n",
       " 'জরুরি।': 2,\n",
       " 'মেরে': 2,\n",
       " 'পারছি': 2,\n",
       " 'যাওয়ায়': 2,\n",
       " 'করোনায়': 2,\n",
       " 'জাতি': 2,\n",
       " 'নেয়ার': 2,\n",
       " 'বসবাস': 2,\n",
       " 'নবাবপুর': 2,\n",
       " 'গুজব': 2,\n",
       " 'বিভ্রান্ত': 2,\n",
       " 'ব্যস্ত': 2,\n",
       " 'যত্ন': 2,\n",
       " 'চায়।': 2,\n",
       " 'গভীরতা': 2,\n",
       " 'শোনার': 2,\n",
       " 'ধর্ষিতা': 2,\n",
       " 'অতীত': 2,\n",
       " 'হারিয়ে': 2,\n",
       " 'স্যরি': 2,\n",
       " 'ঘটে': 2,\n",
       " 'করেও': 2,\n",
       " 'দিনে': 2,\n",
       " 'ধর্যের': 2,\n",
       " 'রাখুন।': 2,\n",
       " 'শিখা': 2,\n",
       " 'মনুষ্যত্বহীন': 2,\n",
       " 'হওয়া': 2,\n",
       " 'ভিতরের': 2,\n",
       " 'মানুষই': 2,\n",
       " 'ভাষণ।': 2,\n",
       " 'বিরত': 2,\n",
       " 'মেয়েকে': 2,\n",
       " 'সামান্য': 2,\n",
       " 'মার্চের': 2,\n",
       " 'মায়া': 2,\n",
       " 'গোপন': 2,\n",
       " 'ধরণ': 2,\n",
       " 'অভিযোগ।': 2,\n",
       " 'ধরণের': 2,\n",
       " 'রোমান্টিক': 2,\n",
       " 'অভিনয়': 2,\n",
       " 'মানুষগুলোর': 2,\n",
       " 'বিয়ের': 2,\n",
       " 'বাড়ির': 2,\n",
       " 'কেউই': 2,\n",
       " 'দিয়েছে': 2,\n",
       " 'আপনজন': 2,\n",
       " 'ফ্যামিলির্': 2,\n",
       " 'শিখুন।': 2,\n",
       " 'অপমান': 2,\n",
       " 'রেজাল্ট': 2,\n",
       " 'মানুষটির': 2,\n",
       " 'আজ': 2,\n",
       " 'সাদা': 2,\n",
       " 'কাজই': 2,\n",
       " 'ব্যক্তিত্ব': 2,\n",
       " 'গুছিয়ে': 2,\n",
       " 'দিবেস': 2,\n",
       " 'দিনই।': 2,\n",
       " 'ছড়িয়ে': 2,\n",
       " 'যেকোনো': 2,\n",
       " 'পারি।': 2,\n",
       " 'ভাবে': 2,\n",
       " 'শুভেচ্ছা': 2,\n",
       " 'খুন': 2,\n",
       " 'চাইলেই': 2,\n",
       " 'লাগান।': 2,\n",
       " 'ফ্রেন্ড': 2,\n",
       " 'সময়কে': 2,\n",
       " 'জন্যই': 2,\n",
       " 'জাজমেন্ট': 2,\n",
       " 'প্রিয়': 2,\n",
       " 'রক্ষা': 2,\n",
       " '৭': 2,\n",
       " 'করানো': 2,\n",
       " 'নাই': 2,\n",
       " 'নিদৃষ্ট': 2,\n",
       " 'প্রত্যেক': 2,\n",
       " 'ঐসময়।': 2,\n",
       " 'একুশ': 2,\n",
       " 'রয়েছে।': 2,\n",
       " 'কাউকেও': 2,\n",
       " 'ধ্বংস': 2,\n",
       " 'দিবস': 2,\n",
       " 'মজাই': 2,\n",
       " 'চিঠি': 2,\n",
       " 'সবচেয়ে': 2,\n",
       " 'দিয়া': 2,\n",
       " 'সম্পত্তি': 2,\n",
       " 'অন্য': 2,\n",
       " 'নির্দিষ্ট': 2,\n",
       " 'দিন।': 2,\n",
       " 'সোজা': 2,\n",
       " 'মান': 2,\n",
       " 'নিখুঁত': 2,\n",
       " 'ভাবছি': 2,\n",
       " 'ডাক্তারের': 2,\n",
       " 'প্রভাব': 2,\n",
       " 'অপরিসীম': 2,\n",
       " 'তারাই': 2,\n",
       " 'দিয়েও': 2,\n",
       " 'অপ্রত্যাশিত': 2,\n",
       " 'ধরা': 2,\n",
       " 'ট্রাফিক': 2,\n",
       " 'সিদ্ধান্ত': 2,\n",
       " 'ততো': 2,\n",
       " 'পরীক্ষা': 2,\n",
       " 'বিচারের': 2,\n",
       " 'অপরকে': 2,\n",
       " 'মুখে': 2,\n",
       " 'পারার': 2,\n",
       " 'পারবে': 2,\n",
       " 'স্বামীর': 2,\n",
       " 'পৃথিবী': 2,\n",
       " 'বিয়ে': 2,\n",
       " 'দ্বারা': 2,\n",
       " 'পর্যন্ত': 2,\n",
       " 'পাবেন': 2,\n",
       " 'শুরু': 2,\n",
       " 'আপোষ': 2,\n",
       " 'গুন': 2,\n",
       " 'কি?': 2,\n",
       " 'বৃদ্ধি': 2,\n",
       " 'ভালোবাসাকে': 2,\n",
       " 'সময়ে': 2,\n",
       " 'পায়।': 2,\n",
       " 'বিপদে': 2,\n",
       " 'জীবনটা': 2,\n",
       " 'সৈয়দ': 2,\n",
       " 'বিদেশে': 2,\n",
       " 'ভালবাসার': 2,\n",
       " 'হয়তো': 2,\n",
       " 'নেওয়ার': 2,\n",
       " 'পুরোনো': 2,\n",
       " 'সেবা': 2,\n",
       " 'জটিল': 2,\n",
       " 'পরিশ্রম': 2,\n",
       " 'ধারণ': 2,\n",
       " 'কোনও': 2,\n",
       " 'আনন্দ': 2,\n",
       " 'শুদ্ধতম': 2,\n",
       " 'সাহিত্য': 2,\n",
       " 'নামে': 2,\n",
       " 'বাঁচতে': 2,\n",
       " 'আল্লাহর': 2,\n",
       " 'একাকিত্ব': 2,\n",
       " 'মার': 2,\n",
       " 'সর্বনাশ': 2,\n",
       " 'হউক': 2,\n",
       " 'হক': 2,\n",
       " 'চোরের': 2,\n",
       " 'হাজার': 2,\n",
       " 'নিস্পাপ': 2,\n",
       " 'লেখার': 2,\n",
       " 'চাহিদা': 2,\n",
       " 'ব্যবসা': 2,\n",
       " 'যাচ্ছি।': 2,\n",
       " 'এখানেই': 2,\n",
       " 'নিজেকেই': 2,\n",
       " 'সমস্যাটা': 2,\n",
       " 'ভেবে': 2,\n",
       " 'করেছি': 2,\n",
       " 'পাল্টা': 2,\n",
       " 'বিদেশ': 2,\n",
       " 'সারা': 2,\n",
       " 'দিকে': 2,\n",
       " 'সয়ে': 2,\n",
       " 'মাকে': 2,\n",
       " 'তাহলে': 2,\n",
       " 'হিসেব': 2,\n",
       " 'বের': 2,\n",
       " 'উঠতে': 2,\n",
       " 'পাল্টে': 2,\n",
       " 'ধুকে': 2,\n",
       " 'দুর্বোধ্য': 2,\n",
       " 'তখনই': 2,\n",
       " 'অদ্ভুত': 2,\n",
       " 'বিশ্ববিদ্যালয়': 2,\n",
       " 'সম্পদ': 2,\n",
       " 'জ্ঞান': 2,\n",
       " 'নিজেরই': 2,\n",
       " 'প্রতিনিয়ত': 2,\n",
       " 'কেও': 2,\n",
       " 'করাটা': 2,\n",
       " 'আকর্ষণ': 2,\n",
       " 'উপকার': 2,\n",
       " 'স্বীকার': 2,\n",
       " 'গল্প।': 2,\n",
       " 'তারা': 2,\n",
       " 'খানের': 2,\n",
       " 'বছর': 2,\n",
       " 'মানুুষ': 2,\n",
       " '.': 2,\n",
       " 'অর্ধেক': 2,\n",
       " 'ভাগ্য': 2,\n",
       " 'আজম': 2,\n",
       " 'করবে।': 2,\n",
       " 'পুরুষ': 2,\n",
       " 'বিড়ম্বনা': 2,\n",
       " 'চুপ': 2,\n",
       " 'মেলাতে': 2,\n",
       " 'পেশা': 2,\n",
       " 'দাবি': 2,\n",
       " 'সমস্যার': 2,\n",
       " 'শহরে': 2,\n",
       " 'একেকটা': 2,\n",
       " 'চলার': 2,\n",
       " 'রুখে': 2,\n",
       " 'ব্যাস্ত': 2,\n",
       " 'সহজেই': 2,\n",
       " 'প্রদান': 1,\n",
       " 'রানে': 1,\n",
       " 'অন্যতম।': 1,\n",
       " '\"জিরো\"': 1,\n",
       " 'মুশফিক': 1,\n",
       " 'তিনটি': 1,\n",
       " 'অনুশোচনা': 1,\n",
       " 'গবেষণা': 1,\n",
       " 'সর্বোচ্চ': 1,\n",
       " 'করেছেন।': 1,\n",
       " 'স্বাস্থ্যে': 1,\n",
       " 'আউট': 1,\n",
       " 'পশুর': 1,\n",
       " 'হলো।': 1,\n",
       " 'রিটেক': 1,\n",
       " 'দল।': 1,\n",
       " 'উপভোগে': 1,\n",
       " 'অভিযোগ': 1,\n",
       " 'ক্যাসিনো': 1,\n",
       " 'শেয়ার': 1,\n",
       " 'বাজার': 1,\n",
       " 'দুইটাই': 1,\n",
       " 'জুয়া।': 1,\n",
       " '১০': 1,\n",
       " 'কোটি': 1,\n",
       " 'গুনতে': 1,\n",
       " 'অবশেষে': 1,\n",
       " 'পেলো': 1,\n",
       " 'লেগ': 1,\n",
       " 'স্পিনার।': 1,\n",
       " 'জ্বালিয়ে': 1,\n",
       " 'মারার': 1,\n",
       " 'মজা': 1,\n",
       " 'কোথাও': 1,\n",
       " 'কমের': 1,\n",
       " 'আকুতি।': 1,\n",
       " 'বাসের': 1,\n",
       " 'লোকাল': 1,\n",
       " 'পেরিয়ে': 1,\n",
       " 'খুশি।': 1,\n",
       " 'কলিজা': 1,\n",
       " 'দাদীকে': 1,\n",
       " 'ভোগায়।': 1,\n",
       " 'অমরত্ব': 1,\n",
       " 'নির্বাচন': 1,\n",
       " 'খালি': 1,\n",
       " 'নাচবো': 1,\n",
       " 'খাবো।': 1,\n",
       " 'পানামা': 1,\n",
       " 'সবসময়': 1,\n",
       " 'বহিরাগত।': 1,\n",
       " 'ক্ষমতাবান।': 1,\n",
       " 'গুলো।': 1,\n",
       " 'জীবন্ত': 1,\n",
       " 'করোনো': 1,\n",
       " 'গড়ে': 1,\n",
       " 'তোলার': 1,\n",
       " 'সফলতা।': 1,\n",
       " 'এরা': 1,\n",
       " 'ছাত্র': 1,\n",
       " 'কিংবদন্তী।': 1,\n",
       " 'শূন্যের': 1,\n",
       " 'সহায়তার': 1,\n",
       " 'এত্ত': 1,\n",
       " 'বোর্ড': 1,\n",
       " 'হোয়াইট': 1,\n",
       " ...}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "id": "jU5Y2x-yu5Ff",
    "outputId": "12a51737-91f2-4499-d55c-6777a0cd6702"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>না</td>\n",
       "      <td>1532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>করে</td>\n",
       "      <td>1516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>আমার</td>\n",
       "      <td>1143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>আর</td>\n",
       "      <td>1107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>আমি</td>\n",
       "      <td>1105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>এই</td>\n",
       "      <td>922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>তার</td>\n",
       "      <td>879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>না।</td>\n",
       "      <td>770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>একটা</td>\n",
       "      <td>765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>যে</td>\n",
       "      <td>753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>থেকে</td>\n",
       "      <td>723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>হয়ে</td>\n",
       "      <td>645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>সে</td>\n",
       "      <td>639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>...</td>\n",
       "      <td>628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>সাথে</td>\n",
       "      <td>624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>জন্য</td>\n",
       "      <td>597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>কি</td>\n",
       "      <td>565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>তোমার</td>\n",
       "      <td>551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>করতে</td>\n",
       "      <td>534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>কথা</td>\n",
       "      <td>504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Words  Count\n",
       "0      না   1532\n",
       "1     করে   1516\n",
       "2    আমার   1143\n",
       "3      আর   1107\n",
       "4     আমি   1105\n",
       "5      এই    922\n",
       "6     তার    879\n",
       "7     না।    770\n",
       "8    একটা    765\n",
       "9      যে    753\n",
       "10   থেকে    723\n",
       "11    হয়ে    645\n",
       "12     সে    639\n",
       "13    ...    628\n",
       "14   সাথে    624\n",
       "15   জন্য    597\n",
       "16     কি    565\n",
       "17  তোমার    551\n",
       "18   করতে    534\n",
       "19    কথা    504"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = pd.DataFrame(c1.items(), columns=['Words', 'Count'])\n",
    "d1 = d1.head(20)\n",
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "id": "mQG517rQuINx",
    "outputId": "08254734-eccf-42f6-8019-678b02c8491d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>না</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>মানুষ</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>জন্য</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>করে</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ভালোবাসা</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>না।</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>অনেক</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>করা</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>মানুষের</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>করতে</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>আর</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>হবে</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>সব</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ভালো</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>হয়</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>কিছু</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>যে</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>সাথে</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>এই</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>কষ্ট</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Words  Count\n",
       "0         না    154\n",
       "1      মানুষ     70\n",
       "2       জন্য     63\n",
       "3        করে     62\n",
       "4   ভালোবাসা     62\n",
       "5        না।     56\n",
       "6       অনেক     54\n",
       "7        করা     48\n",
       "8    মানুষের     45\n",
       "9       করতে     45\n",
       "10        আর     45\n",
       "11       হবে     44\n",
       "12        সব     43\n",
       "13      ভালো     42\n",
       "14        হয়     39\n",
       "15      কিছু     39\n",
       "16        যে     38\n",
       "17      সাথে     34\n",
       "18        এই     33\n",
       "19      কষ্ট     32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2 = pd.DataFrame(c2.items(), columns=['Words', 'Count'])\n",
    "d2 = d2.head(20)\n",
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "6sXflk5xuqhL",
    "outputId": "6e862db2-3764-48ec-88b5-c111f1477942"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import numpy as np\n",
    "prop = fm.FontProperties(fname='kalpurush.ttf')\n",
    "plt.barh(d1.Words,d1.Count)\n",
    "plt.yticks(d1.Words,fontproperties=prop)\n",
    "plt.title('Most Frequent Word in Bengali Text')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "ad2xMkZuvJtL",
    "outputId": "4a43999a-e474-460a-dfae-21e26c7277d7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.barh(d2.Words,d2.Count)\n",
    "plt.yticks(d2.Words,fontproperties=prop)\n",
    "plt.title('Most Frequent Word in Bengali Text Summary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70qGA1wNMB8e",
    "outputId": "ee03133f-37ba-4765-c8e4-63ba46cbfa83"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    <go> বাংলাদেশে কোচিং বানিজ্য বন্ধ এখন সময়ের দা...\n",
       "1    <go> বাংলা ভাষার প্রযুক্তি নিয়ে আমাদের আরো অনে...\n",
       "2    <go> যদি শিশুরা বই পড়ার অভ্যাস করে তাহলে সারা ...\n",
       "3    <go> বাংলাদেশে সব স্তরে নারীর ক্ষমতায়নের জন্য ...\n",
       "4                <go> ভালো কথা বল, নয়ত চুপ থাকো <stop>\n",
       "Name: Summary, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = summary.apply(lambda x: '<go> ' + str(x) + ' <stop>')\n",
    "summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "8HvwpXkyME9l"
   },
   "outputs": [],
   "source": [
    "# since < and > from default tokens cannot be removed\n",
    "filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n",
    "oov_token = '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "BVV8LssUMUrY"
   },
   "outputs": [],
   "source": [
    "document_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token)\n",
    "summary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "0Sj_k9u8Mb1R"
   },
   "outputs": [],
   "source": [
    "document_tokenizer.fit_on_texts(document)\n",
    "summary_tokenizer.fit_on_texts(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "r9adUMBPMmlM"
   },
   "outputs": [],
   "source": [
    "inputs = document_tokenizer.texts_to_sequences(document)\n",
    "targets = summary_tokenizer.texts_to_sequences(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2AOW-f0HMqhc",
    "outputId": "8ed6daac-afff-4602-89eb-f4fdbbf85800"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[153, 547, 942, 379, 61, 2199, 548]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_tokenizer.texts_to_sequences([\"বাংলাদেশে কোচিং বানিজ্য বন্ধ এখন সময়ের দাবি\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NVk8QZJfMw9m",
    "outputId": "f9f4b5f8-7b0d-45b3-90ea-57cf78e2478b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['বাংলাদেশে কোচিং বানিজ্য বন্ধ এখন সময়ের দাবি']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_tokenizer.sequences_to_texts([[153, 547, 942, 379, 61, 134, 548]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pHL2dMQmM9mo",
    "outputId": "018c9090-9012-4cfe-fa48-0e0e7ab019dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23995, 2814)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_vocab_size = len(document_tokenizer.word_index) + 1\n",
    "decoder_vocab_size = len(summary_tokenizer.word_index) + 1\n",
    "\n",
    "encoder_vocab_size, decoder_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "dfaZi7eVNBXi"
   },
   "outputs": [],
   "source": [
    "document_lengths = pd.Series([len(x) for x in document])\n",
    "summary_lengths = pd.Series([len(x) for x in summary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r7dSrFUNNDgC",
    "outputId": "84a7fe3c-a21f-4bf7-817b-733372e7a2ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1026.000000\n",
       "mean      814.735867\n",
       "std       893.202858\n",
       "min        42.000000\n",
       "25%       257.500000\n",
       "50%       524.000000\n",
       "75%      1029.250000\n",
       "max      8262.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jOpgw0ZGNFzV",
    "outputId": "1ba9f947-9ff3-4e2e-8acc-def3943059a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1026.000000\n",
       "mean       54.670565\n",
       "std        23.110480\n",
       "min        15.000000\n",
       "25%        41.000000\n",
       "50%        51.000000\n",
       "75%        63.000000\n",
       "max       462.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "8juPbc01NIr9"
   },
   "outputs": [],
   "source": [
    "encoder_maxlen = 250\n",
    "decoder_maxlen = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ItReQzKWNMFg"
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
    "targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=decoder_maxlen, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "xG0C_tklNPwy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 16:49:52.890894: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-01 16:49:55.581819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15401 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.cast(inputs, dtype=tf.int32)\n",
    "targets = tf.cast(targets, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "89rZHD26NSgF"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 2000\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "CoWgTzpKNV94"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "F6b0UxwHNZSf"
   },
   "outputs": [],
   "source": [
    "def get_angles(position, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return position * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "HQVUljkTNbuN"
   },
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(\n",
    "        np.arange(position)[:, np.newaxis],\n",
    "        np.arange(d_model)[np.newaxis, :],\n",
    "        d_model\n",
    "    )\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "hW2wInusNeLI"
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "6Xq1sTtjNgyi"
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "c9-KePeRNjN4"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "ZA7drIKONlcK"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "        output = self.dense(concat_attention)\n",
    "            \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "G3mHNuHLNoiU"
   },
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),\n",
    "        tf.keras.layers.Dense(d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "5lrg_xJ9N4tx"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "FKaLacvSN7w7"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "dQrnlVMWOBuF"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "    \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "DGC5vXJeOHMh"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "    \n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "ZFTmtmN5OJVM"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
    "\n",
    "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "Hrcb29sJOMR1"
   },
   "outputs": [],
   "source": [
    "# hyper-params\n",
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "EPOCHS = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "GruvDrXiOPmT"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "mITWV3C8OR3S"
   },
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "lXhjR4nzOUVe"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "9rGOkAf3OWzs"
   },
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "37hPdmtAOY_o"
   },
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "uERVOad5OdOV"
   },
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers, \n",
    "    d_model, \n",
    "    num_heads, \n",
    "    dff,\n",
    "    encoder_vocab_size, \n",
    "    decoder_vocab_size, \n",
    "    pe_input=encoder_vocab_size, \n",
    "    pe_target=decoder_vocab_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "JzpgMP4yOgRE"
   },
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W7BPPE7QOipH",
    "outputId": "cc12b3f8-e959-41ba-b1b0-c342d9e29850"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"checkpoints\"\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "YZitSb2COlBR"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(\n",
    "            inp, tar_inp, \n",
    "            True, \n",
    "            enc_padding_mask, \n",
    "            combined_mask, \n",
    "            dec_padding_mask\n",
    "        )\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y3Hz5FbaOnhr",
    "outputId": "47ee5980-e55d-4cef-9f6c-107ec3f02f2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 7.9639\n",
      "Epoch 1 Batch 8 Loss 8.0156\n",
      "Epoch 1 Batch 16 Loss 8.0121\n",
      "Epoch 1 Batch 24 Loss 7.9973\n",
      "Epoch 1 Batch 32 Loss 7.9767\n",
      "Epoch 1 Batch 40 Loss 7.9560\n",
      "Epoch 1 Batch 48 Loss 7.9348\n",
      "Epoch 1 Batch 56 Loss 7.9146\n",
      "Epoch 1 Batch 64 Loss 7.8953\n",
      "Epoch 1 Batch 72 Loss 7.8716\n",
      "Epoch 1 Batch 80 Loss 7.8506\n",
      "Epoch 1 Batch 88 Loss 7.8364\n",
      "Epoch 1 Batch 96 Loss 7.8189\n",
      "Epoch 1 Batch 104 Loss 7.8036\n",
      "Epoch 1 Batch 112 Loss 7.7889\n",
      "Epoch 1 Batch 120 Loss 7.7770\n",
      "Epoch 1 Batch 128 Loss 7.7659\n",
      "Epoch 1 Loss 7.7659\n",
      "Time taken for 1 epoch: 19.33117413520813 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 7.4778\n",
      "Epoch 2 Batch 8 Loss 7.4988\n",
      "Epoch 2 Batch 16 Loss 7.4961\n",
      "Epoch 2 Batch 24 Loss 7.4868\n",
      "Epoch 2 Batch 32 Loss 7.4723\n",
      "Epoch 2 Batch 40 Loss 7.4677\n",
      "Epoch 2 Batch 48 Loss 7.4553\n",
      "Epoch 2 Batch 56 Loss 7.4376\n",
      "Epoch 2 Batch 64 Loss 7.4261\n",
      "Epoch 2 Batch 72 Loss 7.4213\n",
      "Epoch 2 Batch 80 Loss 7.4163\n",
      "Epoch 2 Batch 88 Loss 7.4112\n",
      "Epoch 2 Batch 96 Loss 7.3975\n",
      "Epoch 2 Batch 104 Loss 7.3852\n",
      "Epoch 2 Batch 112 Loss 7.3763\n",
      "Epoch 2 Batch 120 Loss 7.3708\n",
      "Epoch 2 Batch 128 Loss 7.3617\n",
      "Epoch 2 Loss 7.3617\n",
      "Time taken for 1 epoch: 3.1325502395629883 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 7.2026\n",
      "Epoch 3 Batch 8 Loss 7.1369\n",
      "Epoch 3 Batch 16 Loss 7.1232\n",
      "Epoch 3 Batch 24 Loss 7.1262\n",
      "Epoch 3 Batch 32 Loss 7.1169\n",
      "Epoch 3 Batch 40 Loss 7.1056\n",
      "Epoch 3 Batch 48 Loss 7.0822\n",
      "Epoch 3 Batch 56 Loss 7.0621\n",
      "Epoch 3 Batch 64 Loss 7.0579\n",
      "Epoch 3 Batch 72 Loss 7.0386\n",
      "Epoch 3 Batch 80 Loss 7.0322\n",
      "Epoch 3 Batch 88 Loss 7.0232\n",
      "Epoch 3 Batch 96 Loss 7.0197\n",
      "Epoch 3 Batch 104 Loss 7.0112\n",
      "Epoch 3 Batch 112 Loss 7.0020\n",
      "Epoch 3 Batch 120 Loss 6.9938\n",
      "Epoch 3 Batch 128 Loss 6.9911\n",
      "Epoch 3 Loss 6.9911\n",
      "Time taken for 1 epoch: 3.1224231719970703 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 6.6255\n",
      "Epoch 4 Batch 8 Loss 6.7409\n",
      "Epoch 4 Batch 16 Loss 6.7439\n",
      "Epoch 4 Batch 24 Loss 6.7368\n",
      "Epoch 4 Batch 32 Loss 6.7445\n",
      "Epoch 4 Batch 40 Loss 6.7348\n",
      "Epoch 4 Batch 48 Loss 6.7495\n",
      "Epoch 4 Batch 56 Loss 6.7330\n",
      "Epoch 4 Batch 64 Loss 6.7224\n",
      "Epoch 4 Batch 72 Loss 6.7218\n",
      "Epoch 4 Batch 80 Loss 6.7246\n",
      "Epoch 4 Batch 88 Loss 6.7221\n",
      "Epoch 4 Batch 96 Loss 6.7107\n",
      "Epoch 4 Batch 104 Loss 6.7109\n",
      "Epoch 4 Batch 112 Loss 6.7163\n",
      "Epoch 4 Batch 120 Loss 6.7165\n",
      "Epoch 4 Batch 128 Loss 6.7129\n",
      "Epoch 4 Loss 6.7129\n",
      "Time taken for 1 epoch: 3.128861904144287 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 6.6397\n",
      "Epoch 5 Batch 8 Loss 6.6154\n",
      "Epoch 5 Batch 16 Loss 6.6041\n",
      "Epoch 5 Batch 24 Loss 6.5778\n",
      "Epoch 5 Batch 32 Loss 6.5817\n",
      "Epoch 5 Batch 40 Loss 6.5775\n",
      "Epoch 5 Batch 48 Loss 6.5711\n",
      "Epoch 5 Batch 56 Loss 6.5677\n",
      "Epoch 5 Batch 64 Loss 6.5585\n",
      "Epoch 5 Batch 72 Loss 6.5540\n",
      "Epoch 5 Batch 80 Loss 6.5421\n",
      "Epoch 5 Batch 88 Loss 6.5515\n",
      "Epoch 5 Batch 96 Loss 6.5636\n",
      "Epoch 5 Batch 104 Loss 6.5595\n",
      "Epoch 5 Batch 112 Loss 6.5672\n",
      "Epoch 5 Batch 120 Loss 6.5771\n",
      "Epoch 5 Batch 128 Loss 6.5815\n",
      "Saving checkpoint for epoch 5 at checkpoints/ckpt-1\n",
      "Epoch 5 Loss 6.5815\n",
      "Time taken for 1 epoch: 3.428187847137451 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 6.3199\n",
      "Epoch 6 Batch 8 Loss 6.3453\n",
      "Epoch 6 Batch 16 Loss 6.3725\n",
      "Epoch 6 Batch 24 Loss 6.4195\n",
      "Epoch 6 Batch 32 Loss 6.4294\n",
      "Epoch 6 Batch 40 Loss 6.4214\n",
      "Epoch 6 Batch 48 Loss 6.4113\n",
      "Epoch 6 Batch 56 Loss 6.4200\n",
      "Epoch 6 Batch 64 Loss 6.4196\n",
      "Epoch 6 Batch 72 Loss 6.4218\n",
      "Epoch 6 Batch 80 Loss 6.4368\n",
      "Epoch 6 Batch 88 Loss 6.4509\n",
      "Epoch 6 Batch 96 Loss 6.4538\n",
      "Epoch 6 Batch 104 Loss 6.4533\n",
      "Epoch 6 Batch 112 Loss 6.4475\n",
      "Epoch 6 Batch 120 Loss 6.4578\n",
      "Epoch 6 Batch 128 Loss 6.4580\n",
      "Epoch 6 Loss 6.4580\n",
      "Time taken for 1 epoch: 3.1340749263763428 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 6.0601\n",
      "Epoch 7 Batch 8 Loss 6.2777\n",
      "Epoch 7 Batch 16 Loss 6.3525\n",
      "Epoch 7 Batch 24 Loss 6.2772\n",
      "Epoch 7 Batch 32 Loss 6.3200\n",
      "Epoch 7 Batch 40 Loss 6.3014\n",
      "Epoch 7 Batch 48 Loss 6.2882\n",
      "Epoch 7 Batch 56 Loss 6.2823\n",
      "Epoch 7 Batch 64 Loss 6.2832\n",
      "Epoch 7 Batch 72 Loss 6.2802\n",
      "Epoch 7 Batch 80 Loss 6.2663\n",
      "Epoch 7 Batch 88 Loss 6.2771\n",
      "Epoch 7 Batch 96 Loss 6.2815\n",
      "Epoch 7 Batch 104 Loss 6.2810\n",
      "Epoch 7 Batch 112 Loss 6.2796\n",
      "Epoch 7 Batch 120 Loss 6.2903\n",
      "Epoch 7 Batch 128 Loss 6.2990\n",
      "Epoch 7 Loss 6.2990\n",
      "Time taken for 1 epoch: 3.1141276359558105 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 6.0612\n",
      "Epoch 8 Batch 8 Loss 6.1053\n",
      "Epoch 8 Batch 16 Loss 6.1278\n",
      "Epoch 8 Batch 24 Loss 6.1369\n",
      "Epoch 8 Batch 32 Loss 6.1143\n",
      "Epoch 8 Batch 40 Loss 6.1034\n",
      "Epoch 8 Batch 48 Loss 6.0852\n",
      "Epoch 8 Batch 56 Loss 6.0931\n",
      "Epoch 8 Batch 64 Loss 6.0912\n",
      "Epoch 8 Batch 72 Loss 6.1007\n",
      "Epoch 8 Batch 80 Loss 6.1116\n",
      "Epoch 8 Batch 88 Loss 6.1070\n",
      "Epoch 8 Batch 96 Loss 6.1074\n",
      "Epoch 8 Batch 104 Loss 6.1140\n",
      "Epoch 8 Batch 112 Loss 6.1153\n",
      "Epoch 8 Batch 120 Loss 6.1074\n",
      "Epoch 8 Batch 128 Loss 6.1258\n",
      "Epoch 8 Loss 6.1258\n",
      "Time taken for 1 epoch: 3.1298341751098633 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 5.6701\n",
      "Epoch 9 Batch 8 Loss 6.0341\n",
      "Epoch 9 Batch 16 Loss 5.9493\n",
      "Epoch 9 Batch 24 Loss 5.9285\n",
      "Epoch 9 Batch 32 Loss 5.9261\n",
      "Epoch 9 Batch 40 Loss 5.9231\n",
      "Epoch 9 Batch 48 Loss 5.9172\n",
      "Epoch 9 Batch 56 Loss 5.8963\n",
      "Epoch 9 Batch 64 Loss 5.9081\n",
      "Epoch 9 Batch 72 Loss 5.9102\n",
      "Epoch 9 Batch 80 Loss 5.9194\n",
      "Epoch 9 Batch 88 Loss 5.9199\n",
      "Epoch 9 Batch 96 Loss 5.9251\n",
      "Epoch 9 Batch 104 Loss 5.9193\n",
      "Epoch 9 Batch 112 Loss 5.9239\n",
      "Epoch 9 Batch 120 Loss 5.9201\n",
      "Epoch 9 Batch 128 Loss 5.9218\n",
      "Epoch 9 Loss 5.9218\n",
      "Time taken for 1 epoch: 3.111985206604004 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 5.7757\n",
      "Epoch 10 Batch 8 Loss 5.6690\n",
      "Epoch 10 Batch 16 Loss 5.6480\n",
      "Epoch 10 Batch 24 Loss 5.6897\n",
      "Epoch 10 Batch 32 Loss 5.7024\n",
      "Epoch 10 Batch 40 Loss 5.7169\n",
      "Epoch 10 Batch 48 Loss 5.7119\n",
      "Epoch 10 Batch 56 Loss 5.6901\n",
      "Epoch 10 Batch 64 Loss 5.6861\n",
      "Epoch 10 Batch 72 Loss 5.7142\n",
      "Epoch 10 Batch 80 Loss 5.7154\n",
      "Epoch 10 Batch 88 Loss 5.7025\n",
      "Epoch 10 Batch 96 Loss 5.7086\n",
      "Epoch 10 Batch 104 Loss 5.7179\n",
      "Epoch 10 Batch 112 Loss 5.7335\n",
      "Epoch 10 Batch 120 Loss 5.7431\n",
      "Epoch 10 Batch 128 Loss 5.7462\n",
      "Saving checkpoint for epoch 10 at checkpoints/ckpt-2\n",
      "Epoch 10 Loss 5.7462\n",
      "Time taken for 1 epoch: 3.400040626525879 secs\n",
      "\n",
      "Epoch 11 Batch 0 Loss 5.8554\n",
      "Epoch 11 Batch 8 Loss 5.5966\n",
      "Epoch 11 Batch 16 Loss 5.5162\n",
      "Epoch 11 Batch 24 Loss 5.4787\n",
      "Epoch 11 Batch 32 Loss 5.5115\n",
      "Epoch 11 Batch 40 Loss 5.5333\n",
      "Epoch 11 Batch 48 Loss 5.5370\n",
      "Epoch 11 Batch 56 Loss 5.5479\n",
      "Epoch 11 Batch 64 Loss 5.5482\n",
      "Epoch 11 Batch 72 Loss 5.5450\n",
      "Epoch 11 Batch 80 Loss 5.5544\n",
      "Epoch 11 Batch 88 Loss 5.5609\n",
      "Epoch 11 Batch 96 Loss 5.5768\n",
      "Epoch 11 Batch 104 Loss 5.5739\n",
      "Epoch 11 Batch 112 Loss 5.5804\n",
      "Epoch 11 Batch 120 Loss 5.5723\n",
      "Epoch 11 Batch 128 Loss 5.5696\n",
      "Epoch 11 Loss 5.5696\n",
      "Time taken for 1 epoch: 3.1188786029815674 secs\n",
      "\n",
      "Epoch 12 Batch 0 Loss 5.5456\n",
      "Epoch 12 Batch 8 Loss 5.3667\n",
      "Epoch 12 Batch 16 Loss 5.3648\n",
      "Epoch 12 Batch 24 Loss 5.3526\n",
      "Epoch 12 Batch 32 Loss 5.3474\n",
      "Epoch 12 Batch 40 Loss 5.3365\n",
      "Epoch 12 Batch 48 Loss 5.3220\n",
      "Epoch 12 Batch 56 Loss 5.3217\n",
      "Epoch 12 Batch 64 Loss 5.3223\n",
      "Epoch 12 Batch 72 Loss 5.3125\n",
      "Epoch 12 Batch 80 Loss 5.3248\n",
      "Epoch 12 Batch 88 Loss 5.3357\n",
      "Epoch 12 Batch 96 Loss 5.3425\n",
      "Epoch 12 Batch 104 Loss 5.3494\n",
      "Epoch 12 Batch 112 Loss 5.3518\n",
      "Epoch 12 Batch 120 Loss 5.3524\n",
      "Epoch 12 Batch 128 Loss 5.3561\n",
      "Epoch 12 Loss 5.3561\n",
      "Time taken for 1 epoch: 3.120049476623535 secs\n",
      "\n",
      "Epoch 13 Batch 0 Loss 5.0061\n",
      "Epoch 13 Batch 8 Loss 5.2311\n",
      "Epoch 13 Batch 16 Loss 5.2152\n",
      "Epoch 13 Batch 24 Loss 5.1565\n",
      "Epoch 13 Batch 32 Loss 5.1794\n",
      "Epoch 13 Batch 40 Loss 5.1801\n",
      "Epoch 13 Batch 48 Loss 5.1785\n",
      "Epoch 13 Batch 56 Loss 5.1937\n",
      "Epoch 13 Batch 64 Loss 5.2106\n",
      "Epoch 13 Batch 72 Loss 5.1996\n",
      "Epoch 13 Batch 80 Loss 5.1927\n",
      "Epoch 13 Batch 88 Loss 5.1821\n",
      "Epoch 13 Batch 96 Loss 5.1807\n",
      "Epoch 13 Batch 104 Loss 5.1847\n",
      "Epoch 13 Batch 112 Loss 5.1863\n",
      "Epoch 13 Batch 120 Loss 5.1856\n",
      "Epoch 13 Batch 128 Loss 5.1748\n",
      "Epoch 13 Loss 5.1748\n",
      "Time taken for 1 epoch: 3.1169252395629883 secs\n",
      "\n",
      "Epoch 14 Batch 0 Loss 5.1319\n",
      "Epoch 14 Batch 8 Loss 4.9530\n",
      "Epoch 14 Batch 16 Loss 4.9397\n",
      "Epoch 14 Batch 24 Loss 4.8903\n",
      "Epoch 14 Batch 32 Loss 4.8339\n",
      "Epoch 14 Batch 40 Loss 4.8425\n",
      "Epoch 14 Batch 48 Loss 4.8788\n",
      "Epoch 14 Batch 56 Loss 4.8993\n",
      "Epoch 14 Batch 64 Loss 4.8941\n",
      "Epoch 14 Batch 72 Loss 4.9006\n",
      "Epoch 14 Batch 80 Loss 4.8978\n",
      "Epoch 14 Batch 88 Loss 4.9091\n",
      "Epoch 14 Batch 96 Loss 4.9199\n",
      "Epoch 14 Batch 104 Loss 4.9318\n",
      "Epoch 14 Batch 112 Loss 4.9162\n",
      "Epoch 14 Batch 120 Loss 4.9188\n",
      "Epoch 14 Batch 128 Loss 4.9317\n",
      "Epoch 14 Loss 4.9317\n",
      "Time taken for 1 epoch: 3.115736722946167 secs\n",
      "\n",
      "Epoch 15 Batch 0 Loss 4.5094\n",
      "Epoch 15 Batch 8 Loss 4.6033\n",
      "Epoch 15 Batch 16 Loss 4.6871\n",
      "Epoch 15 Batch 24 Loss 4.6506\n",
      "Epoch 15 Batch 32 Loss 4.6467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Batch 40 Loss 4.6717\n",
      "Epoch 15 Batch 48 Loss 4.7003\n",
      "Epoch 15 Batch 56 Loss 4.6948\n",
      "Epoch 15 Batch 64 Loss 4.7162\n",
      "Epoch 15 Batch 72 Loss 4.7177\n",
      "Epoch 15 Batch 80 Loss 4.7347\n",
      "Epoch 15 Batch 88 Loss 4.7377\n",
      "Epoch 15 Batch 96 Loss 4.7347\n",
      "Epoch 15 Batch 104 Loss 4.7302\n",
      "Epoch 15 Batch 112 Loss 4.7307\n",
      "Epoch 15 Batch 120 Loss 4.7225\n",
      "Epoch 15 Batch 128 Loss 4.7259\n",
      "Saving checkpoint for epoch 15 at checkpoints/ckpt-3\n",
      "Epoch 15 Loss 4.7259\n",
      "Time taken for 1 epoch: 3.3953168392181396 secs\n",
      "\n",
      "Epoch 16 Batch 0 Loss 4.5871\n",
      "Epoch 16 Batch 8 Loss 4.3412\n",
      "Epoch 16 Batch 16 Loss 4.3444\n",
      "Epoch 16 Batch 24 Loss 4.2926\n",
      "Epoch 16 Batch 32 Loss 4.3467\n",
      "Epoch 16 Batch 40 Loss 4.3480\n",
      "Epoch 16 Batch 48 Loss 4.3826\n",
      "Epoch 16 Batch 56 Loss 4.3860\n",
      "Epoch 16 Batch 64 Loss 4.4025\n",
      "Epoch 16 Batch 72 Loss 4.4270\n",
      "Epoch 16 Batch 80 Loss 4.4354\n",
      "Epoch 16 Batch 88 Loss 4.4460\n",
      "Epoch 16 Batch 96 Loss 4.4561\n",
      "Epoch 16 Batch 104 Loss 4.4603\n",
      "Epoch 16 Batch 112 Loss 4.4706\n",
      "Epoch 16 Batch 120 Loss 4.4679\n",
      "Epoch 16 Batch 128 Loss 4.4822\n",
      "Epoch 16 Loss 4.4822\n",
      "Time taken for 1 epoch: 3.1181280612945557 secs\n",
      "\n",
      "Epoch 17 Batch 0 Loss 4.4512\n",
      "Epoch 17 Batch 8 Loss 4.0971\n",
      "Epoch 17 Batch 16 Loss 4.0629\n",
      "Epoch 17 Batch 24 Loss 4.0896\n",
      "Epoch 17 Batch 32 Loss 4.0988\n",
      "Epoch 17 Batch 40 Loss 4.1231\n",
      "Epoch 17 Batch 48 Loss 4.1584\n",
      "Epoch 17 Batch 56 Loss 4.2235\n",
      "Epoch 17 Batch 64 Loss 4.2154\n",
      "Epoch 17 Batch 72 Loss 4.2326\n",
      "Epoch 17 Batch 80 Loss 4.2435\n",
      "Epoch 17 Batch 88 Loss 4.2541\n",
      "Epoch 17 Batch 96 Loss 4.2494\n",
      "Epoch 17 Batch 104 Loss 4.2519\n",
      "Epoch 17 Batch 112 Loss 4.2441\n",
      "Epoch 17 Batch 120 Loss 4.2520\n",
      "Epoch 17 Batch 128 Loss 4.2518\n",
      "Epoch 17 Loss 4.2518\n",
      "Time taken for 1 epoch: 3.1285459995269775 secs\n",
      "\n",
      "Epoch 18 Batch 0 Loss 3.8904\n",
      "Epoch 18 Batch 8 Loss 3.9827\n",
      "Epoch 18 Batch 16 Loss 3.9649\n",
      "Epoch 18 Batch 24 Loss 3.9465\n",
      "Epoch 18 Batch 32 Loss 3.9408\n",
      "Epoch 18 Batch 40 Loss 3.9380\n",
      "Epoch 18 Batch 48 Loss 3.9368\n",
      "Epoch 18 Batch 56 Loss 3.9273\n",
      "Epoch 18 Batch 64 Loss 3.9046\n",
      "Epoch 18 Batch 72 Loss 3.9010\n",
      "Epoch 18 Batch 80 Loss 3.9179\n",
      "Epoch 18 Batch 88 Loss 3.9317\n",
      "Epoch 18 Batch 96 Loss 3.9403\n",
      "Epoch 18 Batch 104 Loss 3.9406\n",
      "Epoch 18 Batch 112 Loss 3.9531\n",
      "Epoch 18 Batch 120 Loss 3.9631\n",
      "Epoch 18 Batch 128 Loss 3.9666\n",
      "Epoch 18 Loss 3.9666\n",
      "Time taken for 1 epoch: 3.1182897090911865 secs\n",
      "\n",
      "Epoch 19 Batch 0 Loss 3.3758\n",
      "Epoch 19 Batch 8 Loss 3.5657\n",
      "Epoch 19 Batch 16 Loss 3.6271\n",
      "Epoch 19 Batch 24 Loss 3.6200\n",
      "Epoch 19 Batch 32 Loss 3.6125\n",
      "Epoch 19 Batch 40 Loss 3.6152\n",
      "Epoch 19 Batch 48 Loss 3.6056\n",
      "Epoch 19 Batch 56 Loss 3.5867\n",
      "Epoch 19 Batch 64 Loss 3.6115\n",
      "Epoch 19 Batch 72 Loss 3.6138\n",
      "Epoch 19 Batch 80 Loss 3.6247\n",
      "Epoch 19 Batch 88 Loss 3.6286\n",
      "Epoch 19 Batch 96 Loss 3.6213\n",
      "Epoch 19 Batch 104 Loss 3.6416\n",
      "Epoch 19 Batch 112 Loss 3.6382\n",
      "Epoch 19 Batch 120 Loss 3.6603\n",
      "Epoch 19 Batch 128 Loss 3.6785\n",
      "Epoch 19 Loss 3.6785\n",
      "Time taken for 1 epoch: 3.117262125015259 secs\n",
      "\n",
      "Epoch 20 Batch 0 Loss 3.9607\n",
      "Epoch 20 Batch 8 Loss 3.3408\n",
      "Epoch 20 Batch 16 Loss 3.3867\n",
      "Epoch 20 Batch 24 Loss 3.3763\n",
      "Epoch 20 Batch 32 Loss 3.3468\n",
      "Epoch 20 Batch 40 Loss 3.3531\n",
      "Epoch 20 Batch 48 Loss 3.3523\n",
      "Epoch 20 Batch 56 Loss 3.3827\n",
      "Epoch 20 Batch 64 Loss 3.3846\n",
      "Epoch 20 Batch 72 Loss 3.3991\n",
      "Epoch 20 Batch 80 Loss 3.4029\n",
      "Epoch 20 Batch 88 Loss 3.4125\n",
      "Epoch 20 Batch 96 Loss 3.4175\n",
      "Epoch 20 Batch 104 Loss 3.4233\n",
      "Epoch 20 Batch 112 Loss 3.4221\n",
      "Epoch 20 Batch 120 Loss 3.4239\n",
      "Epoch 20 Batch 128 Loss 3.4248\n",
      "Saving checkpoint for epoch 20 at checkpoints/ckpt-4\n",
      "Epoch 20 Loss 3.4248\n",
      "Time taken for 1 epoch: 3.4349911212921143 secs\n",
      "\n",
      "Epoch 21 Batch 0 Loss 3.2388\n",
      "Epoch 21 Batch 8 Loss 3.2662\n",
      "Epoch 21 Batch 16 Loss 3.1016\n",
      "Epoch 21 Batch 24 Loss 3.0243\n",
      "Epoch 21 Batch 32 Loss 3.0000\n",
      "Epoch 21 Batch 40 Loss 2.9705\n",
      "Epoch 21 Batch 48 Loss 2.9611\n",
      "Epoch 21 Batch 56 Loss 2.9699\n",
      "Epoch 21 Batch 64 Loss 2.9673\n",
      "Epoch 21 Batch 72 Loss 3.0157\n",
      "Epoch 21 Batch 80 Loss 3.0404\n",
      "Epoch 21 Batch 88 Loss 3.0462\n",
      "Epoch 21 Batch 96 Loss 3.0570\n",
      "Epoch 21 Batch 104 Loss 3.0656\n",
      "Epoch 21 Batch 112 Loss 3.0760\n",
      "Epoch 21 Batch 120 Loss 3.0759\n",
      "Epoch 21 Batch 128 Loss 3.0902\n",
      "Epoch 21 Loss 3.0902\n",
      "Time taken for 1 epoch: 3.120601177215576 secs\n",
      "\n",
      "Epoch 22 Batch 0 Loss 2.6458\n",
      "Epoch 22 Batch 8 Loss 2.7358\n",
      "Epoch 22 Batch 16 Loss 2.6983\n",
      "Epoch 22 Batch 24 Loss 2.6577\n",
      "Epoch 22 Batch 32 Loss 2.7240\n",
      "Epoch 22 Batch 40 Loss 2.7426\n",
      "Epoch 22 Batch 48 Loss 2.7287\n",
      "Epoch 22 Batch 56 Loss 2.7303\n",
      "Epoch 22 Batch 64 Loss 2.7198\n",
      "Epoch 22 Batch 72 Loss 2.7198\n",
      "Epoch 22 Batch 80 Loss 2.7155\n",
      "Epoch 22 Batch 88 Loss 2.7307\n",
      "Epoch 22 Batch 96 Loss 2.7534\n",
      "Epoch 22 Batch 104 Loss 2.7639\n",
      "Epoch 22 Batch 112 Loss 2.7820\n",
      "Epoch 22 Batch 120 Loss 2.7939\n",
      "Epoch 22 Batch 128 Loss 2.7982\n",
      "Epoch 22 Loss 2.7982\n",
      "Time taken for 1 epoch: 3.118561029434204 secs\n",
      "\n",
      "Epoch 23 Batch 0 Loss 2.0895\n",
      "Epoch 23 Batch 8 Loss 2.4575\n",
      "Epoch 23 Batch 16 Loss 2.4747\n",
      "Epoch 23 Batch 24 Loss 2.4852\n",
      "Epoch 23 Batch 32 Loss 2.4866\n",
      "Epoch 23 Batch 40 Loss 2.4866\n",
      "Epoch 23 Batch 48 Loss 2.4818\n",
      "Epoch 23 Batch 56 Loss 2.4969\n",
      "Epoch 23 Batch 64 Loss 2.5167\n",
      "Epoch 23 Batch 72 Loss 2.5269\n",
      "Epoch 23 Batch 80 Loss 2.5272\n",
      "Epoch 23 Batch 88 Loss 2.5406\n",
      "Epoch 23 Batch 96 Loss 2.5351\n",
      "Epoch 23 Batch 104 Loss 2.5347\n",
      "Epoch 23 Batch 112 Loss 2.5372\n",
      "Epoch 23 Batch 120 Loss 2.5385\n",
      "Epoch 23 Batch 128 Loss 2.5604\n",
      "Epoch 23 Loss 2.5604\n",
      "Time taken for 1 epoch: 3.1237897872924805 secs\n",
      "\n",
      "Epoch 24 Batch 0 Loss 1.8981\n",
      "Epoch 24 Batch 8 Loss 2.1987\n",
      "Epoch 24 Batch 16 Loss 2.2138\n",
      "Epoch 24 Batch 24 Loss 2.2140\n",
      "Epoch 24 Batch 32 Loss 2.1885\n",
      "Epoch 24 Batch 40 Loss 2.1474\n",
      "Epoch 24 Batch 48 Loss 2.1553\n",
      "Epoch 24 Batch 56 Loss 2.1839\n",
      "Epoch 24 Batch 64 Loss 2.1852\n",
      "Epoch 24 Batch 72 Loss 2.1947\n",
      "Epoch 24 Batch 80 Loss 2.2100\n",
      "Epoch 24 Batch 88 Loss 2.2315\n",
      "Epoch 24 Batch 96 Loss 2.2283\n",
      "Epoch 24 Batch 104 Loss 2.2400\n",
      "Epoch 24 Batch 112 Loss 2.2506\n",
      "Epoch 24 Batch 120 Loss 2.2529\n",
      "Epoch 24 Batch 128 Loss 2.2437\n",
      "Epoch 24 Loss 2.2437\n",
      "Time taken for 1 epoch: 3.134099245071411 secs\n",
      "\n",
      "Epoch 25 Batch 0 Loss 1.5946\n",
      "Epoch 25 Batch 8 Loss 1.7319\n",
      "Epoch 25 Batch 16 Loss 1.7570\n",
      "Epoch 25 Batch 24 Loss 1.7787\n",
      "Epoch 25 Batch 32 Loss 1.7761\n",
      "Epoch 25 Batch 40 Loss 1.7693\n",
      "Epoch 25 Batch 48 Loss 1.7800\n",
      "Epoch 25 Batch 56 Loss 1.8047\n",
      "Epoch 25 Batch 64 Loss 1.8389\n",
      "Epoch 25 Batch 72 Loss 1.8494\n",
      "Epoch 25 Batch 80 Loss 1.8660\n",
      "Epoch 25 Batch 88 Loss 1.8964\n",
      "Epoch 25 Batch 96 Loss 1.9233\n",
      "Epoch 25 Batch 104 Loss 1.9422\n",
      "Epoch 25 Batch 112 Loss 1.9421\n",
      "Epoch 25 Batch 120 Loss 1.9487\n",
      "Epoch 25 Batch 128 Loss 1.9603\n",
      "Saving checkpoint for epoch 25 at checkpoints/ckpt-5\n",
      "Epoch 25 Loss 1.9603\n",
      "Time taken for 1 epoch: 3.408806085586548 secs\n",
      "\n",
      "Epoch 26 Batch 0 Loss 1.8753\n",
      "Epoch 26 Batch 8 Loss 1.6716\n",
      "Epoch 26 Batch 16 Loss 1.6578\n",
      "Epoch 26 Batch 24 Loss 1.6053\n",
      "Epoch 26 Batch 32 Loss 1.5906\n",
      "Epoch 26 Batch 40 Loss 1.6225\n",
      "Epoch 26 Batch 48 Loss 1.6180\n",
      "Epoch 26 Batch 56 Loss 1.6306\n",
      "Epoch 26 Batch 64 Loss 1.6403\n",
      "Epoch 26 Batch 72 Loss 1.6743\n",
      "Epoch 26 Batch 80 Loss 1.6831\n",
      "Epoch 26 Batch 88 Loss 1.6942\n",
      "Epoch 26 Batch 96 Loss 1.6991\n",
      "Epoch 26 Batch 104 Loss 1.7225\n",
      "Epoch 26 Batch 112 Loss 1.7248\n",
      "Epoch 26 Batch 120 Loss 1.7347\n",
      "Epoch 26 Batch 128 Loss 1.7392\n",
      "Epoch 26 Loss 1.7392\n",
      "Time taken for 1 epoch: 3.1208980083465576 secs\n",
      "\n",
      "Epoch 27 Batch 0 Loss 1.0641\n",
      "Epoch 27 Batch 8 Loss 1.2687\n",
      "Epoch 27 Batch 16 Loss 1.2165\n",
      "Epoch 27 Batch 24 Loss 1.2591\n",
      "Epoch 27 Batch 32 Loss 1.2857\n",
      "Epoch 27 Batch 40 Loss 1.2925\n",
      "Epoch 27 Batch 48 Loss 1.3235\n",
      "Epoch 27 Batch 56 Loss 1.3456\n",
      "Epoch 27 Batch 64 Loss 1.3554\n",
      "Epoch 27 Batch 72 Loss 1.3742\n",
      "Epoch 27 Batch 80 Loss 1.3836\n",
      "Epoch 27 Batch 88 Loss 1.4048\n",
      "Epoch 27 Batch 96 Loss 1.4219\n",
      "Epoch 27 Batch 104 Loss 1.4423\n",
      "Epoch 27 Batch 112 Loss 1.4561\n",
      "Epoch 27 Batch 120 Loss 1.4686\n",
      "Epoch 27 Batch 128 Loss 1.4730\n",
      "Epoch 27 Loss 1.4730\n",
      "Time taken for 1 epoch: 3.1231067180633545 secs\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.8913\n",
      "Epoch 28 Batch 8 Loss 1.1247\n",
      "Epoch 28 Batch 16 Loss 1.1359\n",
      "Epoch 28 Batch 24 Loss 1.1238\n",
      "Epoch 28 Batch 32 Loss 1.0999\n",
      "Epoch 28 Batch 40 Loss 1.1045\n",
      "Epoch 28 Batch 48 Loss 1.1063\n",
      "Epoch 28 Batch 56 Loss 1.1220\n",
      "Epoch 28 Batch 64 Loss 1.1316\n",
      "Epoch 28 Batch 72 Loss 1.1479\n",
      "Epoch 28 Batch 80 Loss 1.1604\n",
      "Epoch 28 Batch 88 Loss 1.1765\n",
      "Epoch 28 Batch 96 Loss 1.1929\n",
      "Epoch 28 Batch 104 Loss 1.2265\n",
      "Epoch 28 Batch 112 Loss 1.2430\n",
      "Epoch 28 Batch 120 Loss 1.2540\n",
      "Epoch 28 Batch 128 Loss 1.2628\n",
      "Epoch 28 Loss 1.2628\n",
      "Time taken for 1 epoch: 3.1186299324035645 secs\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.9495\n",
      "Epoch 29 Batch 8 Loss 0.9867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 Batch 16 Loss 0.9647\n",
      "Epoch 29 Batch 24 Loss 0.9520\n",
      "Epoch 29 Batch 32 Loss 0.9234\n",
      "Epoch 29 Batch 40 Loss 0.9282\n",
      "Epoch 29 Batch 48 Loss 0.9495\n",
      "Epoch 29 Batch 56 Loss 0.9625\n",
      "Epoch 29 Batch 64 Loss 0.9796\n",
      "Epoch 29 Batch 72 Loss 0.9782\n",
      "Epoch 29 Batch 80 Loss 0.9932\n",
      "Epoch 29 Batch 88 Loss 1.0059\n",
      "Epoch 29 Batch 96 Loss 1.0195\n",
      "Epoch 29 Batch 104 Loss 1.0315\n",
      "Epoch 29 Batch 112 Loss 1.0407\n",
      "Epoch 29 Batch 120 Loss 1.0469\n",
      "Epoch 29 Batch 128 Loss 1.0609\n",
      "Epoch 29 Loss 1.0609\n",
      "Time taken for 1 epoch: 3.128767490386963 secs\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.7152\n",
      "Epoch 30 Batch 8 Loss 0.8583\n",
      "Epoch 30 Batch 16 Loss 0.8590\n",
      "Epoch 30 Batch 24 Loss 0.8309\n",
      "Epoch 30 Batch 32 Loss 0.7950\n",
      "Epoch 30 Batch 40 Loss 0.8109\n",
      "Epoch 30 Batch 48 Loss 0.8083\n",
      "Epoch 30 Batch 56 Loss 0.8153\n",
      "Epoch 30 Batch 64 Loss 0.8318\n",
      "Epoch 30 Batch 72 Loss 0.8407\n",
      "Epoch 30 Batch 80 Loss 0.8491\n",
      "Epoch 30 Batch 88 Loss 0.8654\n",
      "Epoch 30 Batch 96 Loss 0.8995\n",
      "Epoch 30 Batch 104 Loss 0.9125\n",
      "Epoch 30 Batch 112 Loss 0.9320\n",
      "Epoch 30 Batch 120 Loss 0.9422\n",
      "Epoch 30 Batch 128 Loss 0.9521\n",
      "Saving checkpoint for epoch 30 at checkpoints/ckpt-6\n",
      "Epoch 30 Loss 0.9521\n",
      "Time taken for 1 epoch: 3.44008731842041 secs\n",
      "\n",
      "Epoch 31 Batch 0 Loss 1.6975\n",
      "Epoch 31 Batch 8 Loss 0.8108\n",
      "Epoch 31 Batch 16 Loss 0.7498\n",
      "Epoch 31 Batch 24 Loss 0.7279\n",
      "Epoch 31 Batch 32 Loss 0.7436\n",
      "Epoch 31 Batch 40 Loss 0.7549\n",
      "Epoch 31 Batch 48 Loss 0.7719\n",
      "Epoch 31 Batch 56 Loss 0.7702\n",
      "Epoch 31 Batch 64 Loss 0.7801\n",
      "Epoch 31 Batch 72 Loss 0.7921\n",
      "Epoch 31 Batch 80 Loss 0.7966\n",
      "Epoch 31 Batch 88 Loss 0.8053\n",
      "Epoch 31 Batch 96 Loss 0.8120\n",
      "Epoch 31 Batch 104 Loss 0.8190\n",
      "Epoch 31 Batch 112 Loss 0.8308\n",
      "Epoch 31 Batch 120 Loss 0.8331\n",
      "Epoch 31 Batch 128 Loss 0.8537\n",
      "Epoch 31 Loss 0.8537\n",
      "Time taken for 1 epoch: 3.124124526977539 secs\n",
      "\n",
      "Epoch 32 Batch 0 Loss 0.4838\n",
      "Epoch 32 Batch 8 Loss 0.5599\n",
      "Epoch 32 Batch 16 Loss 0.6123\n",
      "Epoch 32 Batch 24 Loss 0.6212\n",
      "Epoch 32 Batch 32 Loss 0.6337\n",
      "Epoch 32 Batch 40 Loss 0.6429\n",
      "Epoch 32 Batch 48 Loss 0.6611\n",
      "Epoch 32 Batch 56 Loss 0.6721\n",
      "Epoch 32 Batch 64 Loss 0.6787\n",
      "Epoch 32 Batch 72 Loss 0.6898\n",
      "Epoch 32 Batch 80 Loss 0.7033\n",
      "Epoch 32 Batch 88 Loss 0.7175\n",
      "Epoch 32 Batch 96 Loss 0.7299\n",
      "Epoch 32 Batch 104 Loss 0.7397\n",
      "Epoch 32 Batch 112 Loss 0.7391\n",
      "Epoch 32 Batch 120 Loss 0.7413\n",
      "Epoch 32 Batch 128 Loss 0.7541\n",
      "Epoch 32 Loss 0.7541\n",
      "Time taken for 1 epoch: 3.114534616470337 secs\n",
      "\n",
      "Epoch 33 Batch 0 Loss 0.5014\n",
      "Epoch 33 Batch 8 Loss 0.5564\n",
      "Epoch 33 Batch 16 Loss 0.6131\n",
      "Epoch 33 Batch 24 Loss 0.5878\n",
      "Epoch 33 Batch 32 Loss 0.5799\n",
      "Epoch 33 Batch 40 Loss 0.5970\n",
      "Epoch 33 Batch 48 Loss 0.5935\n",
      "Epoch 33 Batch 56 Loss 0.6073\n",
      "Epoch 33 Batch 64 Loss 0.6077\n",
      "Epoch 33 Batch 72 Loss 0.6071\n",
      "Epoch 33 Batch 80 Loss 0.6095\n",
      "Epoch 33 Batch 88 Loss 0.6177\n",
      "Epoch 33 Batch 96 Loss 0.6257\n",
      "Epoch 33 Batch 104 Loss 0.6336\n",
      "Epoch 33 Batch 112 Loss 0.6415\n",
      "Epoch 33 Batch 120 Loss 0.6562\n",
      "Epoch 33 Batch 128 Loss 0.6628\n",
      "Epoch 33 Loss 0.6628\n",
      "Time taken for 1 epoch: 3.126551389694214 secs\n",
      "\n",
      "Epoch 34 Batch 0 Loss 0.5289\n",
      "Epoch 34 Batch 8 Loss 0.4749\n",
      "Epoch 34 Batch 16 Loss 0.4946\n",
      "Epoch 34 Batch 24 Loss 0.5368\n",
      "Epoch 34 Batch 32 Loss 0.5238\n",
      "Epoch 34 Batch 40 Loss 0.5224\n",
      "Epoch 34 Batch 48 Loss 0.5203\n",
      "Epoch 34 Batch 56 Loss 0.5212\n",
      "Epoch 34 Batch 64 Loss 0.5206\n",
      "Epoch 34 Batch 72 Loss 0.5225\n",
      "Epoch 34 Batch 80 Loss 0.5252\n",
      "Epoch 34 Batch 88 Loss 0.5376\n",
      "Epoch 34 Batch 96 Loss 0.5443\n",
      "Epoch 34 Batch 104 Loss 0.5515\n",
      "Epoch 34 Batch 112 Loss 0.5609\n",
      "Epoch 34 Batch 120 Loss 0.5730\n",
      "Epoch 34 Batch 128 Loss 0.5831\n",
      "Epoch 34 Loss 0.5831\n",
      "Time taken for 1 epoch: 3.126434564590454 secs\n",
      "\n",
      "Epoch 35 Batch 0 Loss 0.3657\n",
      "Epoch 35 Batch 8 Loss 0.4636\n",
      "Epoch 35 Batch 16 Loss 0.4432\n",
      "Epoch 35 Batch 24 Loss 0.4307\n",
      "Epoch 35 Batch 32 Loss 0.4376\n",
      "Epoch 35 Batch 40 Loss 0.4439\n",
      "Epoch 35 Batch 48 Loss 0.4524\n",
      "Epoch 35 Batch 56 Loss 0.4625\n",
      "Epoch 35 Batch 64 Loss 0.4686\n",
      "Epoch 35 Batch 72 Loss 0.4731\n",
      "Epoch 35 Batch 80 Loss 0.4739\n",
      "Epoch 35 Batch 88 Loss 0.4774\n",
      "Epoch 35 Batch 96 Loss 0.4782\n",
      "Epoch 35 Batch 104 Loss 0.4895\n",
      "Epoch 35 Batch 112 Loss 0.5041\n",
      "Epoch 35 Batch 120 Loss 0.5098\n",
      "Epoch 35 Batch 128 Loss 0.5147\n",
      "Saving checkpoint for epoch 35 at checkpoints/ckpt-7\n",
      "Epoch 35 Loss 0.5147\n",
      "Time taken for 1 epoch: 3.4610326290130615 secs\n",
      "\n",
      "Epoch 36 Batch 0 Loss 0.3177\n",
      "Epoch 36 Batch 8 Loss 0.4242\n",
      "Epoch 36 Batch 16 Loss 0.3800\n",
      "Epoch 36 Batch 24 Loss 0.3989\n",
      "Epoch 36 Batch 32 Loss 0.3784\n",
      "Epoch 36 Batch 40 Loss 0.3771\n",
      "Epoch 36 Batch 48 Loss 0.3844\n",
      "Epoch 36 Batch 56 Loss 0.3951\n",
      "Epoch 36 Batch 64 Loss 0.4025\n",
      "Epoch 36 Batch 72 Loss 0.3999\n",
      "Epoch 36 Batch 80 Loss 0.4073\n",
      "Epoch 36 Batch 88 Loss 0.4172\n",
      "Epoch 36 Batch 96 Loss 0.4226\n",
      "Epoch 36 Batch 104 Loss 0.4217\n",
      "Epoch 36 Batch 112 Loss 0.4249\n",
      "Epoch 36 Batch 120 Loss 0.4350\n",
      "Epoch 36 Batch 128 Loss 0.4451\n",
      "Epoch 36 Loss 0.4451\n",
      "Time taken for 1 epoch: 3.1259140968322754 secs\n",
      "\n",
      "Epoch 37 Batch 0 Loss 0.2989\n",
      "Epoch 37 Batch 8 Loss 0.3121\n",
      "Epoch 37 Batch 16 Loss 0.3247\n",
      "Epoch 37 Batch 24 Loss 0.3349\n",
      "Epoch 37 Batch 32 Loss 0.3476\n",
      "Epoch 37 Batch 40 Loss 0.3509\n",
      "Epoch 37 Batch 48 Loss 0.3606\n",
      "Epoch 37 Batch 56 Loss 0.3591\n",
      "Epoch 37 Batch 64 Loss 0.3597\n",
      "Epoch 37 Batch 72 Loss 0.3658\n",
      "Epoch 37 Batch 80 Loss 0.3738\n",
      "Epoch 37 Batch 88 Loss 0.3761\n",
      "Epoch 37 Batch 96 Loss 0.3786\n",
      "Epoch 37 Batch 104 Loss 0.3834\n",
      "Epoch 37 Batch 112 Loss 0.3890\n",
      "Epoch 37 Batch 120 Loss 0.3944\n",
      "Epoch 37 Batch 128 Loss 0.4056\n",
      "Epoch 37 Loss 0.4056\n",
      "Time taken for 1 epoch: 3.1365349292755127 secs\n",
      "\n",
      "Epoch 38 Batch 0 Loss 0.1989\n",
      "Epoch 38 Batch 8 Loss 0.3169\n",
      "Epoch 38 Batch 16 Loss 0.3577\n",
      "Epoch 38 Batch 24 Loss 0.3505\n",
      "Epoch 38 Batch 32 Loss 0.3721\n",
      "Epoch 38 Batch 40 Loss 0.3663\n",
      "Epoch 38 Batch 48 Loss 0.3636\n",
      "Epoch 38 Batch 56 Loss 0.3657\n",
      "Epoch 38 Batch 64 Loss 0.3637\n",
      "Epoch 38 Batch 72 Loss 0.3638\n",
      "Epoch 38 Batch 80 Loss 0.3652\n",
      "Epoch 38 Batch 88 Loss 0.3670\n",
      "Epoch 38 Batch 96 Loss 0.3686\n",
      "Epoch 38 Batch 104 Loss 0.3718\n",
      "Epoch 38 Batch 112 Loss 0.3726\n",
      "Epoch 38 Batch 120 Loss 0.3771\n",
      "Epoch 38 Batch 128 Loss 0.3805\n",
      "Epoch 38 Loss 0.3805\n",
      "Time taken for 1 epoch: 3.1225123405456543 secs\n",
      "\n",
      "Epoch 39 Batch 0 Loss 0.2736\n",
      "Epoch 39 Batch 8 Loss 0.2894\n",
      "Epoch 39 Batch 16 Loss 0.2679\n",
      "Epoch 39 Batch 24 Loss 0.2744\n",
      "Epoch 39 Batch 32 Loss 0.2755\n",
      "Epoch 39 Batch 40 Loss 0.2686\n",
      "Epoch 39 Batch 48 Loss 0.2746\n",
      "Epoch 39 Batch 56 Loss 0.2743\n",
      "Epoch 39 Batch 64 Loss 0.2795\n",
      "Epoch 39 Batch 72 Loss 0.2819\n",
      "Epoch 39 Batch 80 Loss 0.2921\n",
      "Epoch 39 Batch 88 Loss 0.3004\n",
      "Epoch 39 Batch 96 Loss 0.3076\n",
      "Epoch 39 Batch 104 Loss 0.3131\n",
      "Epoch 39 Batch 112 Loss 0.3193\n",
      "Epoch 39 Batch 120 Loss 0.3284\n",
      "Epoch 39 Batch 128 Loss 0.3302\n",
      "Epoch 39 Loss 0.3302\n",
      "Time taken for 1 epoch: 3.12424898147583 secs\n",
      "\n",
      "Epoch 40 Batch 0 Loss 0.5845\n",
      "Epoch 40 Batch 8 Loss 0.2964\n",
      "Epoch 40 Batch 16 Loss 0.2805\n",
      "Epoch 40 Batch 24 Loss 0.2671\n",
      "Epoch 40 Batch 32 Loss 0.2624\n",
      "Epoch 40 Batch 40 Loss 0.2579\n",
      "Epoch 40 Batch 48 Loss 0.2607\n",
      "Epoch 40 Batch 56 Loss 0.2663\n",
      "Epoch 40 Batch 64 Loss 0.2761\n",
      "Epoch 40 Batch 72 Loss 0.2900\n",
      "Epoch 40 Batch 80 Loss 0.2939\n",
      "Epoch 40 Batch 88 Loss 0.3038\n",
      "Epoch 40 Batch 96 Loss 0.3084\n",
      "Epoch 40 Batch 104 Loss 0.3145\n",
      "Epoch 40 Batch 112 Loss 0.3206\n",
      "Epoch 40 Batch 120 Loss 0.3210\n",
      "Epoch 40 Batch 128 Loss 0.3187\n",
      "Saving checkpoint for epoch 40 at checkpoints/ckpt-8\n",
      "Epoch 40 Loss 0.3187\n",
      "Time taken for 1 epoch: 3.40950345993042 secs\n",
      "\n",
      "Epoch 41 Batch 0 Loss 0.2201\n",
      "Epoch 41 Batch 8 Loss 0.1990\n",
      "Epoch 41 Batch 16 Loss 0.2133\n",
      "Epoch 41 Batch 24 Loss 0.2423\n",
      "Epoch 41 Batch 32 Loss 0.2560\n",
      "Epoch 41 Batch 40 Loss 0.2526\n",
      "Epoch 41 Batch 48 Loss 0.2519\n",
      "Epoch 41 Batch 56 Loss 0.2496\n",
      "Epoch 41 Batch 64 Loss 0.2543\n",
      "Epoch 41 Batch 72 Loss 0.2550\n",
      "Epoch 41 Batch 80 Loss 0.2531\n",
      "Epoch 41 Batch 88 Loss 0.2540\n",
      "Epoch 41 Batch 96 Loss 0.2582\n",
      "Epoch 41 Batch 104 Loss 0.2637\n",
      "Epoch 41 Batch 112 Loss 0.2679\n",
      "Epoch 41 Batch 120 Loss 0.2778\n",
      "Epoch 41 Batch 128 Loss 0.2872\n",
      "Epoch 41 Loss 0.2872\n",
      "Time taken for 1 epoch: 3.122654438018799 secs\n",
      "\n",
      "Epoch 42 Batch 0 Loss 0.1800\n",
      "Epoch 42 Batch 8 Loss 0.2626\n",
      "Epoch 42 Batch 16 Loss 0.2589\n",
      "Epoch 42 Batch 24 Loss 0.2647\n",
      "Epoch 42 Batch 32 Loss 0.2898\n",
      "Epoch 42 Batch 40 Loss 0.2944\n",
      "Epoch 42 Batch 48 Loss 0.2951\n",
      "Epoch 42 Batch 56 Loss 0.2938\n",
      "Epoch 42 Batch 64 Loss 0.2893\n",
      "Epoch 42 Batch 72 Loss 0.2885\n",
      "Epoch 42 Batch 80 Loss 0.2885\n",
      "Epoch 42 Batch 88 Loss 0.2854\n",
      "Epoch 42 Batch 96 Loss 0.2867\n",
      "Epoch 42 Batch 104 Loss 0.2860\n",
      "Epoch 42 Batch 112 Loss 0.2934\n",
      "Epoch 42 Batch 120 Loss 0.2912\n",
      "Epoch 42 Batch 128 Loss 0.2886\n",
      "Epoch 42 Loss 0.2886\n",
      "Time taken for 1 epoch: 3.116239070892334 secs\n",
      "\n",
      "Epoch 43 Batch 0 Loss 0.1438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 Batch 8 Loss 0.2025\n",
      "Epoch 43 Batch 16 Loss 0.2291\n",
      "Epoch 43 Batch 24 Loss 0.2202\n",
      "Epoch 43 Batch 32 Loss 0.2092\n",
      "Epoch 43 Batch 40 Loss 0.2006\n",
      "Epoch 43 Batch 48 Loss 0.2048\n",
      "Epoch 43 Batch 56 Loss 0.2058\n",
      "Epoch 43 Batch 64 Loss 0.2040\n",
      "Epoch 43 Batch 72 Loss 0.2114\n",
      "Epoch 43 Batch 80 Loss 0.2179\n",
      "Epoch 43 Batch 88 Loss 0.2244\n",
      "Epoch 43 Batch 96 Loss 0.2248\n",
      "Epoch 43 Batch 104 Loss 0.2309\n",
      "Epoch 43 Batch 112 Loss 0.2337\n",
      "Epoch 43 Batch 120 Loss 0.2368\n",
      "Epoch 43 Batch 128 Loss 0.2433\n",
      "Epoch 43 Loss 0.2433\n",
      "Time taken for 1 epoch: 3.137003183364868 secs\n",
      "\n",
      "Epoch 44 Batch 0 Loss 0.1932\n",
      "Epoch 44 Batch 8 Loss 0.2576\n",
      "Epoch 44 Batch 16 Loss 0.2271\n",
      "Epoch 44 Batch 24 Loss 0.2055\n",
      "Epoch 44 Batch 32 Loss 0.2136\n",
      "Epoch 44 Batch 40 Loss 0.2167\n",
      "Epoch 44 Batch 48 Loss 0.2202\n",
      "Epoch 44 Batch 56 Loss 0.2248\n",
      "Epoch 44 Batch 64 Loss 0.2248\n",
      "Epoch 44 Batch 72 Loss 0.2231\n",
      "Epoch 44 Batch 80 Loss 0.2264\n",
      "Epoch 44 Batch 88 Loss 0.2364\n",
      "Epoch 44 Batch 96 Loss 0.2419\n",
      "Epoch 44 Batch 104 Loss 0.2438\n",
      "Epoch 44 Batch 112 Loss 0.2484\n",
      "Epoch 44 Batch 120 Loss 0.2502\n",
      "Epoch 44 Batch 128 Loss 0.2523\n",
      "Epoch 44 Loss 0.2523\n",
      "Time taken for 1 epoch: 3.1311168670654297 secs\n",
      "\n",
      "Epoch 45 Batch 0 Loss 0.0902\n",
      "Epoch 45 Batch 8 Loss 0.1338\n",
      "Epoch 45 Batch 16 Loss 0.1509\n",
      "Epoch 45 Batch 24 Loss 0.1604\n",
      "Epoch 45 Batch 32 Loss 0.1862\n",
      "Epoch 45 Batch 40 Loss 0.1863\n",
      "Epoch 45 Batch 48 Loss 0.1926\n",
      "Epoch 45 Batch 56 Loss 0.1890\n",
      "Epoch 45 Batch 64 Loss 0.1895\n",
      "Epoch 45 Batch 72 Loss 0.1917\n",
      "Epoch 45 Batch 80 Loss 0.1919\n",
      "Epoch 45 Batch 88 Loss 0.1943\n",
      "Epoch 45 Batch 96 Loss 0.1979\n",
      "Epoch 45 Batch 104 Loss 0.2035\n",
      "Epoch 45 Batch 112 Loss 0.2065\n",
      "Epoch 45 Batch 120 Loss 0.2087\n",
      "Epoch 45 Batch 128 Loss 0.2185\n",
      "Saving checkpoint for epoch 45 at checkpoints/ckpt-9\n",
      "Epoch 45 Loss 0.2185\n",
      "Time taken for 1 epoch: 3.426274538040161 secs\n",
      "\n",
      "Epoch 46 Batch 0 Loss 0.1654\n",
      "Epoch 46 Batch 8 Loss 0.1480\n",
      "Epoch 46 Batch 16 Loss 0.1607\n",
      "Epoch 46 Batch 24 Loss 0.2024\n",
      "Epoch 46 Batch 32 Loss 0.2006\n",
      "Epoch 46 Batch 40 Loss 0.1944\n",
      "Epoch 46 Batch 48 Loss 0.1917\n",
      "Epoch 46 Batch 56 Loss 0.1891\n",
      "Epoch 46 Batch 64 Loss 0.1904\n",
      "Epoch 46 Batch 72 Loss 0.1954\n",
      "Epoch 46 Batch 80 Loss 0.1968\n",
      "Epoch 46 Batch 88 Loss 0.1962\n",
      "Epoch 46 Batch 96 Loss 0.2017\n",
      "Epoch 46 Batch 104 Loss 0.2059\n",
      "Epoch 46 Batch 112 Loss 0.2092\n",
      "Epoch 46 Batch 120 Loss 0.2076\n",
      "Epoch 46 Batch 128 Loss 0.2097\n",
      "Epoch 46 Loss 0.2097\n",
      "Time taken for 1 epoch: 3.1259474754333496 secs\n",
      "\n",
      "Epoch 47 Batch 0 Loss 0.0883\n",
      "Epoch 47 Batch 8 Loss 0.1530\n",
      "Epoch 47 Batch 16 Loss 0.1648\n",
      "Epoch 47 Batch 24 Loss 0.1650\n",
      "Epoch 47 Batch 32 Loss 0.1710\n",
      "Epoch 47 Batch 40 Loss 0.1750\n",
      "Epoch 47 Batch 48 Loss 0.1712\n",
      "Epoch 47 Batch 56 Loss 0.1748\n",
      "Epoch 47 Batch 64 Loss 0.1738\n",
      "Epoch 47 Batch 72 Loss 0.1711\n",
      "Epoch 47 Batch 80 Loss 0.1737\n",
      "Epoch 47 Batch 88 Loss 0.1722\n",
      "Epoch 47 Batch 96 Loss 0.1751\n",
      "Epoch 47 Batch 104 Loss 0.1813\n",
      "Epoch 47 Batch 112 Loss 0.1842\n",
      "Epoch 47 Batch 120 Loss 0.1885\n",
      "Epoch 47 Batch 128 Loss 0.1915\n",
      "Epoch 47 Loss 0.1915\n",
      "Time taken for 1 epoch: 3.1171717643737793 secs\n",
      "\n",
      "Epoch 48 Batch 0 Loss 0.2194\n",
      "Epoch 48 Batch 8 Loss 0.1903\n",
      "Epoch 48 Batch 16 Loss 0.2096\n",
      "Epoch 48 Batch 24 Loss 0.1985\n",
      "Epoch 48 Batch 32 Loss 0.1906\n",
      "Epoch 48 Batch 40 Loss 0.1922\n",
      "Epoch 48 Batch 48 Loss 0.1882\n",
      "Epoch 48 Batch 56 Loss 0.1856\n",
      "Epoch 48 Batch 64 Loss 0.1837\n",
      "Epoch 48 Batch 72 Loss 0.1845\n",
      "Epoch 48 Batch 80 Loss 0.1866\n",
      "Epoch 48 Batch 88 Loss 0.1872\n",
      "Epoch 48 Batch 96 Loss 0.1866\n",
      "Epoch 48 Batch 104 Loss 0.1853\n",
      "Epoch 48 Batch 112 Loss 0.1821\n",
      "Epoch 48 Batch 120 Loss 0.1810\n",
      "Epoch 48 Batch 128 Loss 0.1822\n",
      "Epoch 48 Loss 0.1822\n",
      "Time taken for 1 epoch: 3.1254453659057617 secs\n",
      "\n",
      "Epoch 49 Batch 0 Loss 0.1080\n",
      "Epoch 49 Batch 8 Loss 0.1086\n",
      "Epoch 49 Batch 16 Loss 0.1377\n",
      "Epoch 49 Batch 24 Loss 0.1457\n",
      "Epoch 49 Batch 32 Loss 0.1462\n",
      "Epoch 49 Batch 40 Loss 0.1411\n",
      "Epoch 49 Batch 48 Loss 0.1402\n",
      "Epoch 49 Batch 56 Loss 0.1395\n",
      "Epoch 49 Batch 64 Loss 0.1415\n",
      "Epoch 49 Batch 72 Loss 0.1423\n",
      "Epoch 49 Batch 80 Loss 0.1454\n",
      "Epoch 49 Batch 88 Loss 0.1481\n",
      "Epoch 49 Batch 96 Loss 0.1499\n",
      "Epoch 49 Batch 104 Loss 0.1528\n",
      "Epoch 49 Batch 112 Loss 0.1565\n",
      "Epoch 49 Batch 120 Loss 0.1608\n",
      "Epoch 49 Batch 128 Loss 0.1622\n",
      "Epoch 49 Loss 0.1622\n",
      "Time taken for 1 epoch: 3.1248810291290283 secs\n",
      "\n",
      "Epoch 50 Batch 0 Loss 0.1115\n",
      "Epoch 50 Batch 8 Loss 0.1237\n",
      "Epoch 50 Batch 16 Loss 0.1622\n",
      "Epoch 50 Batch 24 Loss 0.1481\n",
      "Epoch 50 Batch 32 Loss 0.1433\n",
      "Epoch 50 Batch 40 Loss 0.1446\n",
      "Epoch 50 Batch 48 Loss 0.1447\n",
      "Epoch 50 Batch 56 Loss 0.1452\n",
      "Epoch 50 Batch 64 Loss 0.1474\n",
      "Epoch 50 Batch 72 Loss 0.1451\n",
      "Epoch 50 Batch 80 Loss 0.1512\n",
      "Epoch 50 Batch 88 Loss 0.1548\n",
      "Epoch 50 Batch 96 Loss 0.1524\n",
      "Epoch 50 Batch 104 Loss 0.1513\n",
      "Epoch 50 Batch 112 Loss 0.1518\n",
      "Epoch 50 Batch 120 Loss 0.1517\n",
      "Epoch 50 Batch 128 Loss 0.1571\n",
      "Saving checkpoint for epoch 50 at checkpoints/ckpt-10\n",
      "Epoch 50 Loss 0.1571\n",
      "Time taken for 1 epoch: 3.6099328994750977 secs\n",
      "\n",
      "Epoch 51 Batch 0 Loss 0.1915\n",
      "Epoch 51 Batch 8 Loss 0.1076\n",
      "Epoch 51 Batch 16 Loss 0.1512\n",
      "Epoch 51 Batch 24 Loss 0.1464\n",
      "Epoch 51 Batch 32 Loss 0.1391\n",
      "Epoch 51 Batch 40 Loss 0.1394\n",
      "Epoch 51 Batch 48 Loss 0.1407\n",
      "Epoch 51 Batch 56 Loss 0.1392\n",
      "Epoch 51 Batch 64 Loss 0.1423\n",
      "Epoch 51 Batch 72 Loss 0.1459\n",
      "Epoch 51 Batch 80 Loss 0.1460\n",
      "Epoch 51 Batch 88 Loss 0.1476\n",
      "Epoch 51 Batch 96 Loss 0.1523\n",
      "Epoch 51 Batch 104 Loss 0.1548\n",
      "Epoch 51 Batch 112 Loss 0.1558\n",
      "Epoch 51 Batch 120 Loss 0.1580\n",
      "Epoch 51 Batch 128 Loss 0.1621\n",
      "Epoch 51 Loss 0.1621\n",
      "Time taken for 1 epoch: 3.1176211833953857 secs\n",
      "\n",
      "Epoch 52 Batch 0 Loss 0.3525\n",
      "Epoch 52 Batch 8 Loss 0.1642\n",
      "Epoch 52 Batch 16 Loss 0.1667\n",
      "Epoch 52 Batch 24 Loss 0.1648\n",
      "Epoch 52 Batch 32 Loss 0.1493\n",
      "Epoch 52 Batch 40 Loss 0.1431\n",
      "Epoch 52 Batch 48 Loss 0.1371\n",
      "Epoch 52 Batch 56 Loss 0.1331\n",
      "Epoch 52 Batch 64 Loss 0.1358\n",
      "Epoch 52 Batch 72 Loss 0.1346\n",
      "Epoch 52 Batch 80 Loss 0.1407\n",
      "Epoch 52 Batch 88 Loss 0.1417\n",
      "Epoch 52 Batch 96 Loss 0.1388\n",
      "Epoch 52 Batch 104 Loss 0.1408\n",
      "Epoch 52 Batch 112 Loss 0.1417\n",
      "Epoch 52 Batch 120 Loss 0.1427\n",
      "Epoch 52 Batch 128 Loss 0.1425\n",
      "Epoch 52 Loss 0.1425\n",
      "Time taken for 1 epoch: 3.1190662384033203 secs\n",
      "\n",
      "Epoch 53 Batch 0 Loss 0.0753\n",
      "Epoch 53 Batch 8 Loss 0.1196\n",
      "Epoch 53 Batch 16 Loss 0.1298\n",
      "Epoch 53 Batch 24 Loss 0.1462\n",
      "Epoch 53 Batch 32 Loss 0.1462\n",
      "Epoch 53 Batch 40 Loss 0.1365\n",
      "Epoch 53 Batch 48 Loss 0.1337\n",
      "Epoch 53 Batch 56 Loss 0.1316\n",
      "Epoch 53 Batch 64 Loss 0.1369\n",
      "Epoch 53 Batch 72 Loss 0.1346\n",
      "Epoch 53 Batch 80 Loss 0.1393\n",
      "Epoch 53 Batch 88 Loss 0.1406\n",
      "Epoch 53 Batch 96 Loss 0.1384\n",
      "Epoch 53 Batch 104 Loss 0.1413\n",
      "Epoch 53 Batch 112 Loss 0.1411\n",
      "Epoch 53 Batch 120 Loss 0.1473\n",
      "Epoch 53 Batch 128 Loss 0.1499\n",
      "Epoch 53 Loss 0.1499\n",
      "Time taken for 1 epoch: 3.126498222351074 secs\n",
      "\n",
      "Epoch 54 Batch 0 Loss 0.2827\n",
      "Epoch 54 Batch 8 Loss 0.1148\n",
      "Epoch 54 Batch 16 Loss 0.1379\n",
      "Epoch 54 Batch 24 Loss 0.1560\n",
      "Epoch 54 Batch 32 Loss 0.1509\n",
      "Epoch 54 Batch 40 Loss 0.1418\n",
      "Epoch 54 Batch 48 Loss 0.1456\n",
      "Epoch 54 Batch 56 Loss 0.1477\n",
      "Epoch 54 Batch 64 Loss 0.1438\n",
      "Epoch 54 Batch 72 Loss 0.1434\n",
      "Epoch 54 Batch 80 Loss 0.1413\n",
      "Epoch 54 Batch 88 Loss 0.1391\n",
      "Epoch 54 Batch 96 Loss 0.1425\n",
      "Epoch 54 Batch 104 Loss 0.1413\n",
      "Epoch 54 Batch 112 Loss 0.1424\n",
      "Epoch 54 Batch 120 Loss 0.1420\n",
      "Epoch 54 Batch 128 Loss 0.1418\n",
      "Epoch 54 Loss 0.1418\n",
      "Time taken for 1 epoch: 3.1245839595794678 secs\n",
      "\n",
      "Epoch 55 Batch 0 Loss 0.0416\n",
      "Epoch 55 Batch 8 Loss 0.1010\n",
      "Epoch 55 Batch 16 Loss 0.0810\n",
      "Epoch 55 Batch 24 Loss 0.0989\n",
      "Epoch 55 Batch 32 Loss 0.1172\n",
      "Epoch 55 Batch 40 Loss 0.1161\n",
      "Epoch 55 Batch 48 Loss 0.1194\n",
      "Epoch 55 Batch 56 Loss 0.1151\n",
      "Epoch 55 Batch 64 Loss 0.1224\n",
      "Epoch 55 Batch 72 Loss 0.1274\n",
      "Epoch 55 Batch 80 Loss 0.1276\n",
      "Epoch 55 Batch 88 Loss 0.1278\n",
      "Epoch 55 Batch 96 Loss 0.1283\n",
      "Epoch 55 Batch 104 Loss 0.1309\n",
      "Epoch 55 Batch 112 Loss 0.1297\n",
      "Epoch 55 Batch 120 Loss 0.1313\n",
      "Epoch 55 Batch 128 Loss 0.1324\n",
      "Saving checkpoint for epoch 55 at checkpoints/ckpt-11\n",
      "Epoch 55 Loss 0.1324\n",
      "Time taken for 1 epoch: 3.451242208480835 secs\n",
      "\n",
      "Epoch 56 Batch 0 Loss 0.0555\n",
      "Epoch 56 Batch 8 Loss 0.0775\n",
      "Epoch 56 Batch 16 Loss 0.0993\n",
      "Epoch 56 Batch 24 Loss 0.1036\n",
      "Epoch 56 Batch 32 Loss 0.1069\n",
      "Epoch 56 Batch 40 Loss 0.1042\n",
      "Epoch 56 Batch 48 Loss 0.0965\n",
      "Epoch 56 Batch 56 Loss 0.0953\n",
      "Epoch 56 Batch 64 Loss 0.0984\n",
      "Epoch 56 Batch 72 Loss 0.1038\n",
      "Epoch 56 Batch 80 Loss 0.1065\n",
      "Epoch 56 Batch 88 Loss 0.1050\n",
      "Epoch 56 Batch 96 Loss 0.1051\n",
      "Epoch 56 Batch 104 Loss 0.1074\n",
      "Epoch 56 Batch 112 Loss 0.1146\n",
      "Epoch 56 Batch 120 Loss 0.1189\n",
      "Epoch 56 Batch 128 Loss 0.1227\n",
      "Epoch 56 Loss 0.1227\n",
      "Time taken for 1 epoch: 3.128459930419922 secs\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57 Batch 0 Loss 0.0263\n",
      "Epoch 57 Batch 8 Loss 0.1255\n",
      "Epoch 57 Batch 16 Loss 0.1219\n",
      "Epoch 57 Batch 24 Loss 0.1103\n",
      "Epoch 57 Batch 32 Loss 0.1217\n",
      "Epoch 57 Batch 40 Loss 0.1185\n",
      "Epoch 57 Batch 48 Loss 0.1166\n",
      "Epoch 57 Batch 56 Loss 0.1192\n",
      "Epoch 57 Batch 64 Loss 0.1228\n",
      "Epoch 57 Batch 72 Loss 0.1235\n",
      "Epoch 57 Batch 80 Loss 0.1230\n",
      "Epoch 57 Batch 88 Loss 0.1201\n",
      "Epoch 57 Batch 96 Loss 0.1167\n",
      "Epoch 57 Batch 104 Loss 0.1148\n",
      "Epoch 57 Batch 112 Loss 0.1155\n",
      "Epoch 57 Batch 120 Loss 0.1162\n",
      "Epoch 57 Batch 128 Loss 0.1205\n",
      "Epoch 57 Loss 0.1205\n",
      "Time taken for 1 epoch: 3.1223723888397217 secs\n",
      "\n",
      "Epoch 58 Batch 0 Loss 0.0798\n",
      "Epoch 58 Batch 8 Loss 0.0923\n",
      "Epoch 58 Batch 16 Loss 0.1011\n",
      "Epoch 58 Batch 24 Loss 0.1099\n",
      "Epoch 58 Batch 32 Loss 0.1095\n",
      "Epoch 58 Batch 40 Loss 0.1131\n",
      "Epoch 58 Batch 48 Loss 0.1087\n",
      "Epoch 58 Batch 56 Loss 0.1150\n",
      "Epoch 58 Batch 64 Loss 0.1140\n",
      "Epoch 58 Batch 72 Loss 0.1124\n",
      "Epoch 58 Batch 80 Loss 0.1111\n",
      "Epoch 58 Batch 88 Loss 0.1134\n",
      "Epoch 58 Batch 96 Loss 0.1126\n",
      "Epoch 58 Batch 104 Loss 0.1159\n",
      "Epoch 58 Batch 112 Loss 0.1146\n",
      "Epoch 58 Batch 120 Loss 0.1141\n",
      "Epoch 58 Batch 128 Loss 0.1135\n",
      "Epoch 58 Loss 0.1135\n",
      "Time taken for 1 epoch: 3.127532958984375 secs\n",
      "\n",
      "Epoch 59 Batch 0 Loss 0.0515\n",
      "Epoch 59 Batch 8 Loss 0.0593\n",
      "Epoch 59 Batch 16 Loss 0.0798\n",
      "Epoch 59 Batch 24 Loss 0.0798\n",
      "Epoch 59 Batch 32 Loss 0.0841\n",
      "Epoch 59 Batch 40 Loss 0.1024\n",
      "Epoch 59 Batch 48 Loss 0.1038\n",
      "Epoch 59 Batch 56 Loss 0.1010\n",
      "Epoch 59 Batch 64 Loss 0.1013\n",
      "Epoch 59 Batch 72 Loss 0.1032\n",
      "Epoch 59 Batch 80 Loss 0.1064\n",
      "Epoch 59 Batch 88 Loss 0.1080\n",
      "Epoch 59 Batch 96 Loss 0.1090\n",
      "Epoch 59 Batch 104 Loss 0.1091\n",
      "Epoch 59 Batch 112 Loss 0.1101\n",
      "Epoch 59 Batch 120 Loss 0.1140\n",
      "Epoch 59 Batch 128 Loss 0.1157\n",
      "Epoch 59 Loss 0.1157\n",
      "Time taken for 1 epoch: 3.140655994415283 secs\n",
      "\n",
      "Epoch 60 Batch 0 Loss 0.0262\n",
      "Epoch 60 Batch 8 Loss 0.0709\n",
      "Epoch 60 Batch 16 Loss 0.1164\n",
      "Epoch 60 Batch 24 Loss 0.1137\n",
      "Epoch 60 Batch 32 Loss 0.1094\n",
      "Epoch 60 Batch 40 Loss 0.0994\n",
      "Epoch 60 Batch 48 Loss 0.1021\n",
      "Epoch 60 Batch 56 Loss 0.0977\n",
      "Epoch 60 Batch 64 Loss 0.0955\n",
      "Epoch 60 Batch 72 Loss 0.0920\n",
      "Epoch 60 Batch 80 Loss 0.0939\n",
      "Epoch 60 Batch 88 Loss 0.0932\n",
      "Epoch 60 Batch 96 Loss 0.0932\n",
      "Epoch 60 Batch 104 Loss 0.0958\n",
      "Epoch 60 Batch 112 Loss 0.0959\n",
      "Epoch 60 Batch 120 Loss 0.0971\n",
      "Epoch 60 Batch 128 Loss 0.0992\n",
      "Saving checkpoint for epoch 60 at checkpoints/ckpt-12\n",
      "Epoch 60 Loss 0.0992\n",
      "Time taken for 1 epoch: 3.401805877685547 secs\n",
      "\n",
      "Epoch 61 Batch 0 Loss 0.0553\n",
      "Epoch 61 Batch 8 Loss 0.0745\n",
      "Epoch 61 Batch 16 Loss 0.0930\n",
      "Epoch 61 Batch 24 Loss 0.0857\n",
      "Epoch 61 Batch 32 Loss 0.0874\n",
      "Epoch 61 Batch 40 Loss 0.0941\n",
      "Epoch 61 Batch 48 Loss 0.0989\n",
      "Epoch 61 Batch 56 Loss 0.0999\n",
      "Epoch 61 Batch 64 Loss 0.1040\n",
      "Epoch 61 Batch 72 Loss 0.1040\n",
      "Epoch 61 Batch 80 Loss 0.1113\n",
      "Epoch 61 Batch 88 Loss 0.1145\n",
      "Epoch 61 Batch 96 Loss 0.1122\n",
      "Epoch 61 Batch 104 Loss 0.1133\n",
      "Epoch 61 Batch 112 Loss 0.1134\n",
      "Epoch 61 Batch 120 Loss 0.1125\n",
      "Epoch 61 Batch 128 Loss 0.1096\n",
      "Epoch 61 Loss 0.1096\n",
      "Time taken for 1 epoch: 3.118630886077881 secs\n",
      "\n",
      "Epoch 62 Batch 0 Loss 0.0085\n",
      "Epoch 62 Batch 8 Loss 0.0930\n",
      "Epoch 62 Batch 16 Loss 0.0741\n",
      "Epoch 62 Batch 24 Loss 0.0788\n",
      "Epoch 62 Batch 32 Loss 0.0760\n",
      "Epoch 62 Batch 40 Loss 0.0820\n",
      "Epoch 62 Batch 48 Loss 0.0842\n",
      "Epoch 62 Batch 56 Loss 0.0841\n",
      "Epoch 62 Batch 64 Loss 0.0852\n",
      "Epoch 62 Batch 72 Loss 0.0908\n",
      "Epoch 62 Batch 80 Loss 0.0946\n",
      "Epoch 62 Batch 88 Loss 0.0944\n",
      "Epoch 62 Batch 96 Loss 0.0932\n",
      "Epoch 62 Batch 104 Loss 0.0927\n",
      "Epoch 62 Batch 112 Loss 0.0925\n",
      "Epoch 62 Batch 120 Loss 0.0925\n",
      "Epoch 62 Batch 128 Loss 0.0952\n",
      "Epoch 62 Loss 0.0952\n",
      "Time taken for 1 epoch: 3.131694793701172 secs\n",
      "\n",
      "Epoch 63 Batch 0 Loss 0.0836\n",
      "Epoch 63 Batch 8 Loss 0.0793\n",
      "Epoch 63 Batch 16 Loss 0.0895\n",
      "Epoch 63 Batch 24 Loss 0.0843\n",
      "Epoch 63 Batch 32 Loss 0.0807\n",
      "Epoch 63 Batch 40 Loss 0.0789\n",
      "Epoch 63 Batch 48 Loss 0.0818\n",
      "Epoch 63 Batch 56 Loss 0.0923\n",
      "Epoch 63 Batch 64 Loss 0.0968\n",
      "Epoch 63 Batch 72 Loss 0.0964\n",
      "Epoch 63 Batch 80 Loss 0.0968\n",
      "Epoch 63 Batch 88 Loss 0.0959\n",
      "Epoch 63 Batch 96 Loss 0.0948\n",
      "Epoch 63 Batch 104 Loss 0.0936\n",
      "Epoch 63 Batch 112 Loss 0.0962\n",
      "Epoch 63 Batch 120 Loss 0.0961\n",
      "Epoch 63 Batch 128 Loss 0.0965\n",
      "Epoch 63 Loss 0.0965\n",
      "Time taken for 1 epoch: 3.1314663887023926 secs\n",
      "\n",
      "Epoch 64 Batch 0 Loss 0.0273\n",
      "Epoch 64 Batch 8 Loss 0.1172\n",
      "Epoch 64 Batch 16 Loss 0.0971\n",
      "Epoch 64 Batch 24 Loss 0.0906\n",
      "Epoch 64 Batch 32 Loss 0.0889\n",
      "Epoch 64 Batch 40 Loss 0.0867\n",
      "Epoch 64 Batch 48 Loss 0.0882\n",
      "Epoch 64 Batch 56 Loss 0.0907\n",
      "Epoch 64 Batch 64 Loss 0.0995\n",
      "Epoch 64 Batch 72 Loss 0.1005\n",
      "Epoch 64 Batch 80 Loss 0.1009\n",
      "Epoch 64 Batch 88 Loss 0.1017\n",
      "Epoch 64 Batch 96 Loss 0.1049\n",
      "Epoch 64 Batch 104 Loss 0.1021\n",
      "Epoch 64 Batch 112 Loss 0.1013\n",
      "Epoch 64 Batch 120 Loss 0.1006\n",
      "Epoch 64 Batch 128 Loss 0.1002\n",
      "Epoch 64 Loss 0.1002\n",
      "Time taken for 1 epoch: 3.119295835494995 secs\n",
      "\n",
      "Epoch 65 Batch 0 Loss 0.0440\n",
      "Epoch 65 Batch 8 Loss 0.1108\n",
      "Epoch 65 Batch 16 Loss 0.0898\n",
      "Epoch 65 Batch 24 Loss 0.0753\n",
      "Epoch 65 Batch 32 Loss 0.0715\n",
      "Epoch 65 Batch 40 Loss 0.0790\n",
      "Epoch 65 Batch 48 Loss 0.0766\n",
      "Epoch 65 Batch 56 Loss 0.0817\n",
      "Epoch 65 Batch 64 Loss 0.0830\n",
      "Epoch 65 Batch 72 Loss 0.0805\n",
      "Epoch 65 Batch 80 Loss 0.0824\n",
      "Epoch 65 Batch 88 Loss 0.0855\n",
      "Epoch 65 Batch 96 Loss 0.0873\n",
      "Epoch 65 Batch 104 Loss 0.0878\n",
      "Epoch 65 Batch 112 Loss 0.0878\n",
      "Epoch 65 Batch 120 Loss 0.0899\n",
      "Epoch 65 Batch 128 Loss 0.0890\n",
      "Saving checkpoint for epoch 65 at checkpoints/ckpt-13\n",
      "Epoch 65 Loss 0.0890\n",
      "Time taken for 1 epoch: 3.4797563552856445 secs\n",
      "\n",
      "Epoch 66 Batch 0 Loss 0.0766\n",
      "Epoch 66 Batch 8 Loss 0.0469\n",
      "Epoch 66 Batch 16 Loss 0.0641\n",
      "Epoch 66 Batch 24 Loss 0.0639\n",
      "Epoch 66 Batch 32 Loss 0.0805\n",
      "Epoch 66 Batch 40 Loss 0.0772\n",
      "Epoch 66 Batch 48 Loss 0.0812\n",
      "Epoch 66 Batch 56 Loss 0.0917\n",
      "Epoch 66 Batch 64 Loss 0.0875\n",
      "Epoch 66 Batch 72 Loss 0.0878\n",
      "Epoch 66 Batch 80 Loss 0.0895\n",
      "Epoch 66 Batch 88 Loss 0.0910\n",
      "Epoch 66 Batch 96 Loss 0.0943\n",
      "Epoch 66 Batch 104 Loss 0.0971\n",
      "Epoch 66 Batch 112 Loss 0.0966\n",
      "Epoch 66 Batch 120 Loss 0.0955\n",
      "Epoch 66 Batch 128 Loss 0.0963\n",
      "Epoch 66 Loss 0.0963\n",
      "Time taken for 1 epoch: 3.1264665126800537 secs\n",
      "\n",
      "Epoch 67 Batch 0 Loss 0.0674\n",
      "Epoch 67 Batch 8 Loss 0.0446\n",
      "Epoch 67 Batch 16 Loss 0.0458\n",
      "Epoch 67 Batch 24 Loss 0.0511\n",
      "Epoch 67 Batch 32 Loss 0.0548\n",
      "Epoch 67 Batch 40 Loss 0.0642\n",
      "Epoch 67 Batch 48 Loss 0.0717\n",
      "Epoch 67 Batch 56 Loss 0.0762\n",
      "Epoch 67 Batch 64 Loss 0.0771\n",
      "Epoch 67 Batch 72 Loss 0.0781\n",
      "Epoch 67 Batch 80 Loss 0.0804\n",
      "Epoch 67 Batch 88 Loss 0.0799\n",
      "Epoch 67 Batch 96 Loss 0.0832\n",
      "Epoch 67 Batch 104 Loss 0.0825\n",
      "Epoch 67 Batch 112 Loss 0.0822\n",
      "Epoch 67 Batch 120 Loss 0.0820\n",
      "Epoch 67 Batch 128 Loss 0.0839\n",
      "Epoch 67 Loss 0.0839\n",
      "Time taken for 1 epoch: 3.123142719268799 secs\n",
      "\n",
      "Epoch 68 Batch 0 Loss 0.2005\n",
      "Epoch 68 Batch 8 Loss 0.0561\n",
      "Epoch 68 Batch 16 Loss 0.0831\n",
      "Epoch 68 Batch 24 Loss 0.0742\n",
      "Epoch 68 Batch 32 Loss 0.0701\n",
      "Epoch 68 Batch 40 Loss 0.0691\n",
      "Epoch 68 Batch 48 Loss 0.0746\n",
      "Epoch 68 Batch 56 Loss 0.0781\n",
      "Epoch 68 Batch 64 Loss 0.0775\n",
      "Epoch 68 Batch 72 Loss 0.0760\n",
      "Epoch 68 Batch 80 Loss 0.0759\n",
      "Epoch 68 Batch 88 Loss 0.0749\n",
      "Epoch 68 Batch 96 Loss 0.0760\n",
      "Epoch 68 Batch 104 Loss 0.0795\n",
      "Epoch 68 Batch 112 Loss 0.0797\n",
      "Epoch 68 Batch 120 Loss 0.0818\n",
      "Epoch 68 Batch 128 Loss 0.0834\n",
      "Epoch 68 Loss 0.0834\n",
      "Time taken for 1 epoch: 3.1263623237609863 secs\n",
      "\n",
      "Epoch 69 Batch 0 Loss 0.0165\n",
      "Epoch 69 Batch 8 Loss 0.0528\n",
      "Epoch 69 Batch 16 Loss 0.0710\n",
      "Epoch 69 Batch 24 Loss 0.0642\n",
      "Epoch 69 Batch 32 Loss 0.0646\n",
      "Epoch 69 Batch 40 Loss 0.0675\n",
      "Epoch 69 Batch 48 Loss 0.0732\n",
      "Epoch 69 Batch 56 Loss 0.0732\n",
      "Epoch 69 Batch 64 Loss 0.0741\n",
      "Epoch 69 Batch 72 Loss 0.0757\n",
      "Epoch 69 Batch 80 Loss 0.0735\n",
      "Epoch 69 Batch 88 Loss 0.0747\n",
      "Epoch 69 Batch 96 Loss 0.0749\n",
      "Epoch 69 Batch 104 Loss 0.0778\n",
      "Epoch 69 Batch 112 Loss 0.0845\n",
      "Epoch 69 Batch 120 Loss 0.0839\n",
      "Epoch 69 Batch 128 Loss 0.0831\n",
      "Epoch 69 Loss 0.0831\n",
      "Time taken for 1 epoch: 3.119158983230591 secs\n",
      "\n",
      "Epoch 70 Batch 0 Loss 0.1432\n",
      "Epoch 70 Batch 8 Loss 0.0663\n",
      "Epoch 70 Batch 16 Loss 0.0667\n",
      "Epoch 70 Batch 24 Loss 0.0592\n",
      "Epoch 70 Batch 32 Loss 0.0576\n",
      "Epoch 70 Batch 40 Loss 0.0621\n",
      "Epoch 70 Batch 48 Loss 0.0600\n",
      "Epoch 70 Batch 56 Loss 0.0667\n",
      "Epoch 70 Batch 64 Loss 0.0683\n",
      "Epoch 70 Batch 72 Loss 0.0761\n",
      "Epoch 70 Batch 80 Loss 0.0759\n",
      "Epoch 70 Batch 88 Loss 0.0758\n",
      "Epoch 70 Batch 96 Loss 0.0766\n",
      "Epoch 70 Batch 104 Loss 0.0791\n",
      "Epoch 70 Batch 112 Loss 0.0776\n",
      "Epoch 70 Batch 120 Loss 0.0782\n",
      "Epoch 70 Batch 128 Loss 0.0811\n",
      "Saving checkpoint for epoch 70 at checkpoints/ckpt-14\n",
      "Epoch 70 Loss 0.0811\n",
      "Time taken for 1 epoch: 3.4033663272857666 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "  \n",
    "    for (batch, (inp, tar)) in enumerate(dataset):\n",
    "        train_step(inp, tar)\n",
    "    \n",
    "        # 55k samples\n",
    "        # we display 3 batch results -- 0th, middle and last one (approx)\n",
    "        # 55k / 64 ~ 858; 858 / 2 = 429\n",
    "        if batch % 8 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, train_loss.result()))\n",
    "      \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
    "    \n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch + 1, train_loss.result()))\n",
    "\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "5MmWB9wpOp6x"
   },
   "outputs": [],
   "source": [
    "def evaluate(input_document):\n",
    "    input_document = document_tokenizer.texts_to_sequences([input_document])\n",
    "    input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
    "\n",
    "    encoder_input = tf.expand_dims(input_document[0], 0)\n",
    "\n",
    "    decoder_input = [summary_tokenizer.word_index[\"<go>\"]]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "    \n",
    "    for i in range(decoder_maxlen):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
    "\n",
    "        predictions, attention_weights = transformer(\n",
    "            encoder_input, \n",
    "            output,\n",
    "            False,\n",
    "            enc_padding_mask,\n",
    "            combined_mask,\n",
    "            dec_padding_mask\n",
    "        )\n",
    "\n",
    "        predictions = predictions[: ,-1:, :]\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        if predicted_id == summary_tokenizer.word_index[\"<stop>\"]:\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "g308MId-PCmh"
   },
   "outputs": [],
   "source": [
    "def summarize(input_document):\n",
    "    summarized = evaluate(input_document=input_document)[0].numpy()\n",
    "    summarized = np.expand_dims(summarized[1:], 0)  # not printing <go> token\n",
    "    return summary_tokenizer.sequences_to_texts(summarized)[0]  # since there is just one translated document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "-ozlvpxJEJXj"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [69]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m output \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnotebook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mText\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      3\u001b[0m     output\u001b[38;5;241m.\u001b[39mappend(summarize(i))\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/tqdm/notebook.py:243\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m unit_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    242\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;241m*\u001b[39m unit_scale \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal\n\u001b[0;32m--> 243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_printer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mncols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer\u001b[38;5;241m.\u001b[39mpbar \u001b[38;5;241m=\u001b[39m proxy(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.9/site-packages/tqdm/notebook.py:118\u001b[0m, in \u001b[0;36mtqdm_notebook.status_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# if not total:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m \n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Prepare IPython progress bar\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m IProgress \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# #187 #451 #558 #872\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total:\n\u001b[1;32m    120\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m IProgress(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mtotal)\n",
      "\u001b[0;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "for i in tqdm.notebook.tqdm(df.Text):\n",
    "    output.append(summarize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AM5e8FMYF_4Q"
   },
   "outputs": [],
   "source": [
    "reference = []\n",
    "for i in df.Summary:\n",
    "  reference.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UZaD-RhZLBwb",
    "outputId": "d4b7f326-adce-446b-f9c9-9582da0e07db"
   },
   "outputs": [],
   "source": [
    "reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "pD6a2l8YPFge",
    "outputId": "9a9c76ea-a998-40f3-e202-f3336fc9435c"
   },
   "outputs": [],
   "source": [
    "summarize(\"মানুষের মুখ খুব শ ক্তিশালী এক জিনিস। মানুষ যেটা বলে সেটার একটা প্রভাব আছে।অভিজ্ঞতা থেকে দেখেছি একটা ছেলেকে ক্ষেপানোর জন্য বলা হতো, অমুক মেয়ের সাথে তুই প্রেম করিস। কদিন পরে সত্যি সত্যি তারা প্রেম করা শুরু করে দিয়েছিল।স্বামী-স্ত্রীর মনোমালিন্যর সময় হয়ত স্ত্রী আফসোস করে বলল, আমি পুরোনো হয়ে গেছি - এখন তো আর আমাকে ভালো লাগবে না।সত্যি সত্যি দেখা যাবে কদিন পরে স্বামীর ঠিক ওই জিনিসটাই মনে হতে থাকবে। অথচ হয়ত সে এ ব্যাপারে আগে ভাবেইনি।একটা ছেলেকে পরিবারের সবাই বলে, তুই কোনো কাজের না - দেখা যাবে ছেলেটা আসলেই কিছু করতে পারছে না।এজন্য রসুল সাল্লাল্লাহু আলাইহি ওয়া সাল্লাম বলেছেন, হয় ভালো কথা বলো নয়ত চুপ থাক।আমাদের জীবনের বহু ভালো পরিস্থিতি খারাপ থেকে খারাপ হয়েছে শুধুমাত্র আমাদের কথার কারণে।জিহবা সাবধান ভাইয়েরা। মুখ সাবধান বোনেরা।রসুল সাল্লাল্লাহু আলাইহি ওয়া সাল্লামের কথাটাকে দাম দিই - সংসারে শান্তি আসবে, আয়ে বারাকাহ আসবে।\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "ERj84JjDP_x-",
    "outputId": "7ba2236f-ceb4-4bc1-89c6-3e7ab17c79ff"
   },
   "outputs": [],
   "source": [
    "summarize(\"ক্লাসে সবচেয়ে দূর্বল ছেলেটি কাল সমাবর্তনে এসেছিল সবার চেয়ে হাই পজিশনের জব নিয়ে। বারবার প্রেমে ব্যর্থ হওয়া মেয়েটি এসেছিল একটি সুন্দর ছোট্ট পরিবার নিয়ে। কারো কাছে পাত্তা না পাওয়া, তোকে দিয়ে কিছু হবে না বলা ছেলেটিই সবচেয়ে সুন্দর বউ নিয়ে এসেছে। পড়াশোনার খরচ যোগাতে টিউশন করে হাত খরচ চালানো মেয়েটি কাল গাড়ি দিয়ে ক্যাম্পাসে এসেছিল। ক্লাসের সবচেয়ে সাক্সেস্ফুল ছেলেটি ডিপ্রেশনে ভুগছে জব না পাওয়ায়। ডিপার্টমেন্টের হার্টথ্রোব মেয়েটির চোখে নিচে কালি বিয়ে হচ্ছে না বয়স হয়ে গেছে।এভাবেই সময়ের সাথে বদলে যায় মানুষের জীবনে ইকুয়েশন। আসলে সমাবর্তনের মাধ্যমে শিক্ষা জীবনের শেষ হলেও সফলতা ও ব্যর্থজীবনের হিসাব গণনা শুরু হয়ে এখান থেকেই।তাই ঘৃণা, হিংসা, কম্পিটিশন বাদ দিয়ে জীবনটাকে বাচা উচিত সম্পূর্ণ স্বাদ ও ভালবাসা নিয়ে। কখন জীবনের কোন মোড় দেখায় কোন নিশ্চয়তা নেই, তাই কোন মুহূর্তের জন্য যাতে আফসোস না থাকে।\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "5YKHMymdSugA",
    "outputId": "c05a2b6f-ed6c-4b06-882f-4dd4bb145497"
   },
   "outputs": [],
   "source": [
    "summarize(\"ইতালির প্রধানমন্ত্রী জুসেপ্পে কন্তে বলেছেন, একেকজন বাংলাদেশি একেকটা ভাইরাস বোমা। অনেক বাংলাদেশি ভাই সেটাকে শেয়ার করে দেশকে পরোক্ষভাবে তিরস্কার করছেন। অথচ কন্তে সাহেবকে বলা দরকার, আমাদের অসচেতনতা নিয়ে এমন ঢালাও মন্তব্য করার আগে আপনার অগ্রজ শাসকদের দিকে তাকান। নিরো সাহেবের দিকে তাকান।আপনি কী জানেন না? রোম যখন পুড়ছিলো, নিরো তখন সাহেব বাঁশি বাজাচ্ছিলেন।আর আমরা একটু বাঁশি বাজালেই দোষ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "Swe6Dl93DEfY",
    "outputId": "4005c43e-67ab-422f-9b87-5574b799429b"
   },
   "outputs": [],
   "source": [
    "summarize(\"মুক্তির সারথি বাংলাদেশ সাধারণ ছাত্র অধিকার সংরক্ষণ পরিষদ আমাদের দেশে স্বাধীনতার পর থেকে চলমান অসুস্থ ধারার রাজনীতি এখনো বহাল তবিয়তে চলছে। এই অবস্থা থেকে বের হয়ে আসতে না পারলে দেশ ও জাতি আরো গভীর অন্ধকারে নিমজ্জিত হবে। একটি দেশের প্রকৃত উন্নয়ন নির্ভর করে সেদেশের রাজনৈতিক স্থিতিশীলতার উপর। দেশে দৃশ্যমান উন্নয়ন অনেক কিন্তু প্রকৃত উন্নয়ন কতটা তা যতেষ্ঠ প্রশ্নের মুখোমুখি আজ। উল্টো দেশের স্তম্ভ গুলো দিনকে দিন দুর্বল থেকে দুর্বলতর করা হচ্ছে। আইন বিচার এবং শাসন বিভাগের অবস্থা বড্ড নাজুক। এখন অনেক সময় দেখি মহান জাতীয় সংসদ কোরাম সংকটে ভুগে। সাংবিধানিক প্রতিষ্ঠান গুলো তাদের স্বকীয়তা হারাচ্ছে অনবরত। বাংলাদেশ নির্বাচন কমিশন, দুর্নীতি দমন কমিশন সহ সাংবিধানিক প্রতিষ্ঠানগুলোকে এখন আর কার্যকর তেমন কোন পদক্ষেপ নিতে দেখি না আর আমরা।রাষ্ট্রের চতুর্থ স্তম্ভ গণমাধ্যম, এই গণমাধ্যমের অবস্থা যে খুব একটা ভাল তাও কিন্তু নয়। তবুও বলবো সব মিলিয়ে এগিয়ে যাচ্ছে প্রিয় স্বদেশ। আগামীতে গণমানুষের প্রত্যাশা পূরণে কাজ করে যাবে ছাত্রসমাজের প্রাণের স্পন্দন বাংলাদেশ সাধারণ ছাত্র অধিকার সংরক্ষণ পরিষদ দেশ ব্যাপি কমিটি হালনাগাদের কার্যক্রম চলমান রয়েছে, আপনি আছেন তো আপনার জেলার কমিটিতে....??যুক্ত না থাকলে এখনি সময় যুক্ত হওয়ার। আপনাদের হাত ধরেই পরিবর্তন আসবে ইনশাআল্লাহ।।\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HUvYIrLxWSAZ",
    "outputId": "237dd07b-a981-4949-85f4-72ed16517c9d"
   },
   "outputs": [],
   "source": [
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qL_JfyFyXgEP"
   },
   "outputs": [],
   "source": [
    "from rouge import Rouge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NSoXj0WkaQw-",
    "outputId": "8faa5d9f-45c1-4f57-ce82-55702c6c443c"
   },
   "outputs": [],
   "source": [
    "hypothesis = \"স্বাধীনতার পর থেকে চলমান অসুস্থ ধারার রাজনীতি এখনো বহাল।\"\n",
    "reference = \"স্বাধীনতার পর থেকে চলমান অসুস্থ ধারার রাজনীতি এখনো বহাল।\"\n",
    "\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hypothesis, reference)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96B3LVoV0DjP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python [conda env:.conda-tf]",
   "language": "python",
   "name": "conda-env-.conda-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
