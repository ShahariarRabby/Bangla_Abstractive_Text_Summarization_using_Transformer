{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w7es6fpoKAoA",
    "outputId": "4dd7b15b-c292-440c-ae95-e1c973a97c42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5KSSto1yK2AJ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "bz8nJse7KXjB",
    "outputId": "25ba6233-07c3-421c-a5db-639e649764cc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-3f189e11-6619-4edb-8ae0-ea1266debf94\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>আমি জানি আমার এই লেখা,টির জন্য আমাকে অনেক গালম...</td>\n",
       "      <td>বাংলাদেশে কোচিং বানিজ্য বন্ধ এখন সময়ের দাবি</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>একটা ভাষায় তুলনামূলক ভাবে অনেক বেশি মানুষ কথা ...</td>\n",
       "      <td>বাংলা ভাষার প্রযুক্তি নিয়ে আমাদের আরো অনেক বেশ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>আমাদের ফেব্রুয়ারি মাসটি ভাষার মাস। এর বাইরেও ত...</td>\n",
       "      <td>যদি শিশুরা বই পড়ার অভ্যাস করে তাহলে সারা জীবনে...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>আমাকে যদি কেউ কখনো জিজ্ঞেস করে বাংলাদেশের সবচে...</td>\n",
       "      <td>বাংলাদেশে সব স্তরে নারীর ক্ষমতায়নের জন্য আরও অ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>মানুষের মুখ খুব শক্তিশালী এক জিনিস। মানুষ যেটা...</td>\n",
       "      <td>ভালো কথা বল, নয়ত চুপ থাকো</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3f189e11-6619-4edb-8ae0-ea1266debf94')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-3f189e11-6619-4edb-8ae0-ea1266debf94 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-3f189e11-6619-4edb-8ae0-ea1266debf94');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  আমি জানি আমার এই লেখা,টির জন্য আমাকে অনেক গালম...   \n",
       "1  একটা ভাষায় তুলনামূলক ভাবে অনেক বেশি মানুষ কথা ...   \n",
       "2  আমাদের ফেব্রুয়ারি মাসটি ভাষার মাস। এর বাইরেও ত...   \n",
       "3  আমাকে যদি কেউ কখনো জিজ্ঞেস করে বাংলাদেশের সবচে...   \n",
       "4  মানুষের মুখ খুব শক্তিশালী এক জিনিস। মানুষ যেটা...   \n",
       "\n",
       "                                             Summary  \n",
       "0        বাংলাদেশে কোচিং বানিজ্য বন্ধ এখন সময়ের দাবি  \n",
       "1  বাংলা ভাষার প্রযুক্তি নিয়ে আমাদের আরো অনেক বেশ...  \n",
       "2  যদি শিশুরা বই পড়ার অভ্যাস করে তাহলে সারা জীবনে...  \n",
       "3  বাংলাদেশে সব স্তরে নারীর ক্ষমতায়নের জন্য আরও অ...  \n",
       "4                          ভালো কথা বল, নয়ত চুপ থাকো  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_excel(\"drive/My Drive/Colab Notebooks/Text Summarization/text_summarization.xlsx\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mPNIN8iqJNQz"
   },
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EwN5of3ZJYkz",
    "outputId": "7b936b5a-de45-4e8e-f2f4-66c136dc067c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text       0\n",
       "Summary    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "0_ScD3LTK0a_"
   },
   "outputs": [],
   "source": [
    "document = df['Text']\n",
    "summary = df['Summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L0NXAlEKLRwW",
    "outputId": "40170720-a57f-429e-f3d7-965ad28f7abd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"গতরাতে আব্বু তার ফোন নিয়ে আমার কাছে এসে বললো, \"বাবা, ফেইসবুকে কোন একটা পোস্টে কিভাবে প্রাইভেসি সেট করে, একটু দেখায় দেও তো\"আমি তাকে একবার দেখালাম প্রসেসটা ... কিন্তু বয়সের কারণে খালি চোখে আব্বু ডান পাশের ছোট ডট দেখতে পাচ্ছিলো না ... বারবার বলতেছিলো, \"কই? কই ক্লিক করবো?\"প্রায় ৫ বার দেখানোর পরেও যখন আব্বু উল্টাপাল্টা ক্লিক করতেসিলো, প্রচণ্ড মেজাজ খারাপ হইলো ... আমি বেখেয়ালে একটু রাগের সুরে বললাম, \"এই সিম্পল জিনিস পারতেছো না? ধুরর\"এই কথাটা বলার সাথে সাথে আব্বুর মুখটা শুকনা হয়ে গেলো ... আমি সাথে সাথে বুঝতে পারলাম, মানুষটাকে আমি ছোট্ট একটা কথা দিয়ে অনেক বড় একটা কষ্ট দিয়ে ফেলসি ... \\'সরি\\' বলে আবার ভালোভাবে প্রসেসটা বুঝায় বললাম !!আমার ধারণা, নিজের অজান্তেই খুব ছোট ছোট কথা দিয়ে আমরা মানুষকে আঘাত করে ফেলি ... পৃথিবীতে সবাই সবকিছু পারে না ... কারো না পারা নিয়ে তার সাথে অপমানের সুরে কথা বললে সে নিজের ভেতর খুব ছোট বোধ করে ... আমার কোন অধিকার নেই কাউকে অপমান করার, কাউকে ছোট করার !!যে বাবার প্রতি আমি বিরক্ত হলাম কারণ সে সামান্য ফোন চালাতে পারছে না, সেই বাবাই ছোট বেলায় কখনো বিরক্ত হয় নি যখন আমি সামান্য হাঁটতেও পারতাম না ... দিনের পর দিন হাতে ধরে ধরে ক্লান্তিহীনভাবে শিখিয়ে গেছে ... তাকে ৫ বার বা ১০ বার ফোন চালানো শিখাতে আমার বিরক্তি আসবে কেন?নিজেকে অন্য মানুষটার জায়গায় কল্পনা করলে বুঝা যায়, কেউ আমার অপারগতা নিয়ে তাচ্ছিল্যের সুরে কথা বললে কতটা কষ্ট লাগে ... হয়তো কোন একটা ব্যাপার আমার আসলেই পারা উচিত, জানা উচিত, কিন্তু আমি পারি না অথবা জানি না ... এই ব্যর্থতা বা অপারগতার জন্য বোধহয় আমি অপমানিত হওয়া ডিজার্ভ করি না !!এই ক্ষুদ্র জীবনে ইচ্ছাকৃত কিংবা অনিচ্ছাকৃতভাবে আমার বলা কথায় যারা কষ্ট পেয়েছেন, তাদের কাছে আমি ক্ষমাপ্রার্থী ... আমি চেষ্টা করবো কখনোই কাউকে তার অজ্ঞতার জন্য ছোট না করতে !!কোন একটা বিষয় না জানাটা কোন অপরাধ না ... কিন্তু সেটা না জানার জন্য কাউকে তাচ্ছিল্য করাটা অবশ্যই অপরাধ !!আমার ধারণা, আমাদের আশেপাশে এমন কিছু মানুষ আছে যারা কথায় কথায় আমাদের ছোট করে, এই মানুষগুলোর আচরণের জন্য আমরা নিজের ভেতর ডিপ্রেসন তৈরি করে ফেলি ... যে মানুষগুলো প্রতিনিয়ত আমার অজ্ঞতা কিংবা অপারগতা নিয়ে অপমানের সুরে কথা বলে, তাদের থেকে দূরে থাকাই ভালো ... অন্যের কথার কারণে নিজের ভেতরে অপমানবোধের যে অনুভূতিটা জন্মায়, তার চেয়ে ভয়ংকর তীব্র অনুভূতি আর একটাওনেই !!',\n",
       " 'সবকিছুর সাথে আপোষ করা যায়,কিন্তু নিজের আত্মসম্মানকে যে আঘাত করে,তার সাথে আপোষ করার প্রশ্নই আসেনা ')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document[30], summary[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vanMVt32tQ1y",
    "outputId": "5981db53-236a-47ba-9acf-9b078a28ffd5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1023"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SN34pX5Xtnl6"
   },
   "outputs": [],
   "source": [
    "c1 = dict(df.Text.str.split(expand=True).stack().value_counts())\n",
    "c1 = dict(sorted(c1.items(), key=lambda x: x[1], reverse=True))\n",
    "c2 = dict(df.Summary.str.split(expand=True).stack().value_counts())\n",
    "c2 = dict(sorted(c2.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6FagwZYt5yI",
    "outputId": "0bae1039-d68b-46b2-cc9e-049ffbcc88cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'না': 154,\n",
       " 'মানুষ': 70,\n",
       " 'জন্য': 63,\n",
       " 'করে': 62,\n",
       " 'ভালোবাসা': 62,\n",
       " 'না।': 56,\n",
       " 'অনেক': 54,\n",
       " 'করা': 48,\n",
       " 'মানুষের': 45,\n",
       " 'করতে': 45,\n",
       " 'আর': 45,\n",
       " 'হবে': 44,\n",
       " 'সব': 43,\n",
       " 'ভালো': 42,\n",
       " 'হয়': 39,\n",
       " 'কিছু': 39,\n",
       " 'যে': 38,\n",
       " 'সাথে': 34,\n",
       " 'এই': 33,\n",
       " 'কষ্ট': 32,\n",
       " 'এর': 32,\n",
       " 'থেকে': 32,\n",
       " 'নিজের': 32,\n",
       " 'ভালোবাসার': 32,\n",
       " 'কোনো': 31,\n",
       " 'সময়': 30,\n",
       " 'ও': 30,\n",
       " 'করার': 28,\n",
       " 'কথা': 28,\n",
       " 'হতে': 28,\n",
       " 'যায়': 28,\n",
       " ',': 28,\n",
       " 'তার': 26,\n",
       " 'নেই': 26,\n",
       " 'একটি': 25,\n",
       " 'উচিত': 25,\n",
       " 'থাকে': 25,\n",
       " 'হয়ে': 24,\n",
       " 'আমাদের': 24,\n",
       " 'কেউ': 24,\n",
       " 'কারো': 24,\n",
       " 'প্রতি': 24,\n",
       " 'পারে': 23,\n",
       " 'প্রেম': 21,\n",
       " '।': 21,\n",
       " 'আমরা': 21,\n",
       " 'মানুষকে': 21,\n",
       " 'উপর': 20,\n",
       " 'একজন': 20,\n",
       " 'যায়।': 20,\n",
       " 'জীবন': 20,\n",
       " 'এক': 20,\n",
       " 'হবে।': 19,\n",
       " 'হয়।': 19,\n",
       " 'নিজেকে': 19,\n",
       " 'কাউকে': 19,\n",
       " 'মন': 19,\n",
       " 'থাকা': 18,\n",
       " 'একটা': 18,\n",
       " 'কখনো': 18,\n",
       " 'তাকে': 18,\n",
       " 'এবং': 18,\n",
       " 'নিয়ে': 18,\n",
       " 'এখন': 18,\n",
       " 'করোনা': 17,\n",
       " 'এমন': 17,\n",
       " 'তা': 16,\n",
       " 'চলে': 16,\n",
       " 'সে': 16,\n",
       " 'জীবনে': 16,\n",
       " 'মধ্যে': 16,\n",
       " 'মতো': 16,\n",
       " 'শুধু': 15,\n",
       " 'যাবে': 15,\n",
       " 'সবার': 15,\n",
       " 'হচ্ছে': 15,\n",
       " 'মানে': 15,\n",
       " 'গুলো': 15,\n",
       " 'বেশি': 15,\n",
       " 'উচিত।': 14,\n",
       " 'সম্মান': 14,\n",
       " 'নেই।': 14,\n",
       " 'ভালোবাসতে': 14,\n",
       " 'খারাপ': 14,\n",
       " 'টাকা': 14,\n",
       " 'আছে': 14,\n",
       " 'দিতে': 13,\n",
       " 'জীবনের': 13,\n",
       " 'আগে': 13,\n",
       " 'হলে': 13,\n",
       " 'বলে': 13,\n",
       " 'মাঝে': 13,\n",
       " 'থাকতে': 13,\n",
       " 'কাছে': 13,\n",
       " 'যা': 13,\n",
       " 'না,': 13,\n",
       " 'দরকার': 12,\n",
       " 'নয়': 12,\n",
       " 'বলতে': 12,\n",
       " 'কিছুই': 12,\n",
       " 'সেই': 12,\n",
       " 'দিয়ে': 12,\n",
       " 'দিয়ে': 12,\n",
       " 'নারীর': 12,\n",
       " 'বা': 12,\n",
       " 'রাখতে': 12,\n",
       " 'মনে': 11,\n",
       " 'আপনি': 11,\n",
       " 'করে।': 11,\n",
       " 'কে': 11,\n",
       " 'কারণ': 11,\n",
       " 'রাখা': 11,\n",
       " 'কাজ': 11,\n",
       " 'হওয়া': 11,\n",
       " 'মেনে': 11,\n",
       " 'পাওয়া': 11,\n",
       " 'মানেই': 11,\n",
       " 'সম্পর্ক': 11,\n",
       " 'কি': 10,\n",
       " 'নিতে': 10,\n",
       " 'শেষ': 10,\n",
       " 'চেষ্টা': 10,\n",
       " 'মনের': 10,\n",
       " 'থাকলে': 10,\n",
       " 'আমি': 10,\n",
       " 'নাই।': 10,\n",
       " 'আমার': 10,\n",
       " 'থাকার': 10,\n",
       " 'সবাই': 10,\n",
       " 'দরকার।': 10,\n",
       " 'খুব': 10,\n",
       " 'নিজে': 9,\n",
       " 'চায়': 9,\n",
       " 'সময়ের': 9,\n",
       " 'প্রয়োজন': 9,\n",
       " 'সম্পর্কে': 9,\n",
       " 'যার': 9,\n",
       " 'নারীরা': 9,\n",
       " 'পেতে': 9,\n",
       " 'কষ্টের': 9,\n",
       " 'ইচ্ছা': 9,\n",
       " 'পর': 9,\n",
       " 'বন্ধু': 9,\n",
       " 'দেয়া': 9,\n",
       " 'গুরুত্ব': 9,\n",
       " 'যত': 9,\n",
       " 'বড়': 9,\n",
       " 'থাকবে': 9,\n",
       " 'মূল্য': 9,\n",
       " 'বদলে': 8,\n",
       " 'সবচেয়ে': 8,\n",
       " 'সত্যিকারের': 8,\n",
       " 'ভালো।': 8,\n",
       " 'মেয়েদের': 8,\n",
       " 'নারী': 8,\n",
       " 'অন্যের': 8,\n",
       " 'কোন': 8,\n",
       " 'নতুন': 8,\n",
       " 'দিন': 8,\n",
       " 'তাই': 8,\n",
       " 'পাশে': 8,\n",
       " 'বাংলাদেশে': 8,\n",
       " 'তাদের': 8,\n",
       " 'নষ্ট': 8,\n",
       " 'চেয়ে': 8,\n",
       " 'বই': 8,\n",
       " 'ভাইরাস': 8,\n",
       " 'যদি': 7,\n",
       " 'ঠিক': 7,\n",
       " 'নিয়ন্ত্রণ': 7,\n",
       " 'বড়': 7,\n",
       " 'সত্যি': 7,\n",
       " 'দিনশেষে': 7,\n",
       " 'প্রকাশ': 7,\n",
       " 'বলা': 7,\n",
       " 'দেশের': 7,\n",
       " 'কিন্তু': 7,\n",
       " 'লাগে': 7,\n",
       " 'দেশে': 7,\n",
       " 'দুই': 7,\n",
       " 'নাম': 7,\n",
       " 'সৌন্দর্য': 7,\n",
       " 'খুশি': 7,\n",
       " 'লেখক': 7,\n",
       " 'গেলে': 7,\n",
       " 'সুখ': 7,\n",
       " 'হয়।': 7,\n",
       " 'ভারত': 7,\n",
       " 'ছাড়া': 7,\n",
       " 'ছেলে': 7,\n",
       " 'কঠিন।': 7,\n",
       " 'বাংলাদেশ': 6,\n",
       " 'মেয়েরা': 6,\n",
       " 'একই': 6,\n",
       " 'চাওয়া': 6,\n",
       " 'আরো': 6,\n",
       " 'চেয়ে': 6,\n",
       " 'নয়।': 6,\n",
       " 'কিছুর': 6,\n",
       " 'পরিবর্তন': 6,\n",
       " 'প্রকৃত': 6,\n",
       " 'দেয়': 6,\n",
       " 'দেখা': 6,\n",
       " 'মিথ্যা': 6,\n",
       " 'আছে।': 6,\n",
       " 'করো': 6,\n",
       " 'ছেড়ে': 6,\n",
       " 'জোর': 6,\n",
       " 'আপনার': 6,\n",
       " 'দেখে': 6,\n",
       " 'প্রতিটি': 6,\n",
       " 'অল্পতেই': 6,\n",
       " 'তো': 6,\n",
       " 'পারে।': 6,\n",
       " 'লেখকের': 6,\n",
       " 'আসবে': 6,\n",
       " 'আত্মসম্মান': 6,\n",
       " 'কখনও': 6,\n",
       " 'থাকাটা': 6,\n",
       " 'যেমন': 6,\n",
       " 'ধর্ম': 6,\n",
       " 'ভুল': 6,\n",
       " 'তুমি': 6,\n",
       " 'আপনাকে': 6,\n",
       " 'দেশ': 6,\n",
       " 'খুঁজে': 6,\n",
       " 'চাইতে': 6,\n",
       " 'জিনিস': 6,\n",
       " 'বিশ্বাস': 6,\n",
       " 'পাওয়ার': 5,\n",
       " 'খরচ': 5,\n",
       " 'ভালোবেসে': 5,\n",
       " 'জরুরী।': 5,\n",
       " 'কবি': 5,\n",
       " 'শিক্ষা': 5,\n",
       " 'যাবে।': 5,\n",
       " 'যারা': 5,\n",
       " 'মায়ের': 5,\n",
       " 'পুরুষের': 5,\n",
       " 'সমাজে': 5,\n",
       " 'হলো': 5,\n",
       " 'আটকে': 5,\n",
       " 'বছরের': 5,\n",
       " 'বলার': 5,\n",
       " 'মাধ্যমে': 5,\n",
       " 'দোষ': 5,\n",
       " 'চাই।': 5,\n",
       " 'আসল': 5,\n",
       " 'দায়িত্ব': 5,\n",
       " 'দেওয়ার': 5,\n",
       " 'বুঝে': 5,\n",
       " 'একদিন': 5,\n",
       " 'যাওয়ার': 5,\n",
       " 'পৃথিবীতে': 5,\n",
       " 'তখন': 5,\n",
       " 'চোখে': 5,\n",
       " 'বেঁচে': 5,\n",
       " 'ভালোবাসায়': 5,\n",
       " 'যখন': 5,\n",
       " 'সুন্দর': 5,\n",
       " 'হয়ে': 5,\n",
       " 'নিজেদের': 5,\n",
       " 'কঠিন': 5,\n",
       " 'বাংলা': 5,\n",
       " 'ফেলে': 5,\n",
       " 'দাম': 5,\n",
       " 'হওয়ার': 5,\n",
       " 'আসে': 5,\n",
       " 'প্রশ্ন': 5,\n",
       " 'ছেলেদের': 5,\n",
       " 'ভালোবাসা।': 5,\n",
       " 'যথেষ্ট': 5,\n",
       " 'থাকে।': 5,\n",
       " 'সুখী': 5,\n",
       " 'ক্ষতি': 5,\n",
       " 'সবারই': 4,\n",
       " 'গিয়ে': 4,\n",
       " 'পাবে': 4,\n",
       " 'লুকিয়ে': 4,\n",
       " 'সন্তানের': 4,\n",
       " 'চোখের': 4,\n",
       " 'দেওয়া': 4,\n",
       " 'স্মৃতি': 4,\n",
       " 'তোমাকে': 4,\n",
       " 'হলেও': 4,\n",
       " 'আলো': 4,\n",
       " 'মজুদ': 4,\n",
       " 'আমাকে': 4,\n",
       " 'পিছনে': 4,\n",
       " 'চিন্তা': 4,\n",
       " 'অন্যকে': 4,\n",
       " 'দৃষ্টিভঙ্গি': 4,\n",
       " 'এ': 4,\n",
       " 'সকল': 4,\n",
       " 'গুলা': 4,\n",
       " 'করুন': 4,\n",
       " 'মানা': 4,\n",
       " 'সারাজীবন': 4,\n",
       " 'হার': 4,\n",
       " 'ইচ্ছে': 4,\n",
       " 'বয়স': 4,\n",
       " 'ভুলের': 4,\n",
       " 'সারারাত': 4,\n",
       " 'বেস্ট': 4,\n",
       " 'পায়': 4,\n",
       " 'নারীকে': 4,\n",
       " 'জেগে': 4,\n",
       " 'ছোটখাটো': 4,\n",
       " 'অধিকার': 4,\n",
       " 'ভালোবাসে': 4,\n",
       " 'পড়া': 4,\n",
       " 'মেয়েদের': 4,\n",
       " 'জীবনকে': 4,\n",
       " 'বড়ই': 4,\n",
       " 'গ্রহ': 4,\n",
       " 'দিয়েছে।': 4,\n",
       " 'রাখার': 4,\n",
       " 'ভাষার': 4,\n",
       " 'বসে': 4,\n",
       " 'সেটা': 4,\n",
       " 'পৃথিবীর': 4,\n",
       " 'ছোট': 4,\n",
       " 'তেমনি': 4,\n",
       " 'অপেক্ষা': 4,\n",
       " 'সকলের': 4,\n",
       " 'বিভিন্ন': 4,\n",
       " 'যাই': 4,\n",
       " 'আবেগ': 4,\n",
       " 'প্রকৃতির': 4,\n",
       " 'যাই।': 4,\n",
       " 'পরিচয়': 4,\n",
       " 'ধরে': 4,\n",
       " 'আক্রান্ত': 4,\n",
       " 'পবিত্র': 4,\n",
       " 'সঠিক': 4,\n",
       " 'থেমে': 4,\n",
       " 'আবার': 4,\n",
       " 'দূরে': 4,\n",
       " 'এখনো': 4,\n",
       " 'অতি': 4,\n",
       " 'যুদ্ধ': 4,\n",
       " 'করেছে': 4,\n",
       " 'ভুলে': 4,\n",
       " 'ভাইরাসের': 4,\n",
       " 'করতেন।': 4,\n",
       " 'যেতে': 4,\n",
       " 'পেয়ে': 4,\n",
       " 'গল্প': 4,\n",
       " 'স্কুল': 4,\n",
       " 'অবস্থায়': 4,\n",
       " 'সমাজের': 4,\n",
       " 'মহৎ': 4,\n",
       " 'যায়।': 4,\n",
       " 'সময়': 4,\n",
       " 'কাজের': 4,\n",
       " 'পড়ে': 4,\n",
       " 'স্বাধীনতা': 4,\n",
       " 'আছে,': 4,\n",
       " 'সচেতন': 4,\n",
       " 'সামনে': 4,\n",
       " 'লোক': 4,\n",
       " 'তোমার': 4,\n",
       " 'দেয়ার': 4,\n",
       " 'করি।': 4,\n",
       " 'সুযোগ': 4,\n",
       " 'দেখানো': 4,\n",
       " 'নেয়া': 4,\n",
       " 'সবচাইতে': 4,\n",
       " 'পাল্টায়': 4,\n",
       " 'চুরি': 4,\n",
       " 'ভাল': 4,\n",
       " 'দেয়।': 3,\n",
       " 'বাবা': 3,\n",
       " 'অনুভূতি': 3,\n",
       " 'যায়,': 3,\n",
       " 'তবে': 3,\n",
       " 'মর্ম': 3,\n",
       " 'কবিকে': 3,\n",
       " 'উপভোগ': 3,\n",
       " 'ঘুম': 3,\n",
       " 'মেয়ের': 3,\n",
       " 'হয়েছে।': 3,\n",
       " 'কিশোর': 3,\n",
       " 'সত্য': 3,\n",
       " 'বাংলাদেশের': 3,\n",
       " 'পথে': 3,\n",
       " 'কখন': 3,\n",
       " 'এটি': 3,\n",
       " 'স্বপ্ন': 3,\n",
       " 'মেয়ে': 3,\n",
       " 'কথা।': 3,\n",
       " 'মিষ্টি': 3,\n",
       " 'আগামী': 3,\n",
       " 'গেছে': 3,\n",
       " 'করলে': 3,\n",
       " 'মিস': 3,\n",
       " 'অভাব': 3,\n",
       " 'পাই': 3,\n",
       " 'শক্ত': 3,\n",
       " 'স্বাধীনতার': 3,\n",
       " 'টা': 3,\n",
       " 'আত্মবিশ্বাস': 3,\n",
       " 'রাখা।': 3,\n",
       " 'হোক।': 3,\n",
       " 'ই': 3,\n",
       " 'ভরসা': 3,\n",
       " 'যায়': 3,\n",
       " 'পাওয়া': 3,\n",
       " 'কথার': 3,\n",
       " 'পূরণ': 3,\n",
       " 'এখনও': 3,\n",
       " 'উপহার': 3,\n",
       " 'শক্তিশালী': 3,\n",
       " 'পার্থক্য': 3,\n",
       " 'সংসার': 3,\n",
       " 'ধরনের': 3,\n",
       " 'বঙ্গবন্ধুর': 3,\n",
       " 'মধ্যবিত্ত': 3,\n",
       " 'লাগে।': 3,\n",
       " 'বিচার': 3,\n",
       " 'কাহিনী।': 3,\n",
       " 'অনেক।': 3,\n",
       " 'নয়।': 3,\n",
       " 'জানানো': 3,\n",
       " 'নেওয়া': 3,\n",
       " 'পুরাতন': 3,\n",
       " 'আলাদা।': 3,\n",
       " 'করবেন': 3,\n",
       " 'ঘৃণা': 3,\n",
       " 'সম্ভব': 3,\n",
       " 'প্রাপ্য': 3,\n",
       " 'আল্লাহ': 3,\n",
       " 'জানতে': 3,\n",
       " 'যাকে': 3,\n",
       " 'করা।': 3,\n",
       " 'যাওয়া': 3,\n",
       " 'অতিরিক্ত': 3,\n",
       " 'প্রয়োজন।': 3,\n",
       " 'উচিৎ।': 3,\n",
       " 'লজ্জা': 3,\n",
       " 'মানুষেরই': 3,\n",
       " 'কার': 3,\n",
       " 'যথেষ্ট।': 3,\n",
       " 'মুক্তি': 3,\n",
       " 'জানে।': 3,\n",
       " 'আসল।': 3,\n",
       " 'সাহায্য': 3,\n",
       " 'একা': 3,\n",
       " 'জিনিসের': 3,\n",
       " 'বিরুদ্ধে': 3,\n",
       " 'চেনা': 3,\n",
       " 'বেপারে': 3,\n",
       " 'করেই': 3,\n",
       " 'হবার': 3,\n",
       " 'বাস্তবতা': 3,\n",
       " 'হবে,': 3,\n",
       " 'মিথ্যে': 3,\n",
       " 'হাত': 3,\n",
       " 'দুঃখ': 3,\n",
       " 'পারফেক্ট': 3,\n",
       " 'খুবই': 3,\n",
       " 'উপর।': 3,\n",
       " 'মেয়েরা': 3,\n",
       " 'করুন।': 3,\n",
       " 'লাভ': 3,\n",
       " 'কাল': 3,\n",
       " 'মানুষগুলো': 3,\n",
       " 'সারাদিন': 3,\n",
       " 'তুচ্ছ': 3,\n",
       " 'পুরুষদের': 3,\n",
       " 'বোকা': 3,\n",
       " 'পরে': 3,\n",
       " 'মাথায়': 3,\n",
       " 'মত': 3,\n",
       " 'বিয়া': 3,\n",
       " 'এটা': 3,\n",
       " 'ব্ল্যাক': 3,\n",
       " 'বানানো': 3,\n",
       " 'সিনেমা': 3,\n",
       " 'স্ট্যাটাস': 3,\n",
       " 'টেস্ট': 3,\n",
       " 'ধারনা': 3,\n",
       " 'ফল': 3,\n",
       " 'তেমন': 3,\n",
       " 'হয়েছে': 3,\n",
       " 'দুনিয়াতে': 3,\n",
       " 'পারলে': 3,\n",
       " 'পড়াশোনা': 3,\n",
       " 'ক্ষমতা': 3,\n",
       " 'যুদ্ধে': 3,\n",
       " 'বিশ্বের': 3,\n",
       " 'হাতে': 3,\n",
       " 'তে': 3,\n",
       " 'শাস্তি': 3,\n",
       " 'মেধা': 3,\n",
       " 'বাক': 3,\n",
       " 'সবাইকে': 3,\n",
       " 'ক্রিকেট': 3,\n",
       " 'ঢাকা': 3,\n",
       " 'কাজে': 3,\n",
       " 'পেলে': 3,\n",
       " 'ভীষণ': 3,\n",
       " 'বিয়ে': 3,\n",
       " 'সমান': 3,\n",
       " 'উপায়': 3,\n",
       " 'পানি': 3,\n",
       " 'চাই': 3,\n",
       " 'হিসাব': 3,\n",
       " 'লাগার': 3,\n",
       " 'আসার': 3,\n",
       " 'যাওয়া': 3,\n",
       " 'সফলতা': 3,\n",
       " 'তত': 3,\n",
       " 'মাস্ক': 3,\n",
       " 'এসে': 3,\n",
       " 'স্ট্রিং': 3,\n",
       " 'বিজ্ঞান': 3,\n",
       " 'নির্ভর': 3,\n",
       " 'চুপচাপ': 3,\n",
       " 'হল': 3,\n",
       " 'গুলোর': 3,\n",
       " 'পড়ার': 3,\n",
       " 'সৃষ্টি': 3,\n",
       " 'থাকুক': 3,\n",
       " 'কেন্দ্র': 3,\n",
       " 'অবহেলা': 3,\n",
       " 'প্রস্তুতি': 3,\n",
       " 'কিট': 3,\n",
       " 'অর্থের': 3,\n",
       " 'বলে,': 3,\n",
       " 'মূল': 3,\n",
       " 'বোঝাতে': 3,\n",
       " 'হিসাবে': 3,\n",
       " 'ভাইরাসে': 3,\n",
       " 'কাছের': 3,\n",
       " 'থেকেই': 3,\n",
       " 'বন্ধ': 3,\n",
       " 'আলোর': 3,\n",
       " 'একে': 3,\n",
       " 'শক্তি': 3,\n",
       " 'আগলে': 2,\n",
       " 'নাটকের': 2,\n",
       " 'অন্তরে': 2,\n",
       " '৩': 2,\n",
       " 'যুগে': 2,\n",
       " ',আমি': 2,\n",
       " 'প্রজন্ম': 2,\n",
       " 'বুদ্ধিজীবী': 2,\n",
       " 'ভাষা': 2,\n",
       " 'ইংরেজি': 2,\n",
       " 'প্রযুক্তির': 2,\n",
       " 'কমে': 2,\n",
       " 'মার্চ': 2,\n",
       " 'আকাশ': 2,\n",
       " 'দিবে': 2,\n",
       " 'শ্রদ্ধা।': 2,\n",
       " 'বাসনা।': 2,\n",
       " 'রেখেই': 2,\n",
       " 'খবর': 2,\n",
       " ',সে': 2,\n",
       " 'যায়.': 2,\n",
       " 'ভাঙ্গার': 2,\n",
       " 'কুসংস্কার': 2,\n",
       " 'তাৎপর্য': 2,\n",
       " 'থেকেও': 2,\n",
       " 'ভোলা': 2,\n",
       " 'একসাথে': 2,\n",
       " 'চাওয়া।': 2,\n",
       " 'চেয়েও': 2,\n",
       " 'পারা': 2,\n",
       " 'বয়সে': 2,\n",
       " 'ইমোশন': 2,\n",
       " 'বৃদ্ধ': 2,\n",
       " 'ভালোবাসি': 2,\n",
       " 'কোঠাটান': 2,\n",
       " ',এর': 2,\n",
       " 'বোঝা': 2,\n",
       " 'ছাড়াও': 2,\n",
       " 'চাবে': 2,\n",
       " 'শত': 2,\n",
       " 'আকুল': 2,\n",
       " 'সেরা': 2,\n",
       " 'অভিমানের': 2,\n",
       " 'নির্ভরশীল': 2,\n",
       " 'দেখেই': 2,\n",
       " 'সনাক্তকরনের': 2,\n",
       " 'পাবনা': 2,\n",
       " 'অবস্থান': 2,\n",
       " 'ঘরে': 2,\n",
       " 'বোঝে': 2,\n",
       " 'আবেগের': 2,\n",
       " 'বৃষ্টির': 2,\n",
       " 'কুলখানি': 2,\n",
       " 'সূর্য': 2,\n",
       " 'কেন': 2,\n",
       " 'প্রতিষ্ঠিত': 2,\n",
       " 'রাগ': 2,\n",
       " 'শক্তির': 2,\n",
       " 'মধ্য': 2,\n",
       " 'যোগ': 2,\n",
       " 'বহুবচন': 2,\n",
       " 'টেলিস্কোপের': 2,\n",
       " 'পরিবারের': 2,\n",
       " 'মাধ্যমের': 2,\n",
       " 'জেনেও': 2,\n",
       " 'আসতে': 2,\n",
       " 'ছিল': 2,\n",
       " 'গুলোকে': 2,\n",
       " 'কণিকা': 2,\n",
       " 'বুঝতে': 2,\n",
       " 'জেদ': 2,\n",
       " 'প্রোটন': 2,\n",
       " 'এনে': 2,\n",
       " 'ব্যবস্থা': 2,\n",
       " 'কম্পনের': 2,\n",
       " 'হিসেবে': 2,\n",
       " 'পদার্থের': 2,\n",
       " 'দ্বিতীয়': 2,\n",
       " 'মেশিন': 2,\n",
       " 'রাখে।': 2,\n",
       " 'ব্যাবহার': 2,\n",
       " 'মায়ায়': 2,\n",
       " 'অসাধারণ': 2,\n",
       " 'সম্মানী': 2,\n",
       " 'চিনে।': 2,\n",
       " 'মোকাবিলায়': 2,\n",
       " 'তুলে।': 2,\n",
       " 'একদিন।': 2,\n",
       " 'ছেলের': 2,\n",
       " 'দেখতে': 2,\n",
       " 'প্রেমে': 2,\n",
       " 'ভালোবাসলে': 2,\n",
       " 'বিচারিক': 2,\n",
       " 'ম্যাজিস্ট্রেটদের': 2,\n",
       " 'প্রয়োজনে': 2,\n",
       " 'কোরিয়া': 2,\n",
       " 'মৃত্যুর': 2,\n",
       " 'যাওয়া।': 2,\n",
       " 'বাঙালি': 2,\n",
       " 'শরীর': 2,\n",
       " 'জাতির': 2,\n",
       " 'দেহ': 2,\n",
       " 'করো।': 2,\n",
       " 'ছায়াপথ।': 2,\n",
       " 'দিনের': 2,\n",
       " 'কখনোই': 2,\n",
       " 'মূলত': 2,\n",
       " 'ঘুমাতে': 2,\n",
       " 'রাষ্ট্র': 2,\n",
       " 'বেশি।': 2,\n",
       " 'বললে': 2,\n",
       " 'অসুস্থ': 2,\n",
       " 'নির্দেশ': 2,\n",
       " 'ইভেন': 2,\n",
       " 'অর্থ': 2,\n",
       " 'জন্মতোই': 2,\n",
       " 'পাশের': 2,\n",
       " 'কাকে': 2,\n",
       " 'কালের': 2,\n",
       " 'সময়টা': 2,\n",
       " 'রাখি।': 2,\n",
       " 'নারীরাই': 2,\n",
       " 'জাগরণের': 2,\n",
       " 'প্রধান': 2,\n",
       " 'অন্তরায়': 2,\n",
       " 'জন্মদিন': 2,\n",
       " 'বোকামি': 2,\n",
       " '\"প্রাক্তন\"': 2,\n",
       " 'কত': 2,\n",
       " 'এক্সদের': 2,\n",
       " 'ব্যাপার।': 2,\n",
       " 'দিক': 2,\n",
       " 'বোঝার': 2,\n",
       " 'বিশ্ববিদ্যালয়ের': 2,\n",
       " 'বরাদ্দ': 2,\n",
       " 'গেছে।': 2,\n",
       " 'ছাড়াই': 2,\n",
       " 'ফেরানো': 2,\n",
       " 'স্যানিটাইজার': 2,\n",
       " 'ফেসবুকে': 2,\n",
       " 'নারীদের': 2,\n",
       " 'পরিষ্কার': 2,\n",
       " 'পরিপূর্ণভাবে': 2,\n",
       " 'খেলায়': 2,\n",
       " 'সাবান': 2,\n",
       " 'ছুটে': 2,\n",
       " 'হ্যান্ড': 2,\n",
       " 'করিনি': 2,\n",
       " 'অতিমুনাফার': 2,\n",
       " 'মৃত্যু': 2,\n",
       " 'যাবেন।': 2,\n",
       " 'এমনেই': 2,\n",
       " 'ভুলার': 2,\n",
       " 'ভালোবেসেছি।': 2,\n",
       " 'মানিয়ে': 2,\n",
       " 'সংখ্যা': 2,\n",
       " 'ঘৃনা': 2,\n",
       " 'ক্রিকেটের': 2,\n",
       " 'চাঞ্চল্য': 2,\n",
       " 'শিক্ষার্থীরা': 2,\n",
       " 'আব্দুল': 2,\n",
       " 'হামিদ': 2,\n",
       " 'করি': 2,\n",
       " 'ভেজাল': 2,\n",
       " 'পাশাপাশি': 2,\n",
       " 'বয়সের': 2,\n",
       " 'কিছুকে': 2,\n",
       " 'পড়াশুনা': 2,\n",
       " 'মানায়': 2,\n",
       " 'অনেকেই': 2,\n",
       " 'জাতীয়': 2,\n",
       " 'কোচিং': 2,\n",
       " 'উপসর্গ': 2,\n",
       " 'শব্দটা': 2,\n",
       " 'ভেতরে': 2,\n",
       " 'নানা': 2,\n",
       " 'মুখ': 2,\n",
       " 'লিখে': 2,\n",
       " 'কষ্টের।': 2,\n",
       " 'চাইলে': 2,\n",
       " 'অল্প': 2,\n",
       " 'লেখা': 2,\n",
       " 'ইতিহাস': 2,\n",
       " 'জরুরি।': 2,\n",
       " 'মেরে': 2,\n",
       " 'পারছি': 2,\n",
       " 'যাওয়ায়': 2,\n",
       " 'করোনায়': 2,\n",
       " 'জাতি': 2,\n",
       " 'নেয়ার': 2,\n",
       " 'বসবাস': 2,\n",
       " 'নবাবপুর': 2,\n",
       " 'গুজব': 2,\n",
       " 'বিভ্রান্ত': 2,\n",
       " 'ব্যস্ত': 2,\n",
       " 'যত্ন': 2,\n",
       " 'চায়।': 2,\n",
       " 'গভীরতা': 2,\n",
       " 'শোনার': 2,\n",
       " 'ধর্ষিতা': 2,\n",
       " 'অতীত': 2,\n",
       " 'হারিয়ে': 2,\n",
       " 'স্যরি': 2,\n",
       " 'ঘটে': 2,\n",
       " 'করেও': 2,\n",
       " 'দিনে': 2,\n",
       " 'ধর্যের': 2,\n",
       " 'রাখুন।': 2,\n",
       " 'শিখা': 2,\n",
       " 'মনুষ্যত্বহীন': 2,\n",
       " 'হওয়া': 2,\n",
       " 'ভিতরের': 2,\n",
       " 'মানুষই': 2,\n",
       " 'ভাষণ।': 2,\n",
       " 'বিরত': 2,\n",
       " 'মেয়েকে': 2,\n",
       " 'সামান্য': 2,\n",
       " 'মার্চের': 2,\n",
       " 'মায়া': 2,\n",
       " 'গোপন': 2,\n",
       " 'ধরণ': 2,\n",
       " 'অভিযোগ।': 2,\n",
       " 'ধরণের': 2,\n",
       " 'রোমান্টিক': 2,\n",
       " 'অভিনয়': 2,\n",
       " 'মানুষগুলোর': 2,\n",
       " 'বিয়ের': 2,\n",
       " 'বাড়ির': 2,\n",
       " 'কেউই': 2,\n",
       " 'দিয়েছে': 2,\n",
       " 'আপনজন': 2,\n",
       " 'ফ্যামিলির্': 2,\n",
       " 'শিখুন।': 2,\n",
       " 'অপমান': 2,\n",
       " 'রেজাল্ট': 2,\n",
       " 'মানুষটির': 2,\n",
       " 'আজ': 2,\n",
       " 'সাদা': 2,\n",
       " 'কাজই': 2,\n",
       " 'ব্যক্তিত্ব': 2,\n",
       " 'গুছিয়ে': 2,\n",
       " 'দিবেস': 2,\n",
       " 'দিনই।': 2,\n",
       " 'ছড়িয়ে': 2,\n",
       " 'যেকোনো': 2,\n",
       " 'পারি।': 2,\n",
       " 'ভাবে': 2,\n",
       " 'শুভেচ্ছা': 2,\n",
       " 'খুন': 2,\n",
       " 'চাইলেই': 2,\n",
       " 'লাগান।': 2,\n",
       " 'ফ্রেন্ড': 2,\n",
       " 'সময়কে': 2,\n",
       " 'জন্যই': 2,\n",
       " 'জাজমেন্ট': 2,\n",
       " 'প্রিয়': 2,\n",
       " 'রক্ষা': 2,\n",
       " '৭': 2,\n",
       " 'করানো': 2,\n",
       " 'নাই': 2,\n",
       " 'নিদৃষ্ট': 2,\n",
       " 'প্রত্যেক': 2,\n",
       " 'ঐসময়।': 2,\n",
       " 'একুশ': 2,\n",
       " 'রয়েছে।': 2,\n",
       " 'কাউকেও': 2,\n",
       " 'ধ্বংস': 2,\n",
       " 'দিবস': 2,\n",
       " 'মজাই': 2,\n",
       " 'চিঠি': 2,\n",
       " 'সবচেয়ে': 2,\n",
       " 'দিয়া': 2,\n",
       " 'সম্পত্তি': 2,\n",
       " 'অন্য': 2,\n",
       " 'নির্দিষ্ট': 2,\n",
       " 'দিন।': 2,\n",
       " 'সোজা': 2,\n",
       " 'মান': 2,\n",
       " 'নিখুঁত': 2,\n",
       " 'ভাবছি': 2,\n",
       " 'ডাক্তারের': 2,\n",
       " 'প্রভাব': 2,\n",
       " 'অপরিসীম': 2,\n",
       " 'তারাই': 2,\n",
       " 'দিয়েও': 2,\n",
       " 'অপ্রত্যাশিত': 2,\n",
       " 'ধরা': 2,\n",
       " 'ট্রাফিক': 2,\n",
       " 'সিদ্ধান্ত': 2,\n",
       " 'ততো': 2,\n",
       " 'পরীক্ষা': 2,\n",
       " 'বিচারের': 2,\n",
       " 'অপরকে': 2,\n",
       " 'মুখে': 2,\n",
       " 'পারার': 2,\n",
       " 'পারবে': 2,\n",
       " 'স্বামীর': 2,\n",
       " 'পৃথিবী': 2,\n",
       " 'বিয়ে': 2,\n",
       " 'দ্বারা': 2,\n",
       " 'পর্যন্ত': 2,\n",
       " 'পাবেন': 2,\n",
       " 'শুরু': 2,\n",
       " 'আপোষ': 2,\n",
       " 'গুন': 2,\n",
       " 'কি?': 2,\n",
       " 'বৃদ্ধি': 2,\n",
       " 'ভালোবাসাকে': 2,\n",
       " 'সময়ে': 2,\n",
       " 'পায়।': 2,\n",
       " 'বিপদে': 2,\n",
       " 'জীবনটা': 2,\n",
       " 'সৈয়দ': 2,\n",
       " 'বিদেশে': 2,\n",
       " 'ভালবাসার': 2,\n",
       " 'হয়তো': 2,\n",
       " 'নেওয়ার': 2,\n",
       " 'পুরোনো': 2,\n",
       " 'সেবা': 2,\n",
       " 'জটিল': 2,\n",
       " 'পরিশ্রম': 2,\n",
       " 'ধারণ': 2,\n",
       " 'কোনও': 2,\n",
       " 'আনন্দ': 2,\n",
       " 'শুদ্ধতম': 2,\n",
       " 'সাহিত্য': 2,\n",
       " 'নামে': 2,\n",
       " 'বাঁচতে': 2,\n",
       " 'আল্লাহর': 2,\n",
       " 'একাকিত্ব': 2,\n",
       " 'মার': 2,\n",
       " 'সর্বনাশ': 2,\n",
       " 'হউক': 2,\n",
       " 'হক': 2,\n",
       " 'চোরের': 2,\n",
       " 'হাজার': 2,\n",
       " 'নিস্পাপ': 2,\n",
       " 'লেখার': 2,\n",
       " 'চাহিদা': 2,\n",
       " 'ব্যবসা': 2,\n",
       " 'যাচ্ছি।': 2,\n",
       " 'এখানেই': 2,\n",
       " 'নিজেকেই': 2,\n",
       " 'সমস্যাটা': 2,\n",
       " 'ভেবে': 2,\n",
       " 'করেছি': 2,\n",
       " 'পাল্টা': 2,\n",
       " 'বিদেশ': 2,\n",
       " 'সারা': 2,\n",
       " 'দিকে': 2,\n",
       " 'সয়ে': 2,\n",
       " 'মাকে': 2,\n",
       " 'তাহলে': 2,\n",
       " 'হিসেব': 2,\n",
       " 'বের': 2,\n",
       " 'উঠতে': 2,\n",
       " 'পাল্টে': 2,\n",
       " 'ধুকে': 2,\n",
       " 'দুর্বোধ্য': 2,\n",
       " 'তখনই': 2,\n",
       " 'অদ্ভুত': 2,\n",
       " 'বিশ্ববিদ্যালয়': 2,\n",
       " 'সম্পদ': 2,\n",
       " 'জ্ঞান': 2,\n",
       " 'নিজেরই': 2,\n",
       " 'প্রতিনিয়ত': 2,\n",
       " 'কেও': 2,\n",
       " 'করাটা': 2,\n",
       " 'আকর্ষণ': 2,\n",
       " 'উপকার': 2,\n",
       " 'স্বীকার': 2,\n",
       " 'গল্প।': 2,\n",
       " 'তারা': 2,\n",
       " 'খানের': 2,\n",
       " 'বছর': 2,\n",
       " 'মানুুষ': 2,\n",
       " '.': 2,\n",
       " 'অর্ধেক': 2,\n",
       " 'ভাগ্য': 2,\n",
       " 'আজম': 2,\n",
       " 'করবে।': 2,\n",
       " 'পুরুষ': 2,\n",
       " 'বিড়ম্বনা': 2,\n",
       " 'চুপ': 2,\n",
       " 'মেলাতে': 2,\n",
       " 'পেশা': 2,\n",
       " 'দাবি': 2,\n",
       " 'সমস্যার': 2,\n",
       " 'শহরে': 2,\n",
       " 'একেকটা': 2,\n",
       " 'চলার': 2,\n",
       " 'রুখে': 2,\n",
       " 'ব্যাস্ত': 2,\n",
       " 'সহজেই': 2,\n",
       " 'প্রদান': 1,\n",
       " 'রানে': 1,\n",
       " 'অন্যতম।': 1,\n",
       " '\"জিরো\"': 1,\n",
       " 'মুশফিক': 1,\n",
       " 'তিনটি': 1,\n",
       " 'অনুশোচনা': 1,\n",
       " 'গবেষণা': 1,\n",
       " 'সর্বোচ্চ': 1,\n",
       " 'করেছেন।': 1,\n",
       " 'স্বাস্থ্যে': 1,\n",
       " 'আউট': 1,\n",
       " 'পশুর': 1,\n",
       " 'হলো।': 1,\n",
       " 'রিটেক': 1,\n",
       " 'দল।': 1,\n",
       " 'উপভোগে': 1,\n",
       " 'অভিযোগ': 1,\n",
       " 'ক্যাসিনো': 1,\n",
       " 'শেয়ার': 1,\n",
       " 'বাজার': 1,\n",
       " 'দুইটাই': 1,\n",
       " 'জুয়া।': 1,\n",
       " '১০': 1,\n",
       " 'কোটি': 1,\n",
       " 'গুনতে': 1,\n",
       " 'অবশেষে': 1,\n",
       " 'পেলো': 1,\n",
       " 'লেগ': 1,\n",
       " 'স্পিনার।': 1,\n",
       " 'জ্বালিয়ে': 1,\n",
       " 'মারার': 1,\n",
       " 'মজা': 1,\n",
       " 'কোথাও': 1,\n",
       " 'কমের': 1,\n",
       " 'আকুতি।': 1,\n",
       " 'বাসের': 1,\n",
       " 'লোকাল': 1,\n",
       " 'পেরিয়ে': 1,\n",
       " 'খুশি।': 1,\n",
       " 'কলিজা': 1,\n",
       " 'দাদীকে': 1,\n",
       " 'ভোগায়।': 1,\n",
       " 'অমরত্ব': 1,\n",
       " 'নির্বাচন': 1,\n",
       " 'খালি': 1,\n",
       " 'নাচবো': 1,\n",
       " 'খাবো।': 1,\n",
       " 'পানামা': 1,\n",
       " 'সবসময়': 1,\n",
       " 'বহিরাগত।': 1,\n",
       " 'ক্ষমতাবান।': 1,\n",
       " 'গুলো।': 1,\n",
       " 'জীবন্ত': 1,\n",
       " 'করোনো': 1,\n",
       " 'গড়ে': 1,\n",
       " 'তোলার': 1,\n",
       " 'সফলতা।': 1,\n",
       " 'এরা': 1,\n",
       " 'ছাত্র': 1,\n",
       " 'কিংবদন্তী।': 1,\n",
       " 'শূন্যের': 1,\n",
       " 'সহায়তার': 1,\n",
       " 'এত্ত': 1,\n",
       " 'বোর্ড': 1,\n",
       " 'হোয়াইট': 1,\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "id": "jU5Y2x-yu5Ff",
    "outputId": "f7138270-6cfb-4caa-f4c7-8f312f120b97"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-f720bb14-ce67-4630-a762-072efb67496a\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>না</td>\n",
       "      <td>1520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>করে</td>\n",
       "      <td>1502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>আমার</td>\n",
       "      <td>1125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>আমি</td>\n",
       "      <td>1097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>আর</td>\n",
       "      <td>1097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>এই</td>\n",
       "      <td>914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>তার</td>\n",
       "      <td>868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>না।</td>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>একটা</td>\n",
       "      <td>761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>যে</td>\n",
       "      <td>747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>থেকে</td>\n",
       "      <td>716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>হয়ে</td>\n",
       "      <td>641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>সে</td>\n",
       "      <td>638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>...</td>\n",
       "      <td>628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>সাথে</td>\n",
       "      <td>620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>জন্য</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>কি</td>\n",
       "      <td>562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>তোমার</td>\n",
       "      <td>547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>করতে</td>\n",
       "      <td>529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>কথা</td>\n",
       "      <td>501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f720bb14-ce67-4630-a762-072efb67496a')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-f720bb14-ce67-4630-a762-072efb67496a button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-f720bb14-ce67-4630-a762-072efb67496a');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "    Words  Count\n",
       "0      না   1520\n",
       "1     করে   1502\n",
       "2    আমার   1125\n",
       "3     আমি   1097\n",
       "4      আর   1097\n",
       "5      এই    914\n",
       "6     তার    868\n",
       "7     না।    768\n",
       "8    একটা    761\n",
       "9      যে    747\n",
       "10   থেকে    716\n",
       "11    হয়ে    641\n",
       "12     সে    638\n",
       "13    ...    628\n",
       "14   সাথে    620\n",
       "15   জন্য    591\n",
       "16     কি    562\n",
       "17  তোমার    547\n",
       "18   করতে    529\n",
       "19    কথা    501"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = pd.DataFrame(c1.items(), columns=['Words', 'Count'])\n",
    "d1 = d1.head(20)\n",
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "id": "mQG517rQuINx",
    "outputId": "03c3cecb-90cd-48be-cf80-88715b38d18b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-c6e0d0c6-8996-48c0-8456-1164c4b8d4b7\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>না</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>মানুষ</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>জন্য</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>করে</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ভালোবাসা</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>না।</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>অনেক</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>করা</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>মানুষের</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>করতে</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>আর</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>হবে</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>সব</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ভালো</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>হয়</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>কিছু</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>যে</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>সাথে</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>এই</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>কষ্ট</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c6e0d0c6-8996-48c0-8456-1164c4b8d4b7')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-c6e0d0c6-8996-48c0-8456-1164c4b8d4b7 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-c6e0d0c6-8996-48c0-8456-1164c4b8d4b7');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "       Words  Count\n",
       "0         না    154\n",
       "1      মানুষ     70\n",
       "2       জন্য     63\n",
       "3        করে     62\n",
       "4   ভালোবাসা     62\n",
       "5        না।     56\n",
       "6       অনেক     54\n",
       "7        করা     48\n",
       "8    মানুষের     45\n",
       "9       করতে     45\n",
       "10        আর     45\n",
       "11       হবে     44\n",
       "12        সব     43\n",
       "13      ভালো     42\n",
       "14        হয়     39\n",
       "15      কিছু     39\n",
       "16        যে     38\n",
       "17      সাথে     34\n",
       "18        এই     33\n",
       "19      কষ্ট     32"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2 = pd.DataFrame(c2.items(), columns=['Words', 'Count'])\n",
    "d2 = d2.head(20)\n",
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "6sXflk5xuqhL",
    "outputId": "69dfe4e0-cca7-4570-9faa-947a14d39949"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import numpy as np\n",
    "prop = fm.FontProperties(fname='/content/drive/My Drive/Colab Notebooks/kalpurush.ttf')\n",
    "plt.barh(d1.Words,d1.Count)\n",
    "plt.yticks(d1.Words,fontproperties=prop)\n",
    "plt.title('Most Frequent Word in Bengali Text')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "ad2xMkZuvJtL",
    "outputId": "c00f924b-a187-4fa1-c315-90f79066d505"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.barh(d2.Words,d2.Count)\n",
    "plt.yticks(d2.Words,fontproperties=prop)\n",
    "plt.title('Most Frequent Word in Bengali Text Summary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70qGA1wNMB8e",
    "outputId": "21ec9c6a-e83a-49a3-98f8-20969efb2f97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    <go> বাংলাদেশে কোচিং বানিজ্য বন্ধ এখন সময়ের দা...\n",
       "1    <go> বাংলা ভাষার প্রযুক্তি নিয়ে আমাদের আরো অনে...\n",
       "2    <go> যদি শিশুরা বই পড়ার অভ্যাস করে তাহলে সারা ...\n",
       "3    <go> বাংলাদেশে সব স্তরে নারীর ক্ষমতায়নের জন্য ...\n",
       "4                <go> ভালো কথা বল, নয়ত চুপ থাকো <stop>\n",
       "Name: Summary, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = summary.apply(lambda x: '<go> ' + str(x) + ' <stop>')\n",
    "summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "8HvwpXkyME9l"
   },
   "outputs": [],
   "source": [
    "# since < and > from default tokens cannot be removed\n",
    "filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n",
    "oov_token = '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "BVV8LssUMUrY"
   },
   "outputs": [],
   "source": [
    "document_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token)\n",
    "summary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "0Sj_k9u8Mb1R"
   },
   "outputs": [],
   "source": [
    "document_tokenizer.fit_on_texts(document)\n",
    "summary_tokenizer.fit_on_texts(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "r9adUMBPMmlM"
   },
   "outputs": [],
   "source": [
    "inputs = document_tokenizer.texts_to_sequences(document)\n",
    "targets = summary_tokenizer.texts_to_sequences(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2AOW-f0HMqhc",
    "outputId": "e651aab1-4348-493b-fc06-0cff93be78af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[153, 546, 941, 379, 61, 134, 547]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_tokenizer.texts_to_sequences([\"বাংলাদেশে কোচিং বানিজ্য বন্ধ এখন সময়ের দাবি\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NVk8QZJfMw9m",
    "outputId": "3a559590-b9b2-4db6-9e8f-6f215cd7d5ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['বাংলাদেশে দাবি প্রযুক্তি বন্ধ এখন সময়ের তাহলে']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_tokenizer.sequences_to_texts([[153, 547, 942, 379, 61, 134, 548]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pHL2dMQmM9mo",
    "outputId": "175e8f44-ef45-4254-9189-332c9abb97fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23761, 2813)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_vocab_size = len(document_tokenizer.word_index) + 1\n",
    "decoder_vocab_size = len(summary_tokenizer.word_index) + 1\n",
    "\n",
    "encoder_vocab_size, decoder_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "dfaZi7eVNBXi"
   },
   "outputs": [],
   "source": [
    "document_lengths = pd.Series([len(x) for x in document])\n",
    "summary_lengths = pd.Series([len(x) for x in summary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r7dSrFUNNDgC",
    "outputId": "96647662-cc55-49fa-df6a-1c040a01dd68"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1023.000000\n",
       "mean      810.001955\n",
       "std       886.880381\n",
       "min        42.000000\n",
       "25%       256.500000\n",
       "50%       518.000000\n",
       "75%      1025.500000\n",
       "max      8262.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jOpgw0ZGNFzV",
    "outputId": "0aa93fa1-ef11-4530-c15b-c3ab22f53418"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1023.000000\n",
       "mean       54.786901\n",
       "std        23.044064\n",
       "min        23.000000\n",
       "25%        41.000000\n",
       "50%        51.000000\n",
       "75%        63.000000\n",
       "max       462.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "8juPbc01NIr9"
   },
   "outputs": [],
   "source": [
    "encoder_maxlen = 250\n",
    "decoder_maxlen = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ItReQzKWNMFg"
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
    "targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=decoder_maxlen, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "xG0C_tklNPwy"
   },
   "outputs": [],
   "source": [
    "inputs = tf.cast(inputs, dtype=tf.int32)\n",
    "targets = tf.cast(targets, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "89rZHD26NSgF"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 2000\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "CoWgTzpKNV94"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "F6b0UxwHNZSf"
   },
   "outputs": [],
   "source": [
    "def get_angles(position, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return position * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "HQVUljkTNbuN"
   },
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(\n",
    "        np.arange(position)[:, np.newaxis],\n",
    "        np.arange(d_model)[np.newaxis, :],\n",
    "        d_model\n",
    "    )\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "hW2wInusNeLI"
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "6Xq1sTtjNgyi"
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "c9-KePeRNjN4"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "ZA7drIKONlcK"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "        output = self.dense(concat_attention)\n",
    "            \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "G3mHNuHLNoiU"
   },
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),\n",
    "        tf.keras.layers.Dense(d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "5lrg_xJ9N4tx"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "FKaLacvSN7w7"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "dQrnlVMWOBuF"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "    \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "DGC5vXJeOHMh"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "    \n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "ZFTmtmN5OJVM"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
    "\n",
    "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "Hrcb29sJOMR1"
   },
   "outputs": [],
   "source": [
    "# hyper-params\n",
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "EPOCHS = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "GruvDrXiOPmT"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "mITWV3C8OR3S"
   },
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "lXhjR4nzOUVe"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "9rGOkAf3OWzs"
   },
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "37hPdmtAOY_o"
   },
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "uERVOad5OdOV"
   },
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers, \n",
    "    d_model, \n",
    "    num_heads, \n",
    "    dff,\n",
    "    encoder_vocab_size, \n",
    "    decoder_vocab_size, \n",
    "    pe_input=encoder_vocab_size, \n",
    "    pe_target=decoder_vocab_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "JzpgMP4yOgRE"
   },
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "W7BPPE7QOipH"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"checkpoints\"\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "YZitSb2COlBR"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(\n",
    "            inp, tar_inp, \n",
    "            True, \n",
    "            enc_padding_mask, \n",
    "            combined_mask, \n",
    "            dec_padding_mask\n",
    "        )\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y3Hz5FbaOnhr",
    "outputId": "2dea4e2c-9daf-4312-9f61-09c3bbba4b39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 7.9371\n",
      "Epoch 1 Batch 8 Loss 7.9925\n",
      "Epoch 1 Batch 16 Loss 7.9876\n",
      "Epoch 1 Batch 24 Loss 7.9689\n",
      "Epoch 1 Batch 32 Loss 7.9597\n",
      "Epoch 1 Batch 40 Loss 7.9465\n",
      "Epoch 1 Batch 48 Loss 7.9284\n",
      "Epoch 1 Batch 56 Loss 7.9034\n",
      "Epoch 1 Batch 64 Loss 7.8829\n",
      "Epoch 1 Batch 72 Loss 7.8670\n",
      "Epoch 1 Batch 80 Loss 7.8499\n",
      "Epoch 1 Batch 88 Loss 7.8336\n",
      "Epoch 1 Batch 96 Loss 7.8189\n",
      "Epoch 1 Batch 104 Loss 7.8023\n",
      "Epoch 1 Batch 112 Loss 7.7869\n",
      "Epoch 1 Batch 120 Loss 7.7742\n",
      "Epoch 1 Loss 7.7644\n",
      "Time taken for 1 epoch: 20.492637872695923 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 7.4269\n",
      "Epoch 2 Batch 8 Loss 7.5135\n",
      "Epoch 2 Batch 16 Loss 7.5157\n",
      "Epoch 2 Batch 24 Loss 7.4874\n",
      "Epoch 2 Batch 32 Loss 7.4696\n",
      "Epoch 2 Batch 40 Loss 7.4675\n",
      "Epoch 2 Batch 48 Loss 7.4608\n",
      "Epoch 2 Batch 56 Loss 7.4518\n",
      "Epoch 2 Batch 64 Loss 7.4450\n",
      "Epoch 2 Batch 72 Loss 7.4362\n",
      "Epoch 2 Batch 80 Loss 7.4257\n",
      "Epoch 2 Batch 88 Loss 7.4172\n",
      "Epoch 2 Batch 96 Loss 7.4091\n",
      "Epoch 2 Batch 104 Loss 7.4032\n",
      "Epoch 2 Batch 112 Loss 7.3905\n",
      "Epoch 2 Batch 120 Loss 7.3844\n",
      "Epoch 2 Loss 7.3774\n",
      "Time taken for 1 epoch: 3.532819986343384 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 6.9566\n",
      "Epoch 3 Batch 8 Loss 7.1432\n",
      "Epoch 3 Batch 16 Loss 7.1096\n",
      "Epoch 3 Batch 24 Loss 7.0863\n",
      "Epoch 3 Batch 32 Loss 7.0872\n",
      "Epoch 3 Batch 40 Loss 7.0784\n",
      "Epoch 3 Batch 48 Loss 7.0763\n",
      "Epoch 3 Batch 56 Loss 7.0700\n",
      "Epoch 3 Batch 64 Loss 7.0592\n",
      "Epoch 3 Batch 72 Loss 7.0609\n",
      "Epoch 3 Batch 80 Loss 7.0487\n",
      "Epoch 3 Batch 88 Loss 7.0384\n",
      "Epoch 3 Batch 96 Loss 7.0274\n",
      "Epoch 3 Batch 104 Loss 7.0219\n",
      "Epoch 3 Batch 112 Loss 7.0196\n",
      "Epoch 3 Batch 120 Loss 7.0174\n",
      "Epoch 3 Loss 7.0101\n",
      "Time taken for 1 epoch: 3.4599215984344482 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 6.6403\n",
      "Epoch 4 Batch 8 Loss 6.8055\n",
      "Epoch 4 Batch 16 Loss 6.7914\n",
      "Epoch 4 Batch 24 Loss 6.8222\n",
      "Epoch 4 Batch 32 Loss 6.7887\n",
      "Epoch 4 Batch 40 Loss 6.7783\n",
      "Epoch 4 Batch 48 Loss 6.7624\n",
      "Epoch 4 Batch 56 Loss 6.7454\n",
      "Epoch 4 Batch 64 Loss 6.7441\n",
      "Epoch 4 Batch 72 Loss 6.7529\n",
      "Epoch 4 Batch 80 Loss 6.7557\n",
      "Epoch 4 Batch 88 Loss 6.7537\n",
      "Epoch 4 Batch 96 Loss 6.7528\n",
      "Epoch 4 Batch 104 Loss 6.7422\n",
      "Epoch 4 Batch 112 Loss 6.7411\n",
      "Epoch 4 Batch 120 Loss 6.7380\n",
      "Epoch 4 Loss 6.7328\n",
      "Time taken for 1 epoch: 3.4648375511169434 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 6.7891\n",
      "Epoch 5 Batch 8 Loss 6.7019\n",
      "Epoch 5 Batch 16 Loss 6.6455\n",
      "Epoch 5 Batch 24 Loss 6.6448\n",
      "Epoch 5 Batch 32 Loss 6.6402\n",
      "Epoch 5 Batch 40 Loss 6.6462\n",
      "Epoch 5 Batch 48 Loss 6.6141\n",
      "Epoch 5 Batch 56 Loss 6.5999\n",
      "Epoch 5 Batch 64 Loss 6.6101\n",
      "Epoch 5 Batch 72 Loss 6.6191\n",
      "Epoch 5 Batch 80 Loss 6.6141\n",
      "Epoch 5 Batch 88 Loss 6.6169\n",
      "Epoch 5 Batch 96 Loss 6.6072\n",
      "Epoch 5 Batch 104 Loss 6.6197\n",
      "Epoch 5 Batch 112 Loss 6.6041\n",
      "Epoch 5 Batch 120 Loss 6.6044\n",
      "Saving checkpoint for epoch 5 at checkpoints/ckpt-1\n",
      "Epoch 5 Loss 6.6024\n",
      "Time taken for 1 epoch: 3.827904462814331 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 6.8871\n",
      "Epoch 6 Batch 8 Loss 6.5487\n",
      "Epoch 6 Batch 16 Loss 6.5351\n",
      "Epoch 6 Batch 24 Loss 6.5080\n",
      "Epoch 6 Batch 32 Loss 6.5139\n",
      "Epoch 6 Batch 40 Loss 6.5141\n",
      "Epoch 6 Batch 48 Loss 6.5002\n",
      "Epoch 6 Batch 56 Loss 6.5067\n",
      "Epoch 6 Batch 64 Loss 6.5216\n",
      "Epoch 6 Batch 72 Loss 6.5077\n",
      "Epoch 6 Batch 80 Loss 6.5073\n",
      "Epoch 6 Batch 88 Loss 6.5071\n",
      "Epoch 6 Batch 96 Loss 6.5075\n",
      "Epoch 6 Batch 104 Loss 6.5070\n",
      "Epoch 6 Batch 112 Loss 6.5054\n",
      "Epoch 6 Batch 120 Loss 6.5060\n",
      "Epoch 6 Loss 6.5033\n",
      "Time taken for 1 epoch: 3.4913976192474365 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 6.8170\n",
      "Epoch 7 Batch 8 Loss 6.2998\n",
      "Epoch 7 Batch 16 Loss 6.3452\n",
      "Epoch 7 Batch 24 Loss 6.3353\n",
      "Epoch 7 Batch 32 Loss 6.3394\n",
      "Epoch 7 Batch 40 Loss 6.3372\n",
      "Epoch 7 Batch 48 Loss 6.3449\n",
      "Epoch 7 Batch 56 Loss 6.3423\n",
      "Epoch 7 Batch 64 Loss 6.3460\n",
      "Epoch 7 Batch 72 Loss 6.3221\n",
      "Epoch 7 Batch 80 Loss 6.3365\n",
      "Epoch 7 Batch 88 Loss 6.3347\n",
      "Epoch 7 Batch 96 Loss 6.3318\n",
      "Epoch 7 Batch 104 Loss 6.3332\n",
      "Epoch 7 Batch 112 Loss 6.3393\n",
      "Epoch 7 Batch 120 Loss 6.3482\n",
      "Epoch 7 Loss 6.3418\n",
      "Time taken for 1 epoch: 3.477543354034424 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 6.2256\n",
      "Epoch 8 Batch 8 Loss 6.1174\n",
      "Epoch 8 Batch 16 Loss 6.1045\n",
      "Epoch 8 Batch 24 Loss 6.0989\n",
      "Epoch 8 Batch 32 Loss 6.1431\n",
      "Epoch 8 Batch 40 Loss 6.1175\n",
      "Epoch 8 Batch 48 Loss 6.1192\n",
      "Epoch 8 Batch 56 Loss 6.1304\n",
      "Epoch 8 Batch 64 Loss 6.1417\n",
      "Epoch 8 Batch 72 Loss 6.1391\n",
      "Epoch 8 Batch 80 Loss 6.1348\n",
      "Epoch 8 Batch 88 Loss 6.1542\n",
      "Epoch 8 Batch 96 Loss 6.1627\n",
      "Epoch 8 Batch 104 Loss 6.1551\n",
      "Epoch 8 Batch 112 Loss 6.1414\n",
      "Epoch 8 Batch 120 Loss 6.1631\n",
      "Epoch 8 Loss 6.1652\n",
      "Time taken for 1 epoch: 3.5240674018859863 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 6.3423\n",
      "Epoch 9 Batch 8 Loss 6.0395\n",
      "Epoch 9 Batch 16 Loss 5.9661\n",
      "Epoch 9 Batch 24 Loss 5.9464\n",
      "Epoch 9 Batch 32 Loss 5.9414\n",
      "Epoch 9 Batch 40 Loss 5.9063\n",
      "Epoch 9 Batch 48 Loss 5.9006\n",
      "Epoch 9 Batch 56 Loss 5.8869\n",
      "Epoch 9 Batch 64 Loss 5.9055\n",
      "Epoch 9 Batch 72 Loss 5.9045\n",
      "Epoch 9 Batch 80 Loss 5.9302\n",
      "Epoch 9 Batch 88 Loss 5.9329\n",
      "Epoch 9 Batch 96 Loss 5.9386\n",
      "Epoch 9 Batch 104 Loss 5.9462\n",
      "Epoch 9 Batch 112 Loss 5.9528\n",
      "Epoch 9 Batch 120 Loss 5.9549\n",
      "Epoch 9 Loss 5.9632\n",
      "Time taken for 1 epoch: 3.4948015213012695 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 5.7458\n",
      "Epoch 10 Batch 8 Loss 5.8545\n",
      "Epoch 10 Batch 16 Loss 5.8369\n",
      "Epoch 10 Batch 24 Loss 5.8203\n",
      "Epoch 10 Batch 32 Loss 5.8196\n",
      "Epoch 10 Batch 40 Loss 5.8260\n",
      "Epoch 10 Batch 48 Loss 5.8092\n",
      "Epoch 10 Batch 56 Loss 5.8024\n",
      "Epoch 10 Batch 64 Loss 5.7888\n",
      "Epoch 10 Batch 72 Loss 5.7778\n",
      "Epoch 10 Batch 80 Loss 5.7760\n",
      "Epoch 10 Batch 88 Loss 5.8045\n",
      "Epoch 10 Batch 96 Loss 5.7996\n",
      "Epoch 10 Batch 104 Loss 5.7927\n",
      "Epoch 10 Batch 112 Loss 5.7912\n",
      "Epoch 10 Batch 120 Loss 5.7848\n",
      "Saving checkpoint for epoch 10 at checkpoints/ckpt-2\n",
      "Epoch 10 Loss 5.7869\n",
      "Time taken for 1 epoch: 4.018029451370239 secs\n",
      "\n",
      "Epoch 11 Batch 0 Loss 5.4262\n",
      "Epoch 11 Batch 8 Loss 5.6253\n",
      "Epoch 11 Batch 16 Loss 5.6227\n",
      "Epoch 11 Batch 24 Loss 5.5771\n",
      "Epoch 11 Batch 32 Loss 5.5506\n",
      "Epoch 11 Batch 40 Loss 5.5896\n",
      "Epoch 11 Batch 48 Loss 5.5801\n",
      "Epoch 11 Batch 56 Loss 5.5999\n",
      "Epoch 11 Batch 64 Loss 5.6062\n",
      "Epoch 11 Batch 72 Loss 5.6033\n",
      "Epoch 11 Batch 80 Loss 5.5929\n",
      "Epoch 11 Batch 88 Loss 5.5993\n",
      "Epoch 11 Batch 96 Loss 5.5987\n",
      "Epoch 11 Batch 104 Loss 5.5971\n",
      "Epoch 11 Batch 112 Loss 5.5967\n",
      "Epoch 11 Batch 120 Loss 5.6027\n",
      "Epoch 11 Loss 5.6145\n",
      "Time taken for 1 epoch: 3.525161027908325 secs\n",
      "\n",
      "Epoch 12 Batch 0 Loss 5.5457\n",
      "Epoch 12 Batch 8 Loss 5.3558\n",
      "Epoch 12 Batch 16 Loss 5.3981\n",
      "Epoch 12 Batch 24 Loss 5.3333\n",
      "Epoch 12 Batch 32 Loss 5.3271\n",
      "Epoch 12 Batch 40 Loss 5.2961\n",
      "Epoch 12 Batch 48 Loss 5.2941\n",
      "Epoch 12 Batch 56 Loss 5.3246\n",
      "Epoch 12 Batch 64 Loss 5.3442\n",
      "Epoch 12 Batch 72 Loss 5.3597\n",
      "Epoch 12 Batch 80 Loss 5.3628\n",
      "Epoch 12 Batch 88 Loss 5.3745\n",
      "Epoch 12 Batch 96 Loss 5.3908\n",
      "Epoch 12 Batch 104 Loss 5.4100\n",
      "Epoch 12 Batch 112 Loss 5.4076\n",
      "Epoch 12 Batch 120 Loss 5.4177\n",
      "Epoch 12 Loss 5.4171\n",
      "Time taken for 1 epoch: 3.5511696338653564 secs\n",
      "\n",
      "Epoch 13 Batch 0 Loss 4.6611\n",
      "Epoch 13 Batch 8 Loss 5.1207\n",
      "Epoch 13 Batch 16 Loss 5.1400\n",
      "Epoch 13 Batch 24 Loss 5.0907\n",
      "Epoch 13 Batch 32 Loss 5.0897\n",
      "Epoch 13 Batch 40 Loss 5.0870\n",
      "Epoch 13 Batch 48 Loss 5.0979\n",
      "Epoch 13 Batch 56 Loss 5.1161\n",
      "Epoch 13 Batch 64 Loss 5.1278\n",
      "Epoch 13 Batch 72 Loss 5.1135\n",
      "Epoch 13 Batch 80 Loss 5.1180\n",
      "Epoch 13 Batch 88 Loss 5.1327\n",
      "Epoch 13 Batch 96 Loss 5.1246\n",
      "Epoch 13 Batch 104 Loss 5.1193\n",
      "Epoch 13 Batch 112 Loss 5.1412\n",
      "Epoch 13 Batch 120 Loss 5.1606\n",
      "Epoch 13 Loss 5.1803\n",
      "Time taken for 1 epoch: 3.532616376876831 secs\n",
      "\n",
      "Epoch 14 Batch 0 Loss 4.8180\n",
      "Epoch 14 Batch 8 Loss 4.9765\n",
      "Epoch 14 Batch 16 Loss 4.9280\n",
      "Epoch 14 Batch 24 Loss 4.9593\n",
      "Epoch 14 Batch 32 Loss 4.9497\n",
      "Epoch 14 Batch 40 Loss 4.9277\n",
      "Epoch 14 Batch 48 Loss 4.8987\n",
      "Epoch 14 Batch 56 Loss 4.8919\n",
      "Epoch 14 Batch 64 Loss 4.8922\n",
      "Epoch 14 Batch 72 Loss 4.8986\n",
      "Epoch 14 Batch 80 Loss 4.8954\n",
      "Epoch 14 Batch 88 Loss 4.9057\n",
      "Epoch 14 Batch 96 Loss 4.9207\n",
      "Epoch 14 Batch 104 Loss 4.9248\n",
      "Epoch 14 Batch 112 Loss 4.9297\n",
      "Epoch 14 Batch 120 Loss 4.9429\n",
      "Epoch 14 Loss 4.9473\n",
      "Time taken for 1 epoch: 3.5483014583587646 secs\n",
      "\n",
      "Epoch 15 Batch 0 Loss 4.6639\n",
      "Epoch 15 Batch 8 Loss 4.8577\n",
      "Epoch 15 Batch 16 Loss 4.8644\n",
      "Epoch 15 Batch 24 Loss 4.7713\n",
      "Epoch 15 Batch 32 Loss 4.7644\n",
      "Epoch 15 Batch 40 Loss 4.7632\n",
      "Epoch 15 Batch 48 Loss 4.7490\n",
      "Epoch 15 Batch 56 Loss 4.7238\n",
      "Epoch 15 Batch 64 Loss 4.7385\n",
      "Epoch 15 Batch 72 Loss 4.7279\n",
      "Epoch 15 Batch 80 Loss 4.7194\n",
      "Epoch 15 Batch 88 Loss 4.7207\n",
      "Epoch 15 Batch 96 Loss 4.6971\n",
      "Epoch 15 Batch 104 Loss 4.7120\n",
      "Epoch 15 Batch 112 Loss 4.7168\n",
      "Epoch 15 Batch 120 Loss 4.7054\n",
      "Saving checkpoint for epoch 15 at checkpoints/ckpt-3\n",
      "Epoch 15 Loss 4.7102\n",
      "Time taken for 1 epoch: 3.8678176403045654 secs\n",
      "\n",
      "Epoch 16 Batch 0 Loss 3.8960\n",
      "Epoch 16 Batch 8 Loss 4.4855\n",
      "Epoch 16 Batch 16 Loss 4.4891\n",
      "Epoch 16 Batch 24 Loss 4.4006\n",
      "Epoch 16 Batch 32 Loss 4.3739\n",
      "Epoch 16 Batch 40 Loss 4.4165\n",
      "Epoch 16 Batch 48 Loss 4.4232\n",
      "Epoch 16 Batch 56 Loss 4.4174\n",
      "Epoch 16 Batch 64 Loss 4.4015\n",
      "Epoch 16 Batch 72 Loss 4.4151\n",
      "Epoch 16 Batch 80 Loss 4.4332\n",
      "Epoch 16 Batch 88 Loss 4.4479\n",
      "Epoch 16 Batch 96 Loss 4.4567\n",
      "Epoch 16 Batch 104 Loss 4.4675\n",
      "Epoch 16 Batch 112 Loss 4.4681\n",
      "Epoch 16 Batch 120 Loss 4.4719\n",
      "Epoch 16 Loss 4.4816\n",
      "Time taken for 1 epoch: 3.5833539962768555 secs\n",
      "\n",
      "Epoch 17 Batch 0 Loss 3.1458\n",
      "Epoch 17 Batch 8 Loss 3.8451\n",
      "Epoch 17 Batch 16 Loss 3.9834\n",
      "Epoch 17 Batch 24 Loss 3.9716\n",
      "Epoch 17 Batch 32 Loss 4.0650\n",
      "Epoch 17 Batch 40 Loss 4.0952\n",
      "Epoch 17 Batch 48 Loss 4.0829\n",
      "Epoch 17 Batch 56 Loss 4.1079\n",
      "Epoch 17 Batch 64 Loss 4.1236\n",
      "Epoch 17 Batch 72 Loss 4.1408\n",
      "Epoch 17 Batch 80 Loss 4.1386\n",
      "Epoch 17 Batch 88 Loss 4.1425\n",
      "Epoch 17 Batch 96 Loss 4.1462\n",
      "Epoch 17 Batch 104 Loss 4.1433\n",
      "Epoch 17 Batch 112 Loss 4.1477\n",
      "Epoch 17 Batch 120 Loss 4.1654\n",
      "Epoch 17 Loss 4.1738\n",
      "Time taken for 1 epoch: 5.796185731887817 secs\n",
      "\n",
      "Epoch 18 Batch 0 Loss 4.1902\n",
      "Epoch 18 Batch 8 Loss 3.9200\n",
      "Epoch 18 Batch 16 Loss 3.9355\n",
      "Epoch 18 Batch 24 Loss 3.9488\n",
      "Epoch 18 Batch 32 Loss 3.8992\n",
      "Epoch 18 Batch 40 Loss 3.9013\n",
      "Epoch 18 Batch 48 Loss 3.8683\n",
      "Epoch 18 Batch 56 Loss 3.8400\n",
      "Epoch 18 Batch 64 Loss 3.8519\n",
      "Epoch 18 Batch 72 Loss 3.8765\n",
      "Epoch 18 Batch 80 Loss 3.8579\n",
      "Epoch 18 Batch 88 Loss 3.8780\n",
      "Epoch 18 Batch 96 Loss 3.8906\n",
      "Epoch 18 Batch 104 Loss 3.9014\n",
      "Epoch 18 Batch 112 Loss 3.9079\n",
      "Epoch 18 Batch 120 Loss 3.9196\n",
      "Epoch 18 Loss 3.9140\n",
      "Time taken for 1 epoch: 3.6979029178619385 secs\n",
      "\n",
      "Epoch 19 Batch 0 Loss 3.9030\n",
      "Epoch 19 Batch 8 Loss 3.7065\n",
      "Epoch 19 Batch 16 Loss 3.6239\n",
      "Epoch 19 Batch 24 Loss 3.6078\n",
      "Epoch 19 Batch 32 Loss 3.5996\n",
      "Epoch 19 Batch 40 Loss 3.5992\n",
      "Epoch 19 Batch 48 Loss 3.5390\n",
      "Epoch 19 Batch 56 Loss 3.5487\n",
      "Epoch 19 Batch 64 Loss 3.5471\n",
      "Epoch 19 Batch 72 Loss 3.5353\n",
      "Epoch 19 Batch 80 Loss 3.5649\n",
      "Epoch 19 Batch 88 Loss 3.5739\n",
      "Epoch 19 Batch 96 Loss 3.5737\n",
      "Epoch 19 Batch 104 Loss 3.5852\n",
      "Epoch 19 Batch 112 Loss 3.5861\n",
      "Epoch 19 Batch 120 Loss 3.5863\n",
      "Epoch 19 Loss 3.5889\n",
      "Time taken for 1 epoch: 3.582531690597534 secs\n",
      "\n",
      "Epoch 20 Batch 0 Loss 3.2330\n",
      "Epoch 20 Batch 8 Loss 3.1417\n",
      "Epoch 20 Batch 16 Loss 3.0427\n",
      "Epoch 20 Batch 24 Loss 3.0712\n",
      "Epoch 20 Batch 32 Loss 3.1112\n",
      "Epoch 20 Batch 40 Loss 3.1496\n",
      "Epoch 20 Batch 48 Loss 3.1638\n",
      "Epoch 20 Batch 56 Loss 3.2109\n",
      "Epoch 20 Batch 64 Loss 3.2265\n",
      "Epoch 20 Batch 72 Loss 3.2155\n",
      "Epoch 20 Batch 80 Loss 3.2411\n",
      "Epoch 20 Batch 88 Loss 3.2517\n",
      "Epoch 20 Batch 96 Loss 3.2526\n",
      "Epoch 20 Batch 104 Loss 3.2735\n",
      "Epoch 20 Batch 112 Loss 3.2855\n",
      "Epoch 20 Batch 120 Loss 3.2931\n",
      "Saving checkpoint for epoch 20 at checkpoints/ckpt-4\n",
      "Epoch 20 Loss 3.2952\n",
      "Time taken for 1 epoch: 3.9239025115966797 secs\n",
      "\n",
      "Epoch 21 Batch 0 Loss 3.1709\n",
      "Epoch 21 Batch 8 Loss 3.0097\n",
      "Epoch 21 Batch 16 Loss 2.9549\n",
      "Epoch 21 Batch 24 Loss 2.9339\n",
      "Epoch 21 Batch 32 Loss 2.9129\n",
      "Epoch 21 Batch 40 Loss 2.9143\n",
      "Epoch 21 Batch 48 Loss 2.9184\n",
      "Epoch 21 Batch 56 Loss 2.9169\n",
      "Epoch 21 Batch 64 Loss 2.9302\n",
      "Epoch 21 Batch 72 Loss 2.9313\n",
      "Epoch 21 Batch 80 Loss 2.9209\n",
      "Epoch 21 Batch 88 Loss 2.9537\n",
      "Epoch 21 Batch 96 Loss 2.9554\n",
      "Epoch 21 Batch 104 Loss 2.9653\n",
      "Epoch 21 Batch 112 Loss 2.9632\n",
      "Epoch 21 Batch 120 Loss 2.9708\n",
      "Epoch 21 Loss 2.9844\n",
      "Time taken for 1 epoch: 3.6112263202667236 secs\n",
      "\n",
      "Epoch 22 Batch 0 Loss 2.6638\n",
      "Epoch 22 Batch 8 Loss 2.4791\n",
      "Epoch 22 Batch 16 Loss 2.5612\n",
      "Epoch 22 Batch 24 Loss 2.5701\n",
      "Epoch 22 Batch 32 Loss 2.6027\n",
      "Epoch 22 Batch 40 Loss 2.5962\n",
      "Epoch 22 Batch 48 Loss 2.6243\n",
      "Epoch 22 Batch 56 Loss 2.6228\n",
      "Epoch 22 Batch 64 Loss 2.6398\n",
      "Epoch 22 Batch 72 Loss 2.6309\n",
      "Epoch 22 Batch 80 Loss 2.6458\n",
      "Epoch 22 Batch 88 Loss 2.6389\n",
      "Epoch 22 Batch 96 Loss 2.6290\n",
      "Epoch 22 Batch 104 Loss 2.6347\n",
      "Epoch 22 Batch 112 Loss 2.6411\n",
      "Epoch 22 Batch 120 Loss 2.6516\n",
      "Epoch 22 Loss 2.6450\n",
      "Time taken for 1 epoch: 3.6319680213928223 secs\n",
      "\n",
      "Epoch 23 Batch 0 Loss 2.1950\n",
      "Epoch 23 Batch 8 Loss 2.1464\n",
      "Epoch 23 Batch 16 Loss 2.1729\n",
      "Epoch 23 Batch 24 Loss 2.2418\n",
      "Epoch 23 Batch 32 Loss 2.2265\n",
      "Epoch 23 Batch 40 Loss 2.2706\n",
      "Epoch 23 Batch 48 Loss 2.2920\n",
      "Epoch 23 Batch 56 Loss 2.2817\n",
      "Epoch 23 Batch 64 Loss 2.3013\n",
      "Epoch 23 Batch 72 Loss 2.3204\n",
      "Epoch 23 Batch 80 Loss 2.3574\n",
      "Epoch 23 Batch 88 Loss 2.3495\n",
      "Epoch 23 Batch 96 Loss 2.3444\n",
      "Epoch 23 Batch 104 Loss 2.3566\n",
      "Epoch 23 Batch 112 Loss 2.3684\n",
      "Epoch 23 Batch 120 Loss 2.3811\n",
      "Epoch 23 Loss 2.3899\n",
      "Time taken for 1 epoch: 3.6240673065185547 secs\n",
      "\n",
      "Epoch 24 Batch 0 Loss 1.8502\n",
      "Epoch 24 Batch 8 Loss 1.8167\n",
      "Epoch 24 Batch 16 Loss 1.8360\n",
      "Epoch 24 Batch 24 Loss 1.8671\n",
      "Epoch 24 Batch 32 Loss 1.9290\n",
      "Epoch 24 Batch 40 Loss 1.9221\n",
      "Epoch 24 Batch 48 Loss 1.9347\n",
      "Epoch 24 Batch 56 Loss 1.9578\n",
      "Epoch 24 Batch 64 Loss 1.9670\n",
      "Epoch 24 Batch 72 Loss 1.9718\n",
      "Epoch 24 Batch 80 Loss 1.9822\n",
      "Epoch 24 Batch 88 Loss 1.9880\n",
      "Epoch 24 Batch 96 Loss 1.9931\n",
      "Epoch 24 Batch 104 Loss 2.0079\n",
      "Epoch 24 Batch 112 Loss 2.0109\n",
      "Epoch 24 Batch 120 Loss 2.0284\n",
      "Epoch 24 Loss 2.0481\n",
      "Time taken for 1 epoch: 4.7140138149261475 secs\n",
      "\n",
      "Epoch 25 Batch 0 Loss 1.2922\n",
      "Epoch 25 Batch 8 Loss 1.7604\n",
      "Epoch 25 Batch 16 Loss 1.6972\n",
      "Epoch 25 Batch 24 Loss 1.6594\n",
      "Epoch 25 Batch 32 Loss 1.6400\n",
      "Epoch 25 Batch 40 Loss 1.6485\n",
      "Epoch 25 Batch 48 Loss 1.6501\n",
      "Epoch 25 Batch 56 Loss 1.6562\n",
      "Epoch 25 Batch 64 Loss 1.6774\n",
      "Epoch 25 Batch 72 Loss 1.6790\n",
      "Epoch 25 Batch 80 Loss 1.6899\n",
      "Epoch 25 Batch 88 Loss 1.6927\n",
      "Epoch 25 Batch 96 Loss 1.6988\n",
      "Epoch 25 Batch 104 Loss 1.7043\n",
      "Epoch 25 Batch 112 Loss 1.7230\n",
      "Epoch 25 Batch 120 Loss 1.7441\n",
      "Saving checkpoint for epoch 25 at checkpoints/ckpt-5\n",
      "Epoch 25 Loss 1.7590\n",
      "Time taken for 1 epoch: 4.398296594619751 secs\n",
      "\n",
      "Epoch 26 Batch 0 Loss 1.3224\n",
      "Epoch 26 Batch 8 Loss 1.3744\n",
      "Epoch 26 Batch 16 Loss 1.3944\n",
      "Epoch 26 Batch 24 Loss 1.4045\n",
      "Epoch 26 Batch 32 Loss 1.3997\n",
      "Epoch 26 Batch 40 Loss 1.4063\n",
      "Epoch 26 Batch 48 Loss 1.4041\n",
      "Epoch 26 Batch 56 Loss 1.4177\n",
      "Epoch 26 Batch 64 Loss 1.4082\n",
      "Epoch 26 Batch 72 Loss 1.4471\n",
      "Epoch 26 Batch 80 Loss 1.4529\n",
      "Epoch 26 Batch 88 Loss 1.4674\n",
      "Epoch 26 Batch 96 Loss 1.4857\n",
      "Epoch 26 Batch 104 Loss 1.4919\n",
      "Epoch 26 Batch 112 Loss 1.5021\n",
      "Epoch 26 Batch 120 Loss 1.5064\n",
      "Epoch 26 Loss 1.5250\n",
      "Time taken for 1 epoch: 3.6235923767089844 secs\n",
      "\n",
      "Epoch 27 Batch 0 Loss 1.0380\n",
      "Epoch 27 Batch 8 Loss 1.1199\n",
      "Epoch 27 Batch 16 Loss 1.1114\n",
      "Epoch 27 Batch 24 Loss 1.0735\n",
      "Epoch 27 Batch 32 Loss 1.0836\n",
      "Epoch 27 Batch 40 Loss 1.1255\n",
      "Epoch 27 Batch 48 Loss 1.1369\n",
      "Epoch 27 Batch 56 Loss 1.1553\n",
      "Epoch 27 Batch 64 Loss 1.1545\n",
      "Epoch 27 Batch 72 Loss 1.1534\n",
      "Epoch 27 Batch 80 Loss 1.1692\n",
      "Epoch 27 Batch 88 Loss 1.1947\n",
      "Epoch 27 Batch 96 Loss 1.2051\n",
      "Epoch 27 Batch 104 Loss 1.2236\n",
      "Epoch 27 Batch 112 Loss 1.2380\n",
      "Epoch 27 Batch 120 Loss 1.2537\n",
      "Epoch 27 Loss 1.2677\n",
      "Time taken for 1 epoch: 3.6284289360046387 secs\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.8964\n",
      "Epoch 28 Batch 8 Loss 0.9765\n",
      "Epoch 28 Batch 16 Loss 0.9610\n",
      "Epoch 28 Batch 24 Loss 0.9436\n",
      "Epoch 28 Batch 32 Loss 0.9238\n",
      "Epoch 28 Batch 40 Loss 0.9387\n",
      "Epoch 28 Batch 48 Loss 0.9405\n",
      "Epoch 28 Batch 56 Loss 0.9487\n",
      "Epoch 28 Batch 64 Loss 0.9712\n",
      "Epoch 28 Batch 72 Loss 0.9746\n",
      "Epoch 28 Batch 80 Loss 0.9830\n",
      "Epoch 28 Batch 88 Loss 1.0035\n",
      "Epoch 28 Batch 96 Loss 1.0264\n",
      "Epoch 28 Batch 104 Loss 1.0450\n",
      "Epoch 28 Batch 112 Loss 1.0652\n",
      "Epoch 28 Batch 120 Loss 1.0827\n",
      "Epoch 28 Loss 1.0901\n",
      "Time taken for 1 epoch: 3.660093069076538 secs\n",
      "\n",
      "Epoch 29 Batch 0 Loss 1.4970\n",
      "Epoch 29 Batch 8 Loss 0.8230\n",
      "Epoch 29 Batch 16 Loss 0.8247\n",
      "Epoch 29 Batch 24 Loss 0.8318\n",
      "Epoch 29 Batch 32 Loss 0.8446\n",
      "Epoch 29 Batch 40 Loss 0.8532\n",
      "Epoch 29 Batch 48 Loss 0.8688\n",
      "Epoch 29 Batch 56 Loss 0.8664\n",
      "Epoch 29 Batch 64 Loss 0.8661\n",
      "Epoch 29 Batch 72 Loss 0.8826\n",
      "Epoch 29 Batch 80 Loss 0.8902\n",
      "Epoch 29 Batch 88 Loss 0.8937\n",
      "Epoch 29 Batch 96 Loss 0.9042\n",
      "Epoch 29 Batch 104 Loss 0.9217\n",
      "Epoch 29 Batch 112 Loss 0.9352\n",
      "Epoch 29 Batch 120 Loss 0.9413\n",
      "Epoch 29 Loss 0.9507\n",
      "Time taken for 1 epoch: 3.624323844909668 secs\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.6356\n",
      "Epoch 30 Batch 8 Loss 0.7283\n",
      "Epoch 30 Batch 16 Loss 0.7055\n",
      "Epoch 30 Batch 24 Loss 0.7278\n",
      "Epoch 30 Batch 32 Loss 0.6929\n",
      "Epoch 30 Batch 40 Loss 0.6748\n",
      "Epoch 30 Batch 48 Loss 0.6734\n",
      "Epoch 30 Batch 56 Loss 0.6789\n",
      "Epoch 30 Batch 64 Loss 0.6830\n",
      "Epoch 30 Batch 72 Loss 0.6979\n",
      "Epoch 30 Batch 80 Loss 0.7161\n",
      "Epoch 30 Batch 88 Loss 0.7401\n",
      "Epoch 30 Batch 96 Loss 0.7599\n",
      "Epoch 30 Batch 104 Loss 0.7798\n",
      "Epoch 30 Batch 112 Loss 0.7949\n",
      "Epoch 30 Batch 120 Loss 0.7988\n",
      "Saving checkpoint for epoch 30 at checkpoints/ckpt-6\n",
      "Epoch 30 Loss 0.8098\n",
      "Time taken for 1 epoch: 3.955699920654297 secs\n",
      "\n",
      "Epoch 31 Batch 0 Loss 0.5682\n",
      "Epoch 31 Batch 8 Loss 0.5834\n",
      "Epoch 31 Batch 16 Loss 0.6105\n",
      "Epoch 31 Batch 24 Loss 0.6418\n",
      "Epoch 31 Batch 32 Loss 0.6664\n",
      "Epoch 31 Batch 40 Loss 0.6767\n",
      "Epoch 31 Batch 48 Loss 0.6854\n",
      "Epoch 31 Batch 56 Loss 0.6929\n",
      "Epoch 31 Batch 64 Loss 0.7041\n",
      "Epoch 31 Batch 72 Loss 0.7021\n",
      "Epoch 31 Batch 80 Loss 0.7070\n",
      "Epoch 31 Batch 88 Loss 0.7115\n",
      "Epoch 31 Batch 96 Loss 0.7186\n",
      "Epoch 31 Batch 104 Loss 0.7329\n",
      "Epoch 31 Batch 112 Loss 0.7434\n",
      "Epoch 31 Batch 120 Loss 0.7478\n",
      "Epoch 31 Loss 0.7594\n",
      "Time taken for 1 epoch: 3.619675874710083 secs\n",
      "\n",
      "Epoch 32 Batch 0 Loss 0.4730\n",
      "Epoch 32 Batch 8 Loss 0.5066\n",
      "Epoch 32 Batch 16 Loss 0.5328\n",
      "Epoch 32 Batch 24 Loss 0.5029\n",
      "Epoch 32 Batch 32 Loss 0.5005\n",
      "Epoch 32 Batch 40 Loss 0.5183\n",
      "Epoch 32 Batch 48 Loss 0.5381\n",
      "Epoch 32 Batch 56 Loss 0.5542\n",
      "Epoch 32 Batch 64 Loss 0.5620\n",
      "Epoch 32 Batch 72 Loss 0.5731\n",
      "Epoch 32 Batch 80 Loss 0.5910\n",
      "Epoch 32 Batch 88 Loss 0.6080\n",
      "Epoch 32 Batch 96 Loss 0.6223\n",
      "Epoch 32 Batch 104 Loss 0.6470\n",
      "Epoch 32 Batch 112 Loss 0.6544\n",
      "Epoch 32 Batch 120 Loss 0.6616\n",
      "Epoch 32 Loss 0.6710\n",
      "Time taken for 1 epoch: 3.628755807876587 secs\n",
      "\n",
      "Epoch 33 Batch 0 Loss 0.4029\n",
      "Epoch 33 Batch 8 Loss 0.5094\n",
      "Epoch 33 Batch 16 Loss 0.5035\n",
      "Epoch 33 Batch 24 Loss 0.5057\n",
      "Epoch 33 Batch 32 Loss 0.5176\n",
      "Epoch 33 Batch 40 Loss 0.5356\n",
      "Epoch 33 Batch 48 Loss 0.5583\n",
      "Epoch 33 Batch 56 Loss 0.5619\n",
      "Epoch 33 Batch 64 Loss 0.5610\n",
      "Epoch 33 Batch 72 Loss 0.5639\n",
      "Epoch 33 Batch 80 Loss 0.5657\n",
      "Epoch 33 Batch 88 Loss 0.5715\n",
      "Epoch 33 Batch 96 Loss 0.5784\n",
      "Epoch 33 Batch 104 Loss 0.5806\n",
      "Epoch 33 Batch 112 Loss 0.5912\n",
      "Epoch 33 Batch 120 Loss 0.6068\n",
      "Epoch 33 Loss 0.6162\n",
      "Time taken for 1 epoch: 3.599411725997925 secs\n",
      "\n",
      "Epoch 34 Batch 0 Loss 0.4221\n",
      "Epoch 34 Batch 8 Loss 0.4336\n",
      "Epoch 34 Batch 16 Loss 0.4383\n",
      "Epoch 34 Batch 24 Loss 0.4136\n",
      "Epoch 34 Batch 32 Loss 0.4175\n",
      "Epoch 34 Batch 40 Loss 0.4203\n",
      "Epoch 34 Batch 48 Loss 0.4323\n",
      "Epoch 34 Batch 56 Loss 0.4443\n",
      "Epoch 34 Batch 64 Loss 0.4476\n",
      "Epoch 34 Batch 72 Loss 0.4545\n",
      "Epoch 34 Batch 80 Loss 0.4620\n",
      "Epoch 34 Batch 88 Loss 0.4675\n",
      "Epoch 34 Batch 96 Loss 0.4751\n",
      "Epoch 34 Batch 104 Loss 0.4778\n",
      "Epoch 34 Batch 112 Loss 0.4851\n",
      "Epoch 34 Batch 120 Loss 0.4957\n",
      "Epoch 34 Loss 0.5011\n",
      "Time taken for 1 epoch: 3.617671251296997 secs\n",
      "\n",
      "Epoch 35 Batch 0 Loss 0.4924\n",
      "Epoch 35 Batch 8 Loss 0.3945\n",
      "Epoch 35 Batch 16 Loss 0.4244\n",
      "Epoch 35 Batch 24 Loss 0.4277\n",
      "Epoch 35 Batch 32 Loss 0.4280\n",
      "Epoch 35 Batch 40 Loss 0.4430\n",
      "Epoch 35 Batch 48 Loss 0.4527\n",
      "Epoch 35 Batch 56 Loss 0.4511\n",
      "Epoch 35 Batch 64 Loss 0.4522\n",
      "Epoch 35 Batch 72 Loss 0.4578\n",
      "Epoch 35 Batch 80 Loss 0.4682\n",
      "Epoch 35 Batch 88 Loss 0.4725\n",
      "Epoch 35 Batch 96 Loss 0.4736\n",
      "Epoch 35 Batch 104 Loss 0.4733\n",
      "Epoch 35 Batch 112 Loss 0.4760\n",
      "Epoch 35 Batch 120 Loss 0.4781\n",
      "Saving checkpoint for epoch 35 at checkpoints/ckpt-7\n",
      "Epoch 35 Loss 0.4836\n",
      "Time taken for 1 epoch: 3.9200124740600586 secs\n",
      "\n",
      "Epoch 36 Batch 0 Loss 0.4316\n",
      "Epoch 36 Batch 8 Loss 0.3157\n",
      "Epoch 36 Batch 16 Loss 0.3314\n",
      "Epoch 36 Batch 24 Loss 0.3411\n",
      "Epoch 36 Batch 32 Loss 0.3421\n",
      "Epoch 36 Batch 40 Loss 0.3326\n",
      "Epoch 36 Batch 48 Loss 0.3495\n",
      "Epoch 36 Batch 56 Loss 0.3590\n",
      "Epoch 36 Batch 64 Loss 0.3537\n",
      "Epoch 36 Batch 72 Loss 0.3495\n",
      "Epoch 36 Batch 80 Loss 0.3625\n",
      "Epoch 36 Batch 88 Loss 0.3760\n",
      "Epoch 36 Batch 96 Loss 0.3832\n",
      "Epoch 36 Batch 104 Loss 0.3931\n",
      "Epoch 36 Batch 112 Loss 0.3986\n",
      "Epoch 36 Batch 120 Loss 0.4026\n",
      "Epoch 36 Loss 0.4068\n",
      "Time taken for 1 epoch: 3.5892248153686523 secs\n",
      "\n",
      "Epoch 37 Batch 0 Loss 0.3160\n",
      "Epoch 37 Batch 8 Loss 0.2626\n",
      "Epoch 37 Batch 16 Loss 0.2666\n",
      "Epoch 37 Batch 24 Loss 0.2880\n",
      "Epoch 37 Batch 32 Loss 0.2885\n",
      "Epoch 37 Batch 40 Loss 0.2976\n",
      "Epoch 37 Batch 48 Loss 0.3047\n",
      "Epoch 37 Batch 56 Loss 0.3087\n",
      "Epoch 37 Batch 64 Loss 0.3114\n",
      "Epoch 37 Batch 72 Loss 0.3181\n",
      "Epoch 37 Batch 80 Loss 0.3258\n",
      "Epoch 37 Batch 88 Loss 0.3286\n",
      "Epoch 37 Batch 96 Loss 0.3358\n",
      "Epoch 37 Batch 104 Loss 0.3437\n",
      "Epoch 37 Batch 112 Loss 0.3513\n",
      "Epoch 37 Batch 120 Loss 0.3577\n",
      "Epoch 37 Loss 0.3684\n",
      "Time taken for 1 epoch: 3.940563201904297 secs\n",
      "\n",
      "Epoch 38 Batch 0 Loss 0.3778\n",
      "Epoch 38 Batch 8 Loss 0.3132\n",
      "Epoch 38 Batch 16 Loss 0.3083\n",
      "Epoch 38 Batch 24 Loss 0.3271\n",
      "Epoch 38 Batch 32 Loss 0.3242\n",
      "Epoch 38 Batch 40 Loss 0.3240\n",
      "Epoch 38 Batch 48 Loss 0.3288\n",
      "Epoch 38 Batch 56 Loss 0.3351\n",
      "Epoch 38 Batch 64 Loss 0.3385\n",
      "Epoch 38 Batch 72 Loss 0.3445\n",
      "Epoch 38 Batch 80 Loss 0.3462\n",
      "Epoch 38 Batch 88 Loss 0.3446\n",
      "Epoch 38 Batch 96 Loss 0.3506\n",
      "Epoch 38 Batch 104 Loss 0.3613\n",
      "Epoch 38 Batch 112 Loss 0.3630\n",
      "Epoch 38 Batch 120 Loss 0.3645\n",
      "Epoch 38 Loss 0.3721\n",
      "Time taken for 1 epoch: 5.009760618209839 secs\n",
      "\n",
      "Epoch 39 Batch 0 Loss 0.3336\n",
      "Epoch 39 Batch 8 Loss 0.2550\n",
      "Epoch 39 Batch 16 Loss 0.2690\n",
      "Epoch 39 Batch 24 Loss 0.2700\n",
      "Epoch 39 Batch 32 Loss 0.2798\n",
      "Epoch 39 Batch 40 Loss 0.2856\n",
      "Epoch 39 Batch 48 Loss 0.2869\n",
      "Epoch 39 Batch 56 Loss 0.3018\n",
      "Epoch 39 Batch 64 Loss 0.3106\n",
      "Epoch 39 Batch 72 Loss 0.3056\n",
      "Epoch 39 Batch 80 Loss 0.3090\n",
      "Epoch 39 Batch 88 Loss 0.3103\n",
      "Epoch 39 Batch 96 Loss 0.3064\n",
      "Epoch 39 Batch 104 Loss 0.3102\n",
      "Epoch 39 Batch 112 Loss 0.3141\n",
      "Epoch 39 Batch 120 Loss 0.3185\n",
      "Epoch 39 Loss 0.3236\n",
      "Time taken for 1 epoch: 4.83027720451355 secs\n",
      "\n",
      "Epoch 40 Batch 0 Loss 0.2338\n",
      "Epoch 40 Batch 8 Loss 0.2421\n",
      "Epoch 40 Batch 16 Loss 0.2522\n",
      "Epoch 40 Batch 24 Loss 0.2444\n",
      "Epoch 40 Batch 32 Loss 0.2471\n",
      "Epoch 40 Batch 40 Loss 0.2523\n",
      "Epoch 40 Batch 48 Loss 0.2506\n",
      "Epoch 40 Batch 56 Loss 0.2516\n",
      "Epoch 40 Batch 64 Loss 0.2620\n",
      "Epoch 40 Batch 72 Loss 0.2647\n",
      "Epoch 40 Batch 80 Loss 0.2643\n",
      "Epoch 40 Batch 88 Loss 0.2667\n",
      "Epoch 40 Batch 96 Loss 0.2707\n",
      "Epoch 40 Batch 104 Loss 0.2767\n",
      "Epoch 40 Batch 112 Loss 0.2791\n",
      "Epoch 40 Batch 120 Loss 0.2808\n",
      "Saving checkpoint for epoch 40 at checkpoints/ckpt-8\n",
      "Epoch 40 Loss 0.2849\n",
      "Time taken for 1 epoch: 4.65145468711853 secs\n",
      "\n",
      "Epoch 41 Batch 0 Loss 0.2975\n",
      "Epoch 41 Batch 8 Loss 0.2431\n",
      "Epoch 41 Batch 16 Loss 0.2269\n",
      "Epoch 41 Batch 24 Loss 0.2397\n",
      "Epoch 41 Batch 32 Loss 0.2397\n",
      "Epoch 41 Batch 40 Loss 0.2494\n",
      "Epoch 41 Batch 48 Loss 0.2492\n",
      "Epoch 41 Batch 56 Loss 0.2531\n",
      "Epoch 41 Batch 64 Loss 0.2561\n",
      "Epoch 41 Batch 72 Loss 0.2541\n",
      "Epoch 41 Batch 80 Loss 0.2553\n",
      "Epoch 41 Batch 88 Loss 0.2633\n",
      "Epoch 41 Batch 96 Loss 0.2700\n",
      "Epoch 41 Batch 104 Loss 0.2710\n",
      "Epoch 41 Batch 112 Loss 0.2739\n",
      "Epoch 41 Batch 120 Loss 0.2731\n",
      "Epoch 41 Loss 0.2714\n",
      "Time taken for 1 epoch: 3.592747449874878 secs\n",
      "\n",
      "Epoch 42 Batch 0 Loss 0.1436\n",
      "Epoch 42 Batch 8 Loss 0.1888\n",
      "Epoch 42 Batch 16 Loss 0.1971\n",
      "Epoch 42 Batch 24 Loss 0.1872\n",
      "Epoch 42 Batch 32 Loss 0.1865\n",
      "Epoch 42 Batch 40 Loss 0.2129\n",
      "Epoch 42 Batch 48 Loss 0.2125\n",
      "Epoch 42 Batch 56 Loss 0.2138\n",
      "Epoch 42 Batch 64 Loss 0.2198\n",
      "Epoch 42 Batch 72 Loss 0.2253\n",
      "Epoch 42 Batch 80 Loss 0.2294\n",
      "Epoch 42 Batch 88 Loss 0.2367\n",
      "Epoch 42 Batch 96 Loss 0.2443\n",
      "Epoch 42 Batch 104 Loss 0.2494\n",
      "Epoch 42 Batch 112 Loss 0.2567\n",
      "Epoch 42 Batch 120 Loss 0.2609\n",
      "Epoch 42 Loss 0.2631\n",
      "Time taken for 1 epoch: 3.6070847511291504 secs\n",
      "\n",
      "Epoch 43 Batch 0 Loss 0.2791\n",
      "Epoch 43 Batch 8 Loss 0.2336\n",
      "Epoch 43 Batch 16 Loss 0.2134\n",
      "Epoch 43 Batch 24 Loss 0.2046\n",
      "Epoch 43 Batch 32 Loss 0.2129\n",
      "Epoch 43 Batch 40 Loss 0.2151\n",
      "Epoch 43 Batch 48 Loss 0.2231\n",
      "Epoch 43 Batch 56 Loss 0.2226\n",
      "Epoch 43 Batch 64 Loss 0.2247\n",
      "Epoch 43 Batch 72 Loss 0.2262\n",
      "Epoch 43 Batch 80 Loss 0.2322\n",
      "Epoch 43 Batch 88 Loss 0.2372\n",
      "Epoch 43 Batch 96 Loss 0.2391\n",
      "Epoch 43 Batch 104 Loss 0.2425\n",
      "Epoch 43 Batch 112 Loss 0.2445\n",
      "Epoch 43 Batch 120 Loss 0.2478\n",
      "Epoch 43 Loss 0.2494\n",
      "Time taken for 1 epoch: 3.607591152191162 secs\n",
      "\n",
      "Epoch 44 Batch 0 Loss 0.2395\n",
      "Epoch 44 Batch 8 Loss 0.1877\n",
      "Epoch 44 Batch 16 Loss 0.1943\n",
      "Epoch 44 Batch 24 Loss 0.1847\n",
      "Epoch 44 Batch 32 Loss 0.1947\n",
      "Epoch 44 Batch 40 Loss 0.2021\n",
      "Epoch 44 Batch 48 Loss 0.2018\n",
      "Epoch 44 Batch 56 Loss 0.1971\n",
      "Epoch 44 Batch 64 Loss 0.2021\n",
      "Epoch 44 Batch 72 Loss 0.2025\n",
      "Epoch 44 Batch 80 Loss 0.2039\n",
      "Epoch 44 Batch 88 Loss 0.2096\n",
      "Epoch 44 Batch 96 Loss 0.2089\n",
      "Epoch 44 Batch 104 Loss 0.2099\n",
      "Epoch 44 Batch 112 Loss 0.2093\n",
      "Epoch 44 Batch 120 Loss 0.2141\n",
      "Epoch 44 Loss 0.2164\n",
      "Time taken for 1 epoch: 3.5994527339935303 secs\n",
      "\n",
      "Epoch 45 Batch 0 Loss 0.1909\n",
      "Epoch 45 Batch 8 Loss 0.1464\n",
      "Epoch 45 Batch 16 Loss 0.1605\n",
      "Epoch 45 Batch 24 Loss 0.1632\n",
      "Epoch 45 Batch 32 Loss 0.1684\n",
      "Epoch 45 Batch 40 Loss 0.1691\n",
      "Epoch 45 Batch 48 Loss 0.1761\n",
      "Epoch 45 Batch 56 Loss 0.1863\n",
      "Epoch 45 Batch 64 Loss 0.1914\n",
      "Epoch 45 Batch 72 Loss 0.2010\n",
      "Epoch 45 Batch 80 Loss 0.2025\n",
      "Epoch 45 Batch 88 Loss 0.2047\n",
      "Epoch 45 Batch 96 Loss 0.2117\n",
      "Epoch 45 Batch 104 Loss 0.2112\n",
      "Epoch 45 Batch 112 Loss 0.2135\n",
      "Epoch 45 Batch 120 Loss 0.2169\n",
      "Saving checkpoint for epoch 45 at checkpoints/ckpt-9\n",
      "Epoch 45 Loss 0.2191\n",
      "Time taken for 1 epoch: 3.9293720722198486 secs\n",
      "\n",
      "Epoch 46 Batch 0 Loss 0.1175\n",
      "Epoch 46 Batch 8 Loss 0.1173\n",
      "Epoch 46 Batch 16 Loss 0.1188\n",
      "Epoch 46 Batch 24 Loss 0.1240\n",
      "Epoch 46 Batch 32 Loss 0.1323\n",
      "Epoch 46 Batch 40 Loss 0.1375\n",
      "Epoch 46 Batch 48 Loss 0.1553\n",
      "Epoch 46 Batch 56 Loss 0.1627\n",
      "Epoch 46 Batch 64 Loss 0.1753\n",
      "Epoch 46 Batch 72 Loss 0.1789\n",
      "Epoch 46 Batch 80 Loss 0.1804\n",
      "Epoch 46 Batch 88 Loss 0.1808\n",
      "Epoch 46 Batch 96 Loss 0.1819\n",
      "Epoch 46 Batch 104 Loss 0.1847\n",
      "Epoch 46 Batch 112 Loss 0.1845\n",
      "Epoch 46 Batch 120 Loss 0.1918\n",
      "Epoch 46 Loss 0.1947\n",
      "Time taken for 1 epoch: 3.5905344486236572 secs\n",
      "\n",
      "Epoch 47 Batch 0 Loss 0.0822\n",
      "Epoch 47 Batch 8 Loss 0.1174\n",
      "Epoch 47 Batch 16 Loss 0.1231\n",
      "Epoch 47 Batch 24 Loss 0.1456\n",
      "Epoch 47 Batch 32 Loss 0.1399\n",
      "Epoch 47 Batch 40 Loss 0.1442\n",
      "Epoch 47 Batch 48 Loss 0.1424\n",
      "Epoch 47 Batch 56 Loss 0.1535\n",
      "Epoch 47 Batch 64 Loss 0.1519\n",
      "Epoch 47 Batch 72 Loss 0.1588\n",
      "Epoch 47 Batch 80 Loss 0.1678\n",
      "Epoch 47 Batch 88 Loss 0.1748\n",
      "Epoch 47 Batch 96 Loss 0.1806\n",
      "Epoch 47 Batch 104 Loss 0.1852\n",
      "Epoch 47 Batch 112 Loss 0.1884\n",
      "Epoch 47 Batch 120 Loss 0.1876\n",
      "Epoch 47 Loss 0.1892\n",
      "Time taken for 1 epoch: 3.603983163833618 secs\n",
      "\n",
      "Epoch 48 Batch 0 Loss 0.0855\n",
      "Epoch 48 Batch 8 Loss 0.1277\n",
      "Epoch 48 Batch 16 Loss 0.1348\n",
      "Epoch 48 Batch 24 Loss 0.1320\n",
      "Epoch 48 Batch 32 Loss 0.1556\n",
      "Epoch 48 Batch 40 Loss 0.1591\n",
      "Epoch 48 Batch 48 Loss 0.1644\n",
      "Epoch 48 Batch 56 Loss 0.1632\n",
      "Epoch 48 Batch 64 Loss 0.1622\n",
      "Epoch 48 Batch 72 Loss 0.1675\n",
      "Epoch 48 Batch 80 Loss 0.1697\n",
      "Epoch 48 Batch 88 Loss 0.1701\n",
      "Epoch 48 Batch 96 Loss 0.1735\n",
      "Epoch 48 Batch 104 Loss 0.1761\n",
      "Epoch 48 Batch 112 Loss 0.1805\n",
      "Epoch 48 Batch 120 Loss 0.1849\n",
      "Epoch 48 Loss 0.1857\n",
      "Time taken for 1 epoch: 3.615100860595703 secs\n",
      "\n",
      "Epoch 49 Batch 0 Loss 0.1813\n",
      "Epoch 49 Batch 8 Loss 0.1575\n",
      "Epoch 49 Batch 16 Loss 0.1433\n",
      "Epoch 49 Batch 24 Loss 0.1350\n",
      "Epoch 49 Batch 32 Loss 0.1385\n",
      "Epoch 49 Batch 40 Loss 0.1390\n",
      "Epoch 49 Batch 48 Loss 0.1388\n",
      "Epoch 49 Batch 56 Loss 0.1395\n",
      "Epoch 49 Batch 64 Loss 0.1441\n",
      "Epoch 49 Batch 72 Loss 0.1424\n",
      "Epoch 49 Batch 80 Loss 0.1456\n",
      "Epoch 49 Batch 88 Loss 0.1426\n",
      "Epoch 49 Batch 96 Loss 0.1422\n",
      "Epoch 49 Batch 104 Loss 0.1458\n",
      "Epoch 49 Batch 112 Loss 0.1492\n",
      "Epoch 49 Batch 120 Loss 0.1536\n",
      "Epoch 49 Loss 0.1580\n",
      "Time taken for 1 epoch: 3.606274366378784 secs\n",
      "\n",
      "Epoch 50 Batch 0 Loss 0.0950\n",
      "Epoch 50 Batch 8 Loss 0.1393\n",
      "Epoch 50 Batch 16 Loss 0.1337\n",
      "Epoch 50 Batch 24 Loss 0.1383\n",
      "Epoch 50 Batch 32 Loss 0.1447\n",
      "Epoch 50 Batch 40 Loss 0.1478\n",
      "Epoch 50 Batch 48 Loss 0.1461\n",
      "Epoch 50 Batch 56 Loss 0.1448\n",
      "Epoch 50 Batch 64 Loss 0.1412\n",
      "Epoch 50 Batch 72 Loss 0.1443\n",
      "Epoch 50 Batch 80 Loss 0.1424\n",
      "Epoch 50 Batch 88 Loss 0.1451\n",
      "Epoch 50 Batch 96 Loss 0.1463\n",
      "Epoch 50 Batch 104 Loss 0.1502\n",
      "Epoch 50 Batch 112 Loss 0.1527\n",
      "Epoch 50 Batch 120 Loss 0.1553\n",
      "Saving checkpoint for epoch 50 at checkpoints/ckpt-10\n",
      "Epoch 50 Loss 0.1599\n",
      "Time taken for 1 epoch: 3.9242467880249023 secs\n",
      "\n",
      "Epoch 51 Batch 0 Loss 0.0362\n",
      "Epoch 51 Batch 8 Loss 0.1008\n",
      "Epoch 51 Batch 16 Loss 0.1071\n",
      "Epoch 51 Batch 24 Loss 0.1299\n",
      "Epoch 51 Batch 32 Loss 0.1366\n",
      "Epoch 51 Batch 40 Loss 0.1343\n",
      "Epoch 51 Batch 48 Loss 0.1330\n",
      "Epoch 51 Batch 56 Loss 0.1388\n",
      "Epoch 51 Batch 64 Loss 0.1410\n",
      "Epoch 51 Batch 72 Loss 0.1475\n",
      "Epoch 51 Batch 80 Loss 0.1521\n",
      "Epoch 51 Batch 88 Loss 0.1524\n",
      "Epoch 51 Batch 96 Loss 0.1565\n",
      "Epoch 51 Batch 104 Loss 0.1557\n",
      "Epoch 51 Batch 112 Loss 0.1554\n",
      "Epoch 51 Batch 120 Loss 0.1568\n",
      "Epoch 51 Loss 0.1563\n",
      "Time taken for 1 epoch: 3.615276336669922 secs\n",
      "\n",
      "Epoch 52 Batch 0 Loss 0.1728\n",
      "Epoch 52 Batch 8 Loss 0.1243\n",
      "Epoch 52 Batch 16 Loss 0.1178\n",
      "Epoch 52 Batch 24 Loss 0.1171\n",
      "Epoch 52 Batch 32 Loss 0.1188\n",
      "Epoch 52 Batch 40 Loss 0.1142\n",
      "Epoch 52 Batch 48 Loss 0.1171\n",
      "Epoch 52 Batch 56 Loss 0.1206\n",
      "Epoch 52 Batch 64 Loss 0.1194\n",
      "Epoch 52 Batch 72 Loss 0.1224\n",
      "Epoch 52 Batch 80 Loss 0.1265\n",
      "Epoch 52 Batch 88 Loss 0.1289\n",
      "Epoch 52 Batch 96 Loss 0.1267\n",
      "Epoch 52 Batch 104 Loss 0.1306\n",
      "Epoch 52 Batch 112 Loss 0.1343\n",
      "Epoch 52 Batch 120 Loss 0.1359\n",
      "Epoch 52 Loss 0.1380\n",
      "Time taken for 1 epoch: 3.62400484085083 secs\n",
      "\n",
      "Epoch 53 Batch 0 Loss 0.1940\n",
      "Epoch 53 Batch 8 Loss 0.1190\n",
      "Epoch 53 Batch 16 Loss 0.1378\n",
      "Epoch 53 Batch 24 Loss 0.1441\n",
      "Epoch 53 Batch 32 Loss 0.1429\n",
      "Epoch 53 Batch 40 Loss 0.1382\n",
      "Epoch 53 Batch 48 Loss 0.1401\n",
      "Epoch 53 Batch 56 Loss 0.1370\n",
      "Epoch 53 Batch 64 Loss 0.1420\n",
      "Epoch 53 Batch 72 Loss 0.1379\n",
      "Epoch 53 Batch 80 Loss 0.1363\n",
      "Epoch 53 Batch 88 Loss 0.1357\n",
      "Epoch 53 Batch 96 Loss 0.1358\n",
      "Epoch 53 Batch 104 Loss 0.1372\n",
      "Epoch 53 Batch 112 Loss 0.1404\n",
      "Epoch 53 Batch 120 Loss 0.1409\n",
      "Epoch 53 Loss 0.1409\n",
      "Time taken for 1 epoch: 3.6247479915618896 secs\n",
      "\n",
      "Epoch 54 Batch 0 Loss 0.0948\n",
      "Epoch 54 Batch 8 Loss 0.0918\n",
      "Epoch 54 Batch 16 Loss 0.1228\n",
      "Epoch 54 Batch 24 Loss 0.1228\n",
      "Epoch 54 Batch 32 Loss 0.1314\n",
      "Epoch 54 Batch 40 Loss 0.1352\n",
      "Epoch 54 Batch 48 Loss 0.1288\n",
      "Epoch 54 Batch 56 Loss 0.1324\n",
      "Epoch 54 Batch 64 Loss 0.1264\n",
      "Epoch 54 Batch 72 Loss 0.1300\n",
      "Epoch 54 Batch 80 Loss 0.1280\n",
      "Epoch 54 Batch 88 Loss 0.1258\n",
      "Epoch 54 Batch 96 Loss 0.1292\n",
      "Epoch 54 Batch 104 Loss 0.1285\n",
      "Epoch 54 Batch 112 Loss 0.1300\n",
      "Epoch 54 Batch 120 Loss 0.1331\n",
      "Epoch 54 Loss 0.1335\n",
      "Time taken for 1 epoch: 3.6351349353790283 secs\n",
      "\n",
      "Epoch 55 Batch 0 Loss 0.1302\n",
      "Epoch 55 Batch 8 Loss 0.0873\n",
      "Epoch 55 Batch 16 Loss 0.1101\n",
      "Epoch 55 Batch 24 Loss 0.1144\n",
      "Epoch 55 Batch 32 Loss 0.1040\n",
      "Epoch 55 Batch 40 Loss 0.1035\n",
      "Epoch 55 Batch 48 Loss 0.1001\n",
      "Epoch 55 Batch 56 Loss 0.1022\n",
      "Epoch 55 Batch 64 Loss 0.1000\n",
      "Epoch 55 Batch 72 Loss 0.1023\n",
      "Epoch 55 Batch 80 Loss 0.1019\n",
      "Epoch 55 Batch 88 Loss 0.1064\n",
      "Epoch 55 Batch 96 Loss 0.1071\n",
      "Epoch 55 Batch 104 Loss 0.1085\n",
      "Epoch 55 Batch 112 Loss 0.1083\n",
      "Epoch 55 Batch 120 Loss 0.1085\n",
      "Saving checkpoint for epoch 55 at checkpoints/ckpt-11\n",
      "Epoch 55 Loss 0.1116\n",
      "Time taken for 1 epoch: 3.9737327098846436 secs\n",
      "\n",
      "Epoch 56 Batch 0 Loss 0.0950\n",
      "Epoch 56 Batch 8 Loss 0.1131\n",
      "Epoch 56 Batch 16 Loss 0.1093\n",
      "Epoch 56 Batch 24 Loss 0.1235\n",
      "Epoch 56 Batch 32 Loss 0.1161\n",
      "Epoch 56 Batch 40 Loss 0.1165\n",
      "Epoch 56 Batch 48 Loss 0.1187\n",
      "Epoch 56 Batch 56 Loss 0.1115\n",
      "Epoch 56 Batch 64 Loss 0.1151\n",
      "Epoch 56 Batch 72 Loss 0.1148\n",
      "Epoch 56 Batch 80 Loss 0.1162\n",
      "Epoch 56 Batch 88 Loss 0.1168\n",
      "Epoch 56 Batch 96 Loss 0.1219\n",
      "Epoch 56 Batch 104 Loss 0.1259\n",
      "Epoch 56 Batch 112 Loss 0.1264\n",
      "Epoch 56 Batch 120 Loss 0.1294\n",
      "Epoch 56 Loss 0.1312\n",
      "Time taken for 1 epoch: 3.6136693954467773 secs\n",
      "\n",
      "Epoch 57 Batch 0 Loss 0.0242\n",
      "Epoch 57 Batch 8 Loss 0.1048\n",
      "Epoch 57 Batch 16 Loss 0.0918\n",
      "Epoch 57 Batch 24 Loss 0.0933\n",
      "Epoch 57 Batch 32 Loss 0.0950\n",
      "Epoch 57 Batch 40 Loss 0.0961\n",
      "Epoch 57 Batch 48 Loss 0.0931\n",
      "Epoch 57 Batch 56 Loss 0.1046\n",
      "Epoch 57 Batch 64 Loss 0.1086\n",
      "Epoch 57 Batch 72 Loss 0.1153\n",
      "Epoch 57 Batch 80 Loss 0.1176\n",
      "Epoch 57 Batch 88 Loss 0.1210\n",
      "Epoch 57 Batch 96 Loss 0.1220\n",
      "Epoch 57 Batch 104 Loss 0.1202\n",
      "Epoch 57 Batch 112 Loss 0.1198\n",
      "Epoch 57 Batch 120 Loss 0.1202\n",
      "Epoch 57 Loss 0.1201\n",
      "Time taken for 1 epoch: 3.9422011375427246 secs\n",
      "\n",
      "Epoch 58 Batch 0 Loss 0.2148\n",
      "Epoch 58 Batch 8 Loss 0.0944\n",
      "Epoch 58 Batch 16 Loss 0.1009\n",
      "Epoch 58 Batch 24 Loss 0.1155\n",
      "Epoch 58 Batch 32 Loss 0.1205\n",
      "Epoch 58 Batch 40 Loss 0.1250\n",
      "Epoch 58 Batch 48 Loss 0.1276\n",
      "Epoch 58 Batch 56 Loss 0.1231\n",
      "Epoch 58 Batch 64 Loss 0.1200\n",
      "Epoch 58 Batch 72 Loss 0.1145\n",
      "Epoch 58 Batch 80 Loss 0.1169\n",
      "Epoch 58 Batch 88 Loss 0.1157\n",
      "Epoch 58 Batch 96 Loss 0.1135\n",
      "Epoch 58 Batch 104 Loss 0.1139\n",
      "Epoch 58 Batch 112 Loss 0.1137\n",
      "Epoch 58 Batch 120 Loss 0.1132\n",
      "Epoch 58 Loss 0.1120\n",
      "Time taken for 1 epoch: 4.15507698059082 secs\n",
      "\n",
      "Epoch 59 Batch 0 Loss 0.0219\n",
      "Epoch 59 Batch 8 Loss 0.1020\n",
      "Epoch 59 Batch 16 Loss 0.0975\n",
      "Epoch 59 Batch 24 Loss 0.0905\n",
      "Epoch 59 Batch 32 Loss 0.0982\n",
      "Epoch 59 Batch 40 Loss 0.0985\n",
      "Epoch 59 Batch 48 Loss 0.0985\n",
      "Epoch 59 Batch 56 Loss 0.0963\n",
      "Epoch 59 Batch 64 Loss 0.0965\n",
      "Epoch 59 Batch 72 Loss 0.0979\n",
      "Epoch 59 Batch 80 Loss 0.0960\n",
      "Epoch 59 Batch 88 Loss 0.0957\n",
      "Epoch 59 Batch 96 Loss 0.0961\n",
      "Epoch 59 Batch 104 Loss 0.0968\n",
      "Epoch 59 Batch 112 Loss 0.0982\n",
      "Epoch 59 Batch 120 Loss 0.0983\n",
      "Epoch 59 Loss 0.1002\n",
      "Time taken for 1 epoch: 3.607855796813965 secs\n",
      "\n",
      "Epoch 60 Batch 0 Loss 0.0502\n",
      "Epoch 60 Batch 8 Loss 0.0737\n",
      "Epoch 60 Batch 16 Loss 0.0743\n",
      "Epoch 60 Batch 24 Loss 0.0848\n",
      "Epoch 60 Batch 32 Loss 0.1020\n",
      "Epoch 60 Batch 40 Loss 0.0945\n",
      "Epoch 60 Batch 48 Loss 0.0949\n",
      "Epoch 60 Batch 56 Loss 0.0957\n",
      "Epoch 60 Batch 64 Loss 0.0968\n",
      "Epoch 60 Batch 72 Loss 0.0987\n",
      "Epoch 60 Batch 80 Loss 0.1030\n",
      "Epoch 60 Batch 88 Loss 0.1039\n",
      "Epoch 60 Batch 96 Loss 0.1027\n",
      "Epoch 60 Batch 104 Loss 0.1036\n",
      "Epoch 60 Batch 112 Loss 0.1072\n",
      "Epoch 60 Batch 120 Loss 0.1061\n",
      "Saving checkpoint for epoch 60 at checkpoints/ckpt-12\n",
      "Epoch 60 Loss 0.1089\n",
      "Time taken for 1 epoch: 3.9395201206207275 secs\n",
      "\n",
      "Epoch 61 Batch 0 Loss 0.0895\n",
      "Epoch 61 Batch 8 Loss 0.0640\n",
      "Epoch 61 Batch 16 Loss 0.0885\n",
      "Epoch 61 Batch 24 Loss 0.0895\n",
      "Epoch 61 Batch 32 Loss 0.0835\n",
      "Epoch 61 Batch 40 Loss 0.0788\n",
      "Epoch 61 Batch 48 Loss 0.0877\n",
      "Epoch 61 Batch 56 Loss 0.0835\n",
      "Epoch 61 Batch 64 Loss 0.0875\n",
      "Epoch 61 Batch 72 Loss 0.0911\n",
      "Epoch 61 Batch 80 Loss 0.0942\n",
      "Epoch 61 Batch 88 Loss 0.0963\n",
      "Epoch 61 Batch 96 Loss 0.0935\n",
      "Epoch 61 Batch 104 Loss 0.0947\n",
      "Epoch 61 Batch 112 Loss 0.0998\n",
      "Epoch 61 Batch 120 Loss 0.1030\n",
      "Epoch 61 Loss 0.1051\n",
      "Time taken for 1 epoch: 3.606438398361206 secs\n",
      "\n",
      "Epoch 62 Batch 0 Loss 0.0420\n",
      "Epoch 62 Batch 8 Loss 0.1089\n",
      "Epoch 62 Batch 16 Loss 0.1041\n",
      "Epoch 62 Batch 24 Loss 0.1012\n",
      "Epoch 62 Batch 32 Loss 0.1030\n",
      "Epoch 62 Batch 40 Loss 0.0938\n",
      "Epoch 62 Batch 48 Loss 0.0920\n",
      "Epoch 62 Batch 56 Loss 0.0899\n",
      "Epoch 62 Batch 64 Loss 0.0900\n",
      "Epoch 62 Batch 72 Loss 0.0904\n",
      "Epoch 62 Batch 80 Loss 0.0899\n",
      "Epoch 62 Batch 88 Loss 0.0903\n",
      "Epoch 62 Batch 96 Loss 0.0909\n",
      "Epoch 62 Batch 104 Loss 0.0931\n",
      "Epoch 62 Batch 112 Loss 0.0921\n",
      "Epoch 62 Batch 120 Loss 0.0932\n",
      "Epoch 62 Loss 0.0933\n",
      "Time taken for 1 epoch: 3.621480941772461 secs\n",
      "\n",
      "Epoch 63 Batch 0 Loss 0.1090\n",
      "Epoch 63 Batch 8 Loss 0.0817\n",
      "Epoch 63 Batch 16 Loss 0.0753\n",
      "Epoch 63 Batch 24 Loss 0.0793\n",
      "Epoch 63 Batch 32 Loss 0.0820\n",
      "Epoch 63 Batch 40 Loss 0.0824\n",
      "Epoch 63 Batch 48 Loss 0.0787\n",
      "Epoch 63 Batch 56 Loss 0.0809\n",
      "Epoch 63 Batch 64 Loss 0.0824\n",
      "Epoch 63 Batch 72 Loss 0.0823\n",
      "Epoch 63 Batch 80 Loss 0.0794\n",
      "Epoch 63 Batch 88 Loss 0.0835\n",
      "Epoch 63 Batch 96 Loss 0.0853\n",
      "Epoch 63 Batch 104 Loss 0.0894\n",
      "Epoch 63 Batch 112 Loss 0.0937\n",
      "Epoch 63 Batch 120 Loss 0.0943\n",
      "Epoch 63 Loss 0.0965\n",
      "Time taken for 1 epoch: 3.6272096633911133 secs\n",
      "\n",
      "Epoch 64 Batch 0 Loss 0.1843\n",
      "Epoch 64 Batch 8 Loss 0.1025\n",
      "Epoch 64 Batch 16 Loss 0.0868\n",
      "Epoch 64 Batch 24 Loss 0.0902\n",
      "Epoch 64 Batch 32 Loss 0.0845\n",
      "Epoch 64 Batch 40 Loss 0.0815\n",
      "Epoch 64 Batch 48 Loss 0.0840\n",
      "Epoch 64 Batch 56 Loss 0.0845\n",
      "Epoch 64 Batch 64 Loss 0.0818\n",
      "Epoch 64 Batch 72 Loss 0.0820\n",
      "Epoch 64 Batch 80 Loss 0.0857\n",
      "Epoch 64 Batch 88 Loss 0.0899\n",
      "Epoch 64 Batch 96 Loss 0.0915\n",
      "Epoch 64 Batch 104 Loss 0.0922\n",
      "Epoch 64 Batch 112 Loss 0.0959\n",
      "Epoch 64 Batch 120 Loss 0.0963\n",
      "Epoch 64 Loss 0.0968\n",
      "Time taken for 1 epoch: 3.623608350753784 secs\n",
      "\n",
      "Epoch 65 Batch 0 Loss 0.0553\n",
      "Epoch 65 Batch 8 Loss 0.0732\n",
      "Epoch 65 Batch 16 Loss 0.0634\n",
      "Epoch 65 Batch 24 Loss 0.0645\n",
      "Epoch 65 Batch 32 Loss 0.0737\n",
      "Epoch 65 Batch 40 Loss 0.0719\n",
      "Epoch 65 Batch 48 Loss 0.0730\n",
      "Epoch 65 Batch 56 Loss 0.0699\n",
      "Epoch 65 Batch 64 Loss 0.0718\n",
      "Epoch 65 Batch 72 Loss 0.0709\n",
      "Epoch 65 Batch 80 Loss 0.0722\n",
      "Epoch 65 Batch 88 Loss 0.0770\n",
      "Epoch 65 Batch 96 Loss 0.0830\n",
      "Epoch 65 Batch 104 Loss 0.0877\n",
      "Epoch 65 Batch 112 Loss 0.0898\n",
      "Epoch 65 Batch 120 Loss 0.0887\n",
      "Saving checkpoint for epoch 65 at checkpoints/ckpt-13\n",
      "Epoch 65 Loss 0.0885\n",
      "Time taken for 1 epoch: 3.946296215057373 secs\n",
      "\n",
      "Epoch 66 Batch 0 Loss 0.0334\n",
      "Epoch 66 Batch 8 Loss 0.0752\n",
      "Epoch 66 Batch 16 Loss 0.0752\n",
      "Epoch 66 Batch 24 Loss 0.0729\n",
      "Epoch 66 Batch 32 Loss 0.0616\n",
      "Epoch 66 Batch 40 Loss 0.0633\n",
      "Epoch 66 Batch 48 Loss 0.0673\n",
      "Epoch 66 Batch 56 Loss 0.0720\n",
      "Epoch 66 Batch 64 Loss 0.0721\n",
      "Epoch 66 Batch 72 Loss 0.0771\n",
      "Epoch 66 Batch 80 Loss 0.0797\n",
      "Epoch 66 Batch 88 Loss 0.0832\n",
      "Epoch 66 Batch 96 Loss 0.0857\n",
      "Epoch 66 Batch 104 Loss 0.0844\n",
      "Epoch 66 Batch 112 Loss 0.0870\n",
      "Epoch 66 Batch 120 Loss 0.0857\n",
      "Epoch 66 Loss 0.0867\n",
      "Time taken for 1 epoch: 3.6017704010009766 secs\n",
      "\n",
      "Epoch 67 Batch 0 Loss 0.0750\n",
      "Epoch 67 Batch 8 Loss 0.0551\n",
      "Epoch 67 Batch 16 Loss 0.0583\n",
      "Epoch 67 Batch 24 Loss 0.0673\n",
      "Epoch 67 Batch 32 Loss 0.0666\n",
      "Epoch 67 Batch 40 Loss 0.0647\n",
      "Epoch 67 Batch 48 Loss 0.0626\n",
      "Epoch 67 Batch 56 Loss 0.0646\n",
      "Epoch 67 Batch 64 Loss 0.0718\n",
      "Epoch 67 Batch 72 Loss 0.0711\n",
      "Epoch 67 Batch 80 Loss 0.0694\n",
      "Epoch 67 Batch 88 Loss 0.0708\n",
      "Epoch 67 Batch 96 Loss 0.0718\n",
      "Epoch 67 Batch 104 Loss 0.0730\n",
      "Epoch 67 Batch 112 Loss 0.0734\n",
      "Epoch 67 Batch 120 Loss 0.0741\n",
      "Epoch 67 Loss 0.0760\n",
      "Time taken for 1 epoch: 3.611581802368164 secs\n",
      "\n",
      "Epoch 68 Batch 0 Loss 0.0276\n",
      "Epoch 68 Batch 8 Loss 0.0506\n",
      "Epoch 68 Batch 16 Loss 0.0460\n",
      "Epoch 68 Batch 24 Loss 0.0628\n",
      "Epoch 68 Batch 32 Loss 0.0598\n",
      "Epoch 68 Batch 40 Loss 0.0639\n",
      "Epoch 68 Batch 48 Loss 0.0673\n",
      "Epoch 68 Batch 56 Loss 0.0688\n",
      "Epoch 68 Batch 64 Loss 0.0735\n",
      "Epoch 68 Batch 72 Loss 0.0807\n",
      "Epoch 68 Batch 80 Loss 0.0787\n",
      "Epoch 68 Batch 88 Loss 0.0831\n",
      "Epoch 68 Batch 96 Loss 0.0840\n",
      "Epoch 68 Batch 104 Loss 0.0843\n",
      "Epoch 68 Batch 112 Loss 0.0845\n",
      "Epoch 68 Batch 120 Loss 0.0863\n",
      "Epoch 68 Loss 0.0879\n",
      "Time taken for 1 epoch: 3.6190550327301025 secs\n",
      "\n",
      "Epoch 69 Batch 0 Loss 0.0680\n",
      "Epoch 69 Batch 8 Loss 0.0664\n",
      "Epoch 69 Batch 16 Loss 0.0783\n",
      "Epoch 69 Batch 24 Loss 0.0771\n",
      "Epoch 69 Batch 32 Loss 0.0758\n",
      "Epoch 69 Batch 40 Loss 0.0755\n",
      "Epoch 69 Batch 48 Loss 0.0740\n",
      "Epoch 69 Batch 56 Loss 0.0710\n",
      "Epoch 69 Batch 64 Loss 0.0672\n",
      "Epoch 69 Batch 72 Loss 0.0675\n",
      "Epoch 69 Batch 80 Loss 0.0679\n",
      "Epoch 69 Batch 88 Loss 0.0679\n",
      "Epoch 69 Batch 96 Loss 0.0710\n",
      "Epoch 69 Batch 104 Loss 0.0717\n",
      "Epoch 69 Batch 112 Loss 0.0719\n",
      "Epoch 69 Batch 120 Loss 0.0744\n",
      "Epoch 69 Loss 0.0755\n",
      "Time taken for 1 epoch: 3.607945203781128 secs\n",
      "\n",
      "Epoch 70 Batch 0 Loss 0.0180\n",
      "Epoch 70 Batch 8 Loss 0.0519\n",
      "Epoch 70 Batch 16 Loss 0.0600\n",
      "Epoch 70 Batch 24 Loss 0.0652\n",
      "Epoch 70 Batch 32 Loss 0.0647\n",
      "Epoch 70 Batch 40 Loss 0.0639\n",
      "Epoch 70 Batch 48 Loss 0.0754\n",
      "Epoch 70 Batch 56 Loss 0.0805\n",
      "Epoch 70 Batch 64 Loss 0.0795\n",
      "Epoch 70 Batch 72 Loss 0.0814\n",
      "Epoch 70 Batch 80 Loss 0.0832\n",
      "Epoch 70 Batch 88 Loss 0.0816\n",
      "Epoch 70 Batch 96 Loss 0.0805\n",
      "Epoch 70 Batch 104 Loss 0.0797\n",
      "Epoch 70 Batch 112 Loss 0.0781\n",
      "Epoch 70 Batch 120 Loss 0.0796\n",
      "Saving checkpoint for epoch 70 at checkpoints/ckpt-14\n",
      "Epoch 70 Loss 0.0807\n",
      "Time taken for 1 epoch: 3.941617250442505 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "  \n",
    "    for (batch, (inp, tar)) in enumerate(dataset):\n",
    "        train_step(inp, tar)\n",
    "    \n",
    "        # 55k samples\n",
    "        # we display 3 batch results -- 0th, middle and last one (approx)\n",
    "        # 55k / 64 ~ 858; 858 / 2 = 429\n",
    "        if batch % 8 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, train_loss.result()))\n",
    "      \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
    "    \n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch + 1, train_loss.result()))\n",
    "\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "5MmWB9wpOp6x"
   },
   "outputs": [],
   "source": [
    "def evaluate(input_document):\n",
    "    input_document = document_tokenizer.texts_to_sequences([input_document])\n",
    "    input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
    "\n",
    "    encoder_input = tf.expand_dims(input_document[0], 0)\n",
    "\n",
    "    decoder_input = [summary_tokenizer.word_index[\"<go>\"]]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "    \n",
    "    for i in range(decoder_maxlen):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
    "\n",
    "        predictions, attention_weights = transformer(\n",
    "            encoder_input, \n",
    "            output,\n",
    "            False,\n",
    "            enc_padding_mask,\n",
    "            combined_mask,\n",
    "            dec_padding_mask\n",
    "        )\n",
    "\n",
    "        predictions = predictions[: ,-1:, :]\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        if predicted_id == summary_tokenizer.word_index[\"<stop>\"]:\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "g308MId-PCmh"
   },
   "outputs": [],
   "source": [
    "def summarize(input_document):\n",
    "    summarized = evaluate(input_document=input_document)[0].numpy()\n",
    "    summarized = np.expand_dims(summarized[1:], 0)  # not printing <go> token\n",
    "    return summary_tokenizer.sequences_to_texts(summarized)[0]  # since there is just one translated document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "pD6a2l8YPFge",
    "outputId": "731e3cab-9852-4a62-8556-0985611fb1df"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'ভালো কথা বল নয়ত চুপ থাকো'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(\"মানুষের মুখ খুব শ ক্তিশালী এক জিনিস। মানুষ যেটা বলে সেটার একটা প্রভাব আছে।অভিজ্ঞতা থেকে দেখেছি একটা ছেলেকে ক্ষেপানোর জন্য বলা হতো, অমুক মেয়ের সাথে তুই প্রেম করিস। কদিন পরে সত্যি সত্যি তারা প্রেম করা শুরু করে দিয়েছিল।স্বামী-স্ত্রীর মনোমালিন্যর সময় হয়ত স্ত্রী আফসোস করে বলল, আমি পুরোনো হয়ে গেছি - এখন তো আর আমাকে ভালো লাগবে না।সত্যি সত্যি দেখা যাবে কদিন পরে স্বামীর ঠিক ওই জিনিসটাই মনে হতে থাকবে। অথচ হয়ত সে এ ব্যাপারে আগে ভাবেইনি।একটা ছেলেকে পরিবারের সবাই বলে, তুই কোনো কাজের না - দেখা যাবে ছেলেটা আসলেই কিছু করতে পারছে না।এজন্য রসুল সাল্লাল্লাহু আলাইহি ওয়া সাল্লাম বলেছেন, হয় ভালো কথা বলো নয়ত চুপ থাক।আমাদের জীবনের বহু ভালো পরিস্থিতি খারাপ থেকে খারাপ হয়েছে শুধুমাত্র আমাদের কথার কারণে।জিহবা সাবধান ভাইয়েরা। মুখ সাবধান বোনেরা।রসুল সাল্লাল্লাহু আলাইহি ওয়া সাল্লামের কথাটাকে দাম দিই - সংসারে শান্তি আসবে, আয়ে বারাকাহ আসবে।\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "ERj84JjDP_x-",
    "outputId": "20c03aa5-5a61-48b9-f80d-870fba81b61b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'সফলতা হচ্ছে একটি আপেক্ষিক ব্যাপার'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(\"ক্লাসে সবচেয়ে দূর্বল ছেলেটি কাল সমাবর্তনে এসেছিল সবার চেয়ে হাই পজিশনের জব নিয়ে। বারবার প্রেমে ব্যর্থ হওয়া মেয়েটি এসেছিল একটি সুন্দর ছোট্ট পরিবার নিয়ে। কারো কাছে পাত্তা না পাওয়া, তোকে দিয়ে কিছু হবে না বলা ছেলেটিই সবচেয়ে সুন্দর বউ নিয়ে এসেছে। পড়াশোনার খরচ যোগাতে টিউশন করে হাত খরচ চালানো মেয়েটি কাল গাড়ি দিয়ে ক্যাম্পাসে এসেছিল। ক্লাসের সবচেয়ে সাক্সেস্ফুল ছেলেটি ডিপ্রেশনে ভুগছে জব না পাওয়ায়। ডিপার্টমেন্টের হার্টথ্রোব মেয়েটির চোখে নিচে কালি বিয়ে হচ্ছে না বয়স হয়ে গেছে।এভাবেই সময়ের সাথে বদলে যায় মানুষের জীবনে ইকুয়েশন। আসলে সমাবর্তনের মাধ্যমে শিক্ষা জীবনের শেষ হলেও সফলতা ও ব্যর্থজীবনের হিসাব গণনা শুরু হয়ে এখান থেকেই।তাই ঘৃণা, হিংসা, কম্পিটিশন বাদ দিয়ে জীবনটাকে বাচা উচিত সম্পূর্ণ স্বাদ ও ভালবাসা নিয়ে। কখন জীবনের কোন মোড় দেখায় কোন নিশ্চয়তা নেই, তাই কোন মুহূর্তের জন্য যাতে আফসোস না থাকে।\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "5YKHMymdSugA",
    "outputId": "768ed7ca-892e-44a9-8cce-7e4f47e50c30"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'কমনসেন্সহীন মানুষগুলোর সমস্যাটা কি'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(\"ইতালির প্রধানমন্ত্রী জুসেপ্পে কন্তে বলেছেন, একেকজন বাংলাদেশি একেকটা ভাইরাস বোমা। অনেক বাংলাদেশি ভাই সেটাকে শেয়ার করে দেশকে পরোক্ষভাবে তিরস্কার করছেন। অথচ কন্তে সাহেবকে বলা দরকার, আমাদের অসচেতনতা নিয়ে এমন ঢালাও মন্তব্য করার আগে আপনার অগ্রজ শাসকদের দিকে তাকান। নিরো সাহেবের দিকে তাকান।আপনি কী জানেন না? রোম যখন পুড়ছিলো, নিরো তখন সাহেব বাঁশি বাজাচ্ছিলেন।আর আমরা একটু বাঁশি বাজালেই দোষ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "Swe6Dl93DEfY",
    "outputId": "58a3bd29-90d1-4da8-fc26-16679247d065"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'জেদ মানুষ কে বড় করতে পারে না'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(\"মুক্তির সারথি বাংলাদেশ সাধারণ ছাত্র অধিকার সংরক্ষণ পরিষদ আমাদের দেশে স্বাধীনতার পর থেকে চলমান অসুস্থ ধারার রাজনীতি এখনো বহাল তবিয়তে চলছে। এই অবস্থা থেকে বের হয়ে আসতে না পারলে দেশ ও জাতি আরো গভীর অন্ধকারে নিমজ্জিত হবে। একটি দেশের প্রকৃত উন্নয়ন নির্ভর করে সেদেশের রাজনৈতিক স্থিতিশীলতার উপর। দেশে দৃশ্যমান উন্নয়ন অনেক কিন্তু প্রকৃত উন্নয়ন কতটা তা যতেষ্ঠ প্রশ্নের মুখোমুখি আজ। উল্টো দেশের স্তম্ভ গুলো দিনকে দিন দুর্বল থেকে দুর্বলতর করা হচ্ছে। আইন বিচার এবং শাসন বিভাগের অবস্থা বড্ড নাজুক। এখন অনেক সময় দেখি মহান জাতীয় সংসদ কোরাম সংকটে ভুগে। সাংবিধানিক প্রতিষ্ঠান গুলো তাদের স্বকীয়তা হারাচ্ছে অনবরত। বাংলাদেশ নির্বাচন কমিশন, দুর্নীতি দমন কমিশন সহ সাংবিধানিক প্রতিষ্ঠানগুলোকে এখন আর কার্যকর তেমন কোন পদক্ষেপ নিতে দেখি না আর আমরা।রাষ্ট্রের চতুর্থ স্তম্ভ গণমাধ্যম, এই গণমাধ্যমের অবস্থা যে খুব একটা ভাল তাও কিন্তু নয়। তবুও বলবো সব মিলিয়ে এগিয়ে যাচ্ছে প্রিয় স্বদেশ। আগামীতে গণমানুষের প্রত্যাশা পূরণে কাজ করে যাবে ছাত্রসমাজের প্রাণের স্পন্দন বাংলাদেশ সাধারণ ছাত্র অধিকার সংরক্ষণ পরিষদ দেশ ব্যাপি কমিটি হালনাগাদের কার্যক্রম চলমান রয়েছে, আপনি আছেন তো আপনার জেলার কমিটিতে....??যুক্ত না থাকলে এখনি সময় যুক্ত হওয়ার। আপনাদের হাত ধরেই পরিবর্তন আসবে ইনশাআল্লাহ।।\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HUvYIrLxWSAZ",
    "outputId": "43852f7b-7857-4a1a-87ca-cd867e5c1dec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting rouge\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from rouge) (1.15.0)\n",
      "Installing collected packages: rouge\n",
      "Successfully installed rouge-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "qL_JfyFyXgEP"
   },
   "outputs": [],
   "source": [
    "from rouge import Rouge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NSoXj0WkaQw-",
    "outputId": "1741e374-62f1-47bd-93ad-4263e18e88e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n"
     ]
    }
   ],
   "source": [
    "hypothesis = \"স্বাধীনতার পর থেকে চলমান অসুস্থ ধারার রাজনীতি এখনো বহাল।\"\n",
    "reference = \"স্বাধীনতার পর থেকে চলমান অসুস্থ ধারার রাজনীতি এখনো বহাল।\"\n",
    "\n",
    "\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hypothesis, reference)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96B3LVoV0DjP",
    "outputId": "ed80f4fe-77a1-478b-db23-51400d6b08ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4642857142857143\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "BLEUscore = nltk.translate.bleu_score.corpus_bleu([reference], [hypothesis], weights = [1])\n",
    "print(BLEUscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aC8wqpOeLa66"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
