{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "5KSSto1yK2AJ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import re\n",
    "import pickle\n",
    "import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "bz8nJse7KXjB",
    "outputId": "25ba6233-07c3-421c-a5db-639e649764cc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>আমি জানি আমার এই লেখা,টির জন্য আমাকে অনেক গালম...</td>\n",
       "      <td>বাংলাদেশে কোচিং বানিজ্য বন্ধ এখন সময়ের দাবি</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>একটা ভাষায় তুলনামূলক ভাবে অনেক বেশি মানুষ কথা ...</td>\n",
       "      <td>বাংলা ভাষার প্রযুক্তি নিয়ে আমাদের আরো অনেক বেশ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>আমাদের ফেব্রুয়ারি মাসটি ভাষার মাস। এর বাইরেও ত...</td>\n",
       "      <td>যদি শিশুরা বই পড়ার অভ্যাস করে তাহলে সারা জীবনে...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>আমাকে যদি কেউ কখনো জিজ্ঞেস করে বাংলাদেশের সবচে...</td>\n",
       "      <td>বাংলাদেশে সব স্তরে নারীর ক্ষমতায়নের জন্য আরও অ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>মানুষের মুখ খুব শক্তিশালী এক জিনিস। মানুষ যেটা...</td>\n",
       "      <td>ভালো কথা বল, নয়ত চুপ থাকো</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  আমি জানি আমার এই লেখা,টির জন্য আমাকে অনেক গালম...   \n",
       "1  একটা ভাষায় তুলনামূলক ভাবে অনেক বেশি মানুষ কথা ...   \n",
       "2  আমাদের ফেব্রুয়ারি মাসটি ভাষার মাস। এর বাইরেও ত...   \n",
       "3  আমাকে যদি কেউ কখনো জিজ্ঞেস করে বাংলাদেশের সবচে...   \n",
       "4  মানুষের মুখ খুব শক্তিশালী এক জিনিস। মানুষ যেটা...   \n",
       "\n",
       "                                             Summary  \n",
       "0        বাংলাদেশে কোচিং বানিজ্য বন্ধ এখন সময়ের দাবি  \n",
       "1  বাংলা ভাষার প্রযুক্তি নিয়ে আমাদের আরো অনেক বেশ...  \n",
       "2  যদি শিশুরা বই পড়ার অভ্যাস করে তাহলে সারা জীবনে...  \n",
       "3  বাংলাদেশে সব স্তরে নারীর ক্ষমতায়নের জন্য আরও অ...  \n",
       "4                          ভালো কথা বল, নয়ত চুপ থাকো  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_excel(\"text-summarization.xlsx\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "mPNIN8iqJNQz"
   },
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EwN5of3ZJYkz",
    "outputId": "7b936b5a-de45-4e8e-f2f4-66c136dc067c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text       0\n",
       "Summary    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df.sample(frac = 0.10)\n",
    " \n",
    "df = df.drop(df_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "0_ScD3LTK0a_"
   },
   "outputs": [],
   "source": [
    "document = df['Text']\n",
    "summary = df['Summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_document = df_test['Text']\n",
    "test_summary = df_test['Summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L0NXAlEKLRwW",
    "outputId": "40170720-a57f-429e-f3d7-965ad28f7abd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"গতরাতে আব্বু তার ফোন নিয়ে আমার কাছে এসে বললো, \"বাবা, ফেইসবুকে কোন একটা পোস্টে কিভাবে প্রাইভেসি সেট করে, একটু দেখায় দেও তো\"আমি তাকে একবার দেখালাম প্রসেসটা ... কিন্তু বয়সের কারণে খালি চোখে আব্বু ডান পাশের ছোট ডট দেখতে পাচ্ছিলো না ... বারবার বলতেছিলো, \"কই? কই ক্লিক করবো?\"প্রায় ৫ বার দেখানোর পরেও যখন আব্বু উল্টাপাল্টা ক্লিক করতেসিলো, প্রচণ্ড মেজাজ খারাপ হইলো ... আমি বেখেয়ালে একটু রাগের সুরে বললাম, \"এই সিম্পল জিনিস পারতেছো না? ধুরর\"এই কথাটা বলার সাথে সাথে আব্বুর মুখটা শুকনা হয়ে গেলো ... আমি সাথে সাথে বুঝতে পারলাম, মানুষটাকে আমি ছোট্ট একটা কথা দিয়ে অনেক বড় একটা কষ্ট দিয়ে ফেলসি ... \\'সরি\\' বলে আবার ভালোভাবে প্রসেসটা বুঝায় বললাম !!আমার ধারণা, নিজের অজান্তেই খুব ছোট ছোট কথা দিয়ে আমরা মানুষকে আঘাত করে ফেলি ... পৃথিবীতে সবাই সবকিছু পারে না ... কারো না পারা নিয়ে তার সাথে অপমানের সুরে কথা বললে সে নিজের ভেতর খুব ছোট বোধ করে ... আমার কোন অধিকার নেই কাউকে অপমান করার, কাউকে ছোট করার !!যে বাবার প্রতি আমি বিরক্ত হলাম কারণ সে সামান্য ফোন চালাতে পারছে না, সেই বাবাই ছোট বেলায় কখনো বিরক্ত হয় নি যখন আমি সামান্য হাঁটতেও পারতাম না ... দিনের পর দিন হাতে ধরে ধরে ক্লান্তিহীনভাবে শিখিয়ে গেছে ... তাকে ৫ বার বা ১০ বার ফোন চালানো শিখাতে আমার বিরক্তি আসবে কেন?নিজেকে অন্য মানুষটার জায়গায় কল্পনা করলে বুঝা যায়, কেউ আমার অপারগতা নিয়ে তাচ্ছিল্যের সুরে কথা বললে কতটা কষ্ট লাগে ... হয়তো কোন একটা ব্যাপার আমার আসলেই পারা উচিত, জানা উচিত, কিন্তু আমি পারি না অথবা জানি না ... এই ব্যর্থতা বা অপারগতার জন্য বোধহয় আমি অপমানিত হওয়া ডিজার্ভ করি না !!এই ক্ষুদ্র জীবনে ইচ্ছাকৃত কিংবা অনিচ্ছাকৃতভাবে আমার বলা কথায় যারা কষ্ট পেয়েছেন, তাদের কাছে আমি ক্ষমাপ্রার্থী ... আমি চেষ্টা করবো কখনোই কাউকে তার অজ্ঞতার জন্য ছোট না করতে !!কোন একটা বিষয় না জানাটা কোন অপরাধ না ... কিন্তু সেটা না জানার জন্য কাউকে তাচ্ছিল্য করাটা অবশ্যই অপরাধ !!আমার ধারণা, আমাদের আশেপাশে এমন কিছু মানুষ আছে যারা কথায় কথায় আমাদের ছোট করে, এই মানুষগুলোর আচরণের জন্য আমরা নিজের ভেতর ডিপ্রেসন তৈরি করে ফেলি ... যে মানুষগুলো প্রতিনিয়ত আমার অজ্ঞতা কিংবা অপারগতা নিয়ে অপমানের সুরে কথা বলে, তাদের থেকে দূরে থাকাই ভালো ... অন্যের কথার কারণে নিজের ভেতরে অপমানবোধের যে অনুভূতিটা জন্মায়, তার চেয়ে ভয়ংকর তীব্র অনুভূতি আর একটাওনেই !!',\n",
       " 'সবকিছুর সাথে আপোষ করা যায়,কিন্তু নিজের আত্মসম্মানকে যে আঘাত করে,তার সাথে আপোষ করার প্রশ্নই আসেনা ')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document[30], summary[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vanMVt32tQ1y",
    "outputId": "5981db53-236a-47ba-9acf-9b078a28ffd5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "921"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "SN34pX5Xtnl6"
   },
   "outputs": [],
   "source": [
    "c1 = dict(df.Text.str.split(expand=True).stack().value_counts())\n",
    "c1 = dict(sorted(c1.items(), key=lambda x: x[1], reverse=True))\n",
    "c2 = dict(df.Summary.str.split(expand=True).stack().value_counts())\n",
    "c2 = dict(sorted(c2.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6FagwZYt5yI",
    "outputId": "0bae1039-d68b-46b2-cc9e-049ffbcc88cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'না': 138,\n",
       " 'মানুষ': 66,\n",
       " 'করে': 60,\n",
       " 'জন্য': 59,\n",
       " 'ভালোবাসা': 54,\n",
       " 'না।': 50,\n",
       " 'করা': 44,\n",
       " 'অনেক': 44,\n",
       " 'মানুষের': 42,\n",
       " 'আর': 41,\n",
       " 'ভালো': 40,\n",
       " 'করতে': 40,\n",
       " 'সব': 39,\n",
       " 'হবে': 39,\n",
       " 'হয়': 38,\n",
       " 'যে': 35,\n",
       " 'সাথে': 33,\n",
       " 'কিছু': 33,\n",
       " 'নিজের': 31,\n",
       " 'ভালোবাসার': 30,\n",
       " 'এই': 30,\n",
       " 'এর': 29,\n",
       " 'সময়': 28,\n",
       " 'হতে': 28,\n",
       " 'কষ্ট': 27,\n",
       " 'ও': 26,\n",
       " 'করার': 26,\n",
       " 'যায়': 26,\n",
       " 'কথা': 26,\n",
       " 'উচিত': 25,\n",
       " 'থেকে': 25,\n",
       " 'একটি': 24,\n",
       " 'হয়ে': 23,\n",
       " ',': 23,\n",
       " 'পারে': 23,\n",
       " 'আমাদের': 23,\n",
       " 'তার': 23,\n",
       " 'কোনো': 22,\n",
       " 'কারো': 22,\n",
       " 'আমরা': 21,\n",
       " 'প্রতি': 21,\n",
       " 'থাকে': 21,\n",
       " 'নেই': 21,\n",
       " 'এক': 20,\n",
       " 'প্রেম': 20,\n",
       " '।': 20,\n",
       " 'উপর': 19,\n",
       " 'একজন': 19,\n",
       " 'হয়।': 18,\n",
       " 'কেউ': 18,\n",
       " 'মানুষকে': 18,\n",
       " 'যায়।': 18,\n",
       " 'জীবন': 17,\n",
       " 'কাউকে': 17,\n",
       " 'কখনো': 17,\n",
       " 'তাকে': 17,\n",
       " 'হবে।': 17,\n",
       " 'এখন': 17,\n",
       " 'এবং': 17,\n",
       " 'নিজেকে': 17,\n",
       " 'একটা': 16,\n",
       " 'নিয়ে': 16,\n",
       " 'এমন': 16,\n",
       " 'সে': 16,\n",
       " 'থাকা': 16,\n",
       " 'চলে': 15,\n",
       " 'মন': 15,\n",
       " 'মানে': 14,\n",
       " 'শুধু': 14,\n",
       " 'করোনা': 14,\n",
       " 'যাবে': 14,\n",
       " 'মতো': 14,\n",
       " 'হচ্ছে': 14,\n",
       " 'বেশি': 14,\n",
       " 'আছে': 13,\n",
       " 'মাঝে': 13,\n",
       " 'মধ্যে': 13,\n",
       " 'সম্মান': 13,\n",
       " 'জীবনে': 13,\n",
       " 'গুলো': 13,\n",
       " 'তা': 13,\n",
       " 'আগে': 13,\n",
       " 'যা': 13,\n",
       " 'সবার': 13,\n",
       " 'নেই।': 13,\n",
       " 'দিতে': 12,\n",
       " 'দরকার': 12,\n",
       " 'বলতে': 12,\n",
       " 'টাকা': 12,\n",
       " 'নয়': 12,\n",
       " 'থাকতে': 12,\n",
       " 'বা': 12,\n",
       " 'খারাপ': 12,\n",
       " 'সেই': 12,\n",
       " 'দিয়ে': 12,\n",
       " 'উচিত।': 12,\n",
       " 'হলে': 12,\n",
       " 'জীবনের': 12,\n",
       " 'রাখতে': 12,\n",
       " 'কাছে': 12,\n",
       " 'না,': 12,\n",
       " 'মনে': 11,\n",
       " 'করে।': 11,\n",
       " 'ভালোবাসতে': 11,\n",
       " 'হওয়া': 11,\n",
       " 'নারীর': 11,\n",
       " 'দিয়ে': 11,\n",
       " 'সম্পর্ক': 11,\n",
       " 'বলে': 11,\n",
       " 'কিছুই': 10,\n",
       " 'চেষ্টা': 10,\n",
       " 'কি': 10,\n",
       " 'সবাই': 10,\n",
       " 'আপনি': 10,\n",
       " 'নাই।': 10,\n",
       " 'নিতে': 10,\n",
       " 'মেনে': 10,\n",
       " 'রাখা': 10,\n",
       " 'যার': 9,\n",
       " 'কে': 9,\n",
       " 'চায়': 9,\n",
       " 'নিজে': 9,\n",
       " 'যত': 9,\n",
       " 'প্রয়োজন': 9,\n",
       " 'ইচ্ছা': 9,\n",
       " 'শেষ': 9,\n",
       " 'খুব': 9,\n",
       " 'কারণ': 9,\n",
       " 'দেয়া': 9,\n",
       " 'মনের': 9,\n",
       " 'নারীরা': 9,\n",
       " 'কাজ': 9,\n",
       " 'কষ্টের': 9,\n",
       " 'বড়': 9,\n",
       " 'মানেই': 9,\n",
       " 'পর': 9,\n",
       " 'নতুন': 8,\n",
       " 'থাকার': 8,\n",
       " 'পেতে': 8,\n",
       " 'সম্পর্কে': 8,\n",
       " 'কোন': 8,\n",
       " 'আমি': 8,\n",
       " 'তাই': 8,\n",
       " 'অন্যের': 8,\n",
       " 'ভালো।': 8,\n",
       " 'নষ্ট': 8,\n",
       " 'পাওয়া': 8,\n",
       " 'গুরুত্ব': 8,\n",
       " 'বাংলাদেশে': 8,\n",
       " 'দরকার।': 8,\n",
       " 'থাকবে': 8,\n",
       " 'বই': 8,\n",
       " 'মূল্য': 8,\n",
       " 'তাদের': 8,\n",
       " 'সত্যি': 7,\n",
       " 'নাম': 7,\n",
       " 'পাশে': 7,\n",
       " 'ঠিক': 7,\n",
       " 'আমার': 7,\n",
       " 'সত্যিকারের': 7,\n",
       " 'নারী': 7,\n",
       " 'দিন': 7,\n",
       " 'চেয়ে': 7,\n",
       " 'সময়ের': 7,\n",
       " 'ভাইরাস': 7,\n",
       " 'বড়': 7,\n",
       " 'বদলে': 7,\n",
       " 'দিনশেষে': 7,\n",
       " 'বন্ধু': 7,\n",
       " 'যদি': 7,\n",
       " 'বলা': 7,\n",
       " 'সৌন্দর্য': 7,\n",
       " 'নিয়ন্ত্রণ': 7,\n",
       " 'গেলে': 7,\n",
       " 'ভারত': 7,\n",
       " 'ছেলে': 7,\n",
       " 'সবচেয়ে': 7,\n",
       " 'থাকলে': 7,\n",
       " 'একই': 6,\n",
       " 'কিছুর': 6,\n",
       " 'ধর্ম': 6,\n",
       " 'বিশ্বাস': 6,\n",
       " 'আপনার': 6,\n",
       " 'চেয়ে': 6,\n",
       " 'ছেড়ে': 6,\n",
       " 'মিথ্যা': 6,\n",
       " 'চাওয়া': 6,\n",
       " 'দেয়': 6,\n",
       " 'খুশি': 6,\n",
       " 'আসবে': 6,\n",
       " 'মেয়েদের': 6,\n",
       " 'করো': 6,\n",
       " 'দেশের': 6,\n",
       " 'দেশে': 6,\n",
       " 'কিন্তু': 6,\n",
       " 'প্রতিটি': 6,\n",
       " 'যেমন': 6,\n",
       " 'লেখক': 6,\n",
       " 'লেখকের': 6,\n",
       " 'প্রকাশ': 6,\n",
       " 'ভুল': 6,\n",
       " 'লাগে': 6,\n",
       " 'দেখা': 6,\n",
       " 'কখনও': 6,\n",
       " 'আত্মসম্মান': 6,\n",
       " 'সুখ': 6,\n",
       " 'দোষ': 5,\n",
       " 'দাম': 5,\n",
       " 'পৃথিবীতে': 5,\n",
       " 'আছে।': 5,\n",
       " 'প্রকৃত': 5,\n",
       " 'দায়িত্ব': 5,\n",
       " 'বুঝে': 5,\n",
       " 'দুই': 5,\n",
       " 'আটকে': 5,\n",
       " 'তুমি': 5,\n",
       " 'অল্পতেই': 5,\n",
       " 'বাংলাদেশ': 5,\n",
       " 'তখন': 5,\n",
       " 'চোখে': 5,\n",
       " 'বাংলা': 5,\n",
       " 'সুন্দর': 5,\n",
       " 'জিনিস': 5,\n",
       " 'হওয়ার': 5,\n",
       " 'নিজেদের': 5,\n",
       " 'দেখে': 5,\n",
       " 'কবি': 5,\n",
       " 'তো': 5,\n",
       " 'যখন': 5,\n",
       " 'দেওয়ার': 5,\n",
       " 'ক্ষতি': 5,\n",
       " 'ছাড়া': 5,\n",
       " 'থাকে।': 5,\n",
       " 'পরিবর্তন': 5,\n",
       " 'নয়।': 5,\n",
       " 'দেশ': 5,\n",
       " 'আরো': 5,\n",
       " 'চাইতে': 5,\n",
       " 'পারে।': 5,\n",
       " 'যাওয়ার': 5,\n",
       " 'জোর': 5,\n",
       " 'সুখী': 5,\n",
       " 'পুরুষের': 5,\n",
       " 'যারা': 5,\n",
       " 'হয়।': 5,\n",
       " 'থাকাটা': 5,\n",
       " 'ফেলে': 5,\n",
       " 'ভালোবাসায়': 5,\n",
       " 'খুঁজে': 5,\n",
       " 'প্রশ্ন': 5,\n",
       " 'সঠিক': 4,\n",
       " 'ছোটখাটো': 4,\n",
       " 'মাধ্যমে': 4,\n",
       " 'করেছে': 4,\n",
       " 'পাবে': 4,\n",
       " 'দেখানো': 4,\n",
       " 'সমাজের': 4,\n",
       " 'আসে': 4,\n",
       " 'হয়ে': 4,\n",
       " 'সুযোগ': 4,\n",
       " 'করতেন।': 4,\n",
       " 'বছরের': 4,\n",
       " 'সবচাইতে': 4,\n",
       " 'সবারই': 4,\n",
       " 'যথেষ্ট': 4,\n",
       " 'এ': 4,\n",
       " 'যাবে।': 4,\n",
       " 'সমাজে': 4,\n",
       " 'হলেও': 4,\n",
       " 'পবিত্র': 4,\n",
       " 'আক্রান্ত': 4,\n",
       " 'শিক্ষা': 4,\n",
       " 'যাই': 4,\n",
       " 'আমাকে': 4,\n",
       " 'তোমার': 4,\n",
       " 'ভুলের': 4,\n",
       " 'স্মৃতি': 4,\n",
       " 'বড়ই': 4,\n",
       " 'মজুদ': 4,\n",
       " 'বিভিন্ন': 4,\n",
       " 'তেমনি': 4,\n",
       " 'যেতে': 4,\n",
       " 'চুরি': 4,\n",
       " 'করুন': 4,\n",
       " 'পাওয়ার': 4,\n",
       " 'ধরে': 4,\n",
       " 'আছে,': 4,\n",
       " 'মহৎ': 4,\n",
       " 'করি।': 4,\n",
       " 'গিয়ে': 4,\n",
       " 'দূরে': 4,\n",
       " 'আবার': 4,\n",
       " 'ইচ্ছে': 4,\n",
       " 'আপনাকে': 4,\n",
       " 'মেয়েরা': 4,\n",
       " 'তোমাকে': 4,\n",
       " 'থেমে': 4,\n",
       " 'ছোট': 4,\n",
       " 'অধিকার': 4,\n",
       " 'আবেগ': 4,\n",
       " 'পায়': 4,\n",
       " 'বয়স': 4,\n",
       " 'দেয়ার': 4,\n",
       " 'ভাষার': 4,\n",
       " 'সময়': 4,\n",
       " 'গ্রহ': 4,\n",
       " 'পরিচয়': 4,\n",
       " 'আলো': 4,\n",
       " 'হার': 4,\n",
       " 'বসে': 4,\n",
       " 'কাজের': 4,\n",
       " 'ভালোবাসে': 4,\n",
       " 'অন্যকে': 4,\n",
       " 'চোখের': 4,\n",
       " 'গুলা': 4,\n",
       " 'সারাজীবন': 4,\n",
       " 'চাই।': 4,\n",
       " 'বলার': 4,\n",
       " 'যায়।': 4,\n",
       " 'ভুলে': 4,\n",
       " 'লোক': 4,\n",
       " 'প্রকৃতির': 4,\n",
       " 'সচেতন': 4,\n",
       " 'এখনো': 4,\n",
       " 'পড়া': 4,\n",
       " 'রাখার': 4,\n",
       " 'জরুরী।': 4,\n",
       " 'যুদ্ধ': 4,\n",
       " 'ভালোবাসা।': 4,\n",
       " 'অবস্থায়': 4,\n",
       " 'গল্প': 4,\n",
       " 'আল্লাহ': 3,\n",
       " 'মাস্ক': 3,\n",
       " 'প্রাপ্য': 3,\n",
       " 'বেস্ট': 3,\n",
       " 'উচিৎ।': 3,\n",
       " 'তত': 3,\n",
       " 'জানে।': 3,\n",
       " 'প্রয়োজন।': 3,\n",
       " 'মানুষেরই': 3,\n",
       " 'হিসাব': 3,\n",
       " 'সেটা': 3,\n",
       " 'সামনে': 3,\n",
       " 'ভাইরাসে': 3,\n",
       " 'বোঝাতে': 3,\n",
       " 'জানতে': 3,\n",
       " 'একে': 3,\n",
       " 'সফলতা': 3,\n",
       " 'করা।': 3,\n",
       " 'বন্ধ': 3,\n",
       " 'ভাইরাসের': 3,\n",
       " 'তবে': 3,\n",
       " 'লুকিয়ে': 3,\n",
       " 'বিচার': 3,\n",
       " 'লাগে।': 3,\n",
       " 'চিন্তা': 3,\n",
       " 'স্বাধীনতা': 3,\n",
       " 'অপেক্ষা': 3,\n",
       " 'পড়ার': 3,\n",
       " 'সারারাত': 3,\n",
       " 'যায়,': 3,\n",
       " 'জেগে': 3,\n",
       " 'মিষ্টি': 3,\n",
       " 'প্রস্তুতি': 3,\n",
       " 'অনুভূতি': 3,\n",
       " 'বিজ্ঞান': 3,\n",
       " 'মূল': 3,\n",
       " 'ঘৃণা': 3,\n",
       " 'পথে': 3,\n",
       " 'ঘুম': 3,\n",
       " 'কবিকে': 3,\n",
       " 'স্ট্রিং': 3,\n",
       " 'স্কুল': 3,\n",
       " 'নেওয়া': 3,\n",
       " 'হলো': 3,\n",
       " 'পড়ে': 3,\n",
       " 'সম্ভব': 3,\n",
       " 'আলোর': 3,\n",
       " 'সাহায্য': 3,\n",
       " 'করবেন': 3,\n",
       " 'গুলোর': 3,\n",
       " 'হিসাবে': 3,\n",
       " 'আসল।': 3,\n",
       " 'সকল': 3,\n",
       " 'দৃষ্টিভঙ্গি': 3,\n",
       " 'কিশোর': 3,\n",
       " 'সকলের': 3,\n",
       " 'সত্য': 3,\n",
       " 'আলাদা।': 3,\n",
       " 'শক্ত': 3,\n",
       " 'মর্ম': 3,\n",
       " 'বাস্তবতা': 3,\n",
       " 'মায়ের': 3,\n",
       " 'এসে': 3,\n",
       " 'আসার': 3,\n",
       " 'সংসার': 3,\n",
       " 'যথেষ্ট।': 3,\n",
       " 'যাওয়া': 3,\n",
       " 'জিনিসের': 3,\n",
       " 'অনেক।': 3,\n",
       " 'পৃথিবীর': 3,\n",
       " 'খুবই': 3,\n",
       " 'এখনও': 3,\n",
       " 'বলে,': 3,\n",
       " 'পানি': 3,\n",
       " 'সৃষ্টি': 3,\n",
       " 'ভাল': 3,\n",
       " 'বঙ্গবন্ধুর': 3,\n",
       " 'পারফেক্ট': 3,\n",
       " 'সিনেমা': 3,\n",
       " 'নেয়া': 3,\n",
       " 'ভীষণ': 3,\n",
       " 'পড়াশোনা': 3,\n",
       " 'কখন': 3,\n",
       " 'তুচ্ছ': 3,\n",
       " 'পারলে': 3,\n",
       " 'দুনিয়াতে': 3,\n",
       " 'ফল': 3,\n",
       " 'পরে': 3,\n",
       " 'নারীকে': 3,\n",
       " 'দিয়েছে।': 3,\n",
       " 'লাগার': 3,\n",
       " 'মিথ্যে': 3,\n",
       " 'উপর।': 3,\n",
       " 'মেধা': 3,\n",
       " 'হয়েছে।': 3,\n",
       " 'শাস্তি': 3,\n",
       " 'পেলে': 3,\n",
       " 'হাতে': 3,\n",
       " 'ক্ষমতা': 3,\n",
       " 'সবাইকে': 3,\n",
       " 'যুদ্ধে': 3,\n",
       " 'দেয়।': 3,\n",
       " 'উপায়': 3,\n",
       " 'হবার': 3,\n",
       " 'রাখা।': 3,\n",
       " 'ভরসা': 3,\n",
       " 'চুপচাপ': 3,\n",
       " 'বাংলাদেশের': 3,\n",
       " 'খরচ': 3,\n",
       " 'তে': 3,\n",
       " 'ভালোবেসে': 3,\n",
       " 'ব্ল্যাক': 3,\n",
       " 'উপহার': 3,\n",
       " 'পুরাতন': 3,\n",
       " 'ধারনা': 3,\n",
       " 'হবে,': 3,\n",
       " 'যাকে': 3,\n",
       " 'সারাদিন': 3,\n",
       " 'অতিরিক্ত': 3,\n",
       " 'হোক।': 3,\n",
       " 'পাওয়া': 3,\n",
       " 'আসল': 3,\n",
       " 'থাকুক': 3,\n",
       " 'যায়': 3,\n",
       " 'পূরণ': 3,\n",
       " 'আত্মবিশ্বাস': 3,\n",
       " 'পেয়ে': 3,\n",
       " 'পিছনে': 3,\n",
       " 'পাই': 3,\n",
       " 'অভাব': 3,\n",
       " 'করেই': 3,\n",
       " 'মিস': 3,\n",
       " 'থেকেই': 3,\n",
       " 'ক্রিকেট': 3,\n",
       " 'একদিন': 3,\n",
       " 'দুঃখ': 3,\n",
       " 'কথার': 3,\n",
       " 'করলে': 3,\n",
       " 'লাভ': 3,\n",
       " 'কঠিন।': 3,\n",
       " 'মেয়েরা': 3,\n",
       " 'টা': 3,\n",
       " 'বিয়া': 3,\n",
       " 'অর্থের': 3,\n",
       " 'ঢাকা': 3,\n",
       " 'বাক': 3,\n",
       " 'কিট': 3,\n",
       " 'অতি': 3,\n",
       " 'পার্থক্য': 3,\n",
       " 'টেস্ট': 3,\n",
       " 'কেন্দ্র': 3,\n",
       " 'সমান': 3,\n",
       " 'স্বাধীনতার': 3,\n",
       " 'শক্তি': 3,\n",
       " 'কাজে': 3,\n",
       " 'হল': 3,\n",
       " 'নির্ভর': 3,\n",
       " 'বেপারে': 3,\n",
       " 'ঘুমাতে': 2,\n",
       " 'প্রত্যেক': 2,\n",
       " 'বয়সে': 2,\n",
       " 'রাগ': 2,\n",
       " 'অসাধারণ': 2,\n",
       " 'মানুষই': 2,\n",
       " 'দিবে': 2,\n",
       " 'সবচেয়ে': 2,\n",
       " 'ইংরেজি': 2,\n",
       " 'ভাষা': 2,\n",
       " 'রাখে।': 2,\n",
       " 'জানানো': 2,\n",
       " 'নিদৃষ্ট': 2,\n",
       " 'একসাথে': 2,\n",
       " 'আগলে': 2,\n",
       " 'নয়।': 2,\n",
       " 'অন্য': 2,\n",
       " 'বিরত': 2,\n",
       " 'এনে': 2,\n",
       " 'স্বপ্ন': 2,\n",
       " 'অন্তরে': 2,\n",
       " 'দিন।': 2,\n",
       " 'কাহিনী।': 2,\n",
       " 'দিয়েছে': 2,\n",
       " 'লিখে': 2,\n",
       " 'ঘটে': 2,\n",
       " 'ধরণের': 2,\n",
       " 'কাউকেও': 2,\n",
       " 'বুঝতে': 2,\n",
       " 'পারা': 2,\n",
       " 'যাওয়া': 2,\n",
       " 'অভিনয়': 2,\n",
       " 'মানুষগুলোর': 2,\n",
       " 'নিজেকেই': 2,\n",
       " 'ছিল': 2,\n",
       " 'কার': 2,\n",
       " 'আকাশ': 2,\n",
       " 'অপমান': 2,\n",
       " 'মায়া': 2,\n",
       " 'শিখুন।': 2,\n",
       " 'যাচ্ছি।': 2,\n",
       " 'মধ্যবিত্ত': 2,\n",
       " 'দিনে': 2,\n",
       " 'হারিয়ে': 2,\n",
       " 'ছেলের': 2,\n",
       " 'লজ্জা': 2,\n",
       " 'মেয়ের': 2,\n",
       " 'মায়ায়': 2,\n",
       " 'শ্রদ্ধা।': 2,\n",
       " 'আজ': 2,\n",
       " 'কথা।': 2,\n",
       " 'সেরা': 2,\n",
       " 'বললে': 2,\n",
       " 'দেওয়া': 2,\n",
       " 'মানুষটির': 2,\n",
       " 'অতীত': 2,\n",
       " 'বেশি।': 2,\n",
       " 'জেদ': 2,\n",
       " 'সোজা': 2,\n",
       " 'একদিন।': 2,\n",
       " 'চেয়েও': 2,\n",
       " 'অল্প': 2,\n",
       " 'ধ্বংস': 2,\n",
       " 'দিয়া': 2,\n",
       " 'প্রযুক্তির': 2,\n",
       " 'যুগে': 2,\n",
       " 'নাটকের': 2,\n",
       " 'ইমোশন': 2,\n",
       " 'নির্ভরশীল': 2,\n",
       " 'অভিমানের': 2,\n",
       " 'চাবে': 2,\n",
       " ',এর': 2,\n",
       " 'ভাঙ্গার': 2,\n",
       " 'চাওয়া।': 2,\n",
       " 'রেখেই': 2,\n",
       " 'ভালোবাসি': 2,\n",
       " 'কোঠাটান': 2,\n",
       " 'ছাড়াও': 2,\n",
       " 'মোকাবিলায়': 2,\n",
       " '৩': 2,\n",
       " 'মার্চ': 2,\n",
       " 'তাৎপর্য': 2,\n",
       " 'পাবনা': 2,\n",
       " 'জেনেও': 2,\n",
       " 'যাওয়া।': 2,\n",
       " 'জন্মতোই': 2,\n",
       " 'চিনে।': 2,\n",
       " 'কাকে': 2,\n",
       " 'শোনার': 2,\n",
       " 'করানো': 2,\n",
       " 'নাই': 2,\n",
       " '৭': 2,\n",
       " 'মার্চের': 2,\n",
       " 'ভাষণ।': 2,\n",
       " 'ভিতরের': 2,\n",
       " 'হওয়া': 2,\n",
       " 'মনুষ্যত্বহীন': 2,\n",
       " 'ধর্যের': 2,\n",
       " 'করেও': 2,\n",
       " 'গভীরতা': 2,\n",
       " 'স্যরি': 2,\n",
       " 'রাখুন।': 2,\n",
       " 'পাশের': 2,\n",
       " 'বাড়ির': 2,\n",
       " 'রেজাল্ট': 2,\n",
       " 'বিয়ের': 2,\n",
       " 'রোমান্টিক': 2,\n",
       " 'অভিযোগ।': 2,\n",
       " 'পাল্টায়': 2,\n",
       " 'স্ট্যাটাস': 2,\n",
       " 'ছেলেদের': 2,\n",
       " 'মেয়েদের': 2,\n",
       " 'পদার্থের': 2,\n",
       " 'কাল': 2,\n",
       " 'ইভেন': 2,\n",
       " 'অর্থ': 2,\n",
       " 'ব্যাবহার': 2,\n",
       " 'বৃষ্টির': 2,\n",
       " 'বহুবচন': 2,\n",
       " 'যোগ': 2,\n",
       " 'ঘরে': 2,\n",
       " 'বানানো': 2,\n",
       " 'হিসেবে': 2,\n",
       " 'নির্দেশ': 2,\n",
       " 'অবস্থান': 2,\n",
       " 'কম্পনের': 2,\n",
       " 'কুলখানি': 2,\n",
       " 'প্রোটন': 2,\n",
       " 'কণিকা': 2,\n",
       " 'গুলোকে': 2,\n",
       " 'আবেগের': 2,\n",
       " 'প্রতিষ্ঠিত': 2,\n",
       " 'মাধ্যমের': 2,\n",
       " 'সূর্য': 2,\n",
       " 'অসুস্থ': 2,\n",
       " 'ছায়াপথ।': 2,\n",
       " 'সম্মানী': 2,\n",
       " 'কখনোই': 2,\n",
       " 'করুন।': 2,\n",
       " 'মূলত': 2,\n",
       " 'যাই।': 2,\n",
       " 'মৃত্যুর': 2,\n",
       " 'বেঁচে': 2,\n",
       " 'প্রয়োজনে': 2,\n",
       " 'ম্যাজিস্ট্রেটদের': 2,\n",
       " 'বিচারিক': 2,\n",
       " 'দেখতে': 2,\n",
       " 'রাষ্ট্র': 2,\n",
       " 'টেলিস্কোপের': 2,\n",
       " 'এটি': 2,\n",
       " 'মধ্য': 2,\n",
       " 'শক্তির': 2,\n",
       " 'বাঙালি': 2,\n",
       " 'জাতির': 2,\n",
       " 'একা': 2,\n",
       " 'দেহ': 2,\n",
       " 'কেন': 2,\n",
       " 'মজাই': 2,\n",
       " 'চিঠি': 2,\n",
       " 'হয়েছে': 2,\n",
       " 'পড়াশুনা': 2,\n",
       " 'অন্তরায়': 2,\n",
       " 'বিশ্ববিদ্যালয়ের': 2,\n",
       " 'ঘৃনা': 2,\n",
       " 'সংখ্যা': 2,\n",
       " 'মেরে': 2,\n",
       " 'লেখা': 2,\n",
       " 'নেয়ার': 2,\n",
       " 'ইতিহাস': 2,\n",
       " 'করোনায়': 2,\n",
       " 'যাওয়ায়': 2,\n",
       " 'জাগরণের': 2,\n",
       " 'জাতি': 2,\n",
       " 'বিভ্রান্ত': 2,\n",
       " 'অনেকেই': 2,\n",
       " 'আব্দুল': 2,\n",
       " 'হামিদ': 2,\n",
       " 'সনাক্তকরনের': 2,\n",
       " 'বয়সের': 2,\n",
       " 'চাঞ্চল্য': 2,\n",
       " 'কিছুকে': 2,\n",
       " 'প্রধান': 2,\n",
       " 'নারীরাই': 2,\n",
       " 'ভেজাল': 2,\n",
       " 'কাজই': 2,\n",
       " 'নির্দিষ্ট': 2,\n",
       " 'দিবস': 2,\n",
       " 'ব্যক্তিত্ব': 2,\n",
       " 'রক্ষা': 2,\n",
       " 'খুন': 2,\n",
       " 'প্রিয়': 2,\n",
       " 'গুছিয়ে': 2,\n",
       " 'ছড়িয়ে': 2,\n",
       " 'যেকোনো': 2,\n",
       " 'ভাবে': 2,\n",
       " 'মানিয়ে': 2,\n",
       " 'সাদা': 2,\n",
       " 'ফ্রেন্ড': 2,\n",
       " 'পরিপূর্ণভাবে': 2,\n",
       " 'চায়।': 2,\n",
       " 'যত্ন': 2,\n",
       " 'মানা': 2,\n",
       " 'গেছে।': 2,\n",
       " 'খেলায়': 2,\n",
       " 'মৃত্যু': 2,\n",
       " 'মানায়': 2,\n",
       " 'করি': 2,\n",
       " 'বিশ্বের': 2,\n",
       " 'ছুটে': 2,\n",
       " '\"প্রাক্তন\"': 2,\n",
       " 'করিনি': 2,\n",
       " 'ভালোবেসেছি।': 2,\n",
       " 'ভুলার': 2,\n",
       " 'এমনেই': 2,\n",
       " 'যাবেন।': 2,\n",
       " 'অতিমুনাফার': 2,\n",
       " 'হ্যান্ড': 2,\n",
       " 'স্যানিটাইজার': 2,\n",
       " 'সাবান': 2,\n",
       " 'রাখি।': 2,\n",
       " 'হাত': 2,\n",
       " 'পরিষ্কার': 2,\n",
       " 'নারীদের': 2,\n",
       " 'জাজমেন্ট': 2,\n",
       " 'সময়কে': 2,\n",
       " 'লাগান।': 2,\n",
       " 'চাইলেই': 2,\n",
       " 'পারি।': 2,\n",
       " 'দিনই।': 2,\n",
       " 'সময়টা': 2,\n",
       " 'কালের': 2,\n",
       " 'উপভোগ': 2,\n",
       " 'নবাবপুর': 2,\n",
       " 'শব্দটা': 2,\n",
       " 'নানা': 2,\n",
       " 'ব্যস্ত': 2,\n",
       " 'ভেতরে': 2,\n",
       " 'গুজব': 2,\n",
       " 'উপসর্গ': 2,\n",
       " 'জাতীয়': 2,\n",
       " 'পাশাপাশি': 2,\n",
       " 'পারছি': 2,\n",
       " 'বসবাস': 2,\n",
       " 'আগামী': 2,\n",
       " 'শক্তিশালী': 2,\n",
       " 'জরুরি।': 2,\n",
       " 'বছর': 2,\n",
       " 'চাইলে': 2,\n",
       " 'ফেরানো': 2,\n",
       " 'দিক': 2,\n",
       " 'ব্যাপার।': 2,\n",
       " 'এক্সদের': 2,\n",
       " 'বোকামি': 2,\n",
       " '.': 2,\n",
       " 'আসতে': 2,\n",
       " 'গল্প।': 2,\n",
       " 'ধরনের': 2,\n",
       " 'সৈয়দ': 2,\n",
       " 'সাহিত্য': 2,\n",
       " 'অপরিসীম': 2,\n",
       " 'পৃথিবী': 2,\n",
       " 'কেও': 2,\n",
       " 'পাবেন': 2,\n",
       " 'দিয়েও': 2,\n",
       " 'মাথায়': 2,\n",
       " 'নামে': 2,\n",
       " 'নেওয়ার': 2,\n",
       " 'সেবা': 2,\n",
       " 'কোনও': 2,\n",
       " 'বাঁচতে': 2,\n",
       " 'বৃদ্ধি': 2,\n",
       " 'পায়।': 2,\n",
       " 'প্রভাব': 2,\n",
       " 'বাবা': 2,\n",
       " 'চলার': 2,\n",
       " 'উঠতে': 2,\n",
       " 'মানুষগুলো': 2,\n",
       " 'বিয়ে': 2,\n",
       " 'পারার': 2,\n",
       " 'কাছের': 2,\n",
       " 'অপ্রত্যাশিত': 2,\n",
       " 'হক': 2,\n",
       " 'গুন': 2,\n",
       " 'ততো': 2,\n",
       " 'মাকে': 2,\n",
       " 'পাল্টে': 2,\n",
       " 'দিকে': 2,\n",
       " 'ই': 2,\n",
       " 'হাজার': 2,\n",
       " 'লেখার': 2,\n",
       " 'পুরোনো': 2,\n",
       " 'আনন্দ': 2,\n",
       " 'চোরের': 2,\n",
       " 'ধরা': 2,\n",
       " 'তারা': 2,\n",
       " 'ভাগ্য': 2,\n",
       " 'কি?': 2,\n",
       " 'রুখে': 2,\n",
       " 'করবে।': 2,\n",
       " 'বিরুদ্ধে': 2,\n",
       " 'নিজেরই': 2,\n",
       " 'করেছি': 2,\n",
       " 'ভেবে': 2,\n",
       " 'আকর্ষণ': 2,\n",
       " 'মুক্তি': 2,\n",
       " 'বিড়ম্বনা': 2,\n",
       " 'অপরকে': 2,\n",
       " 'ট্রাফিক': 2,\n",
       " 'ভালোবাসাকে': 2,\n",
       " 'বের': 2,\n",
       " 'একেকটা': 2,\n",
       " 'সমস্যার': 2,\n",
       " 'মানুুষ': 2,\n",
       " 'আপোষ': 2,\n",
       " 'সিদ্ধান্ত': 2,\n",
       " 'পরিশ্রম': 2,\n",
       " 'জীবনটা': 2,\n",
       " 'তারাই': 2,\n",
       " 'মেলাতে': 2,\n",
       " 'হিসেব': 2,\n",
       " 'দুর্বোধ্য': 2,\n",
       " 'অবহেলা': 2,\n",
       " 'তখনই': 2,\n",
       " 'স্বীকার': 2,\n",
       " 'উপকার': 2,\n",
       " 'মুখে': 2,\n",
       " 'সর্বনাশ': 2,\n",
       " 'ধারণ': 2,\n",
       " 'শুরু': 2,\n",
       " 'সারা': 2,\n",
       " 'তাহলে': 2,\n",
       " 'দাবি': 2,\n",
       " 'পাল্টা': 2,\n",
       " 'বিদেশ': 2,\n",
       " 'বিদেশে': 2,\n",
       " 'ব্যাস্ত': 2,\n",
       " 'ব্যবসা': 2,\n",
       " 'প্রতিনিয়ত': 2,\n",
       " 'পুরুষ': 2,\n",
       " 'পর্যন্ত': 2,\n",
       " 'এটা': 2,\n",
       " 'পুরুষদের': 2,\n",
       " 'হয়তো': 2,\n",
       " 'ডাক্তারের': 2,\n",
       " 'পারবে': 2,\n",
       " 'শহরে': 2,\n",
       " 'তেমন': 2,\n",
       " 'বিশ্ববিদ্যালয়': 2,\n",
       " 'চাই': 2,\n",
       " 'এখানেই': 2,\n",
       " 'সমস্যাটা': 2,\n",
       " 'কঠিন': 2,\n",
       " 'সয়ে': 2,\n",
       " 'পেশা': 2,\n",
       " 'মত': 2,\n",
       " 'অর্ধেক': 2,\n",
       " 'সম্পদ': 2,\n",
       " 'জ্ঞান': 2,\n",
       " 'স্বামীর': 2,\n",
       " 'অদ্ভুত': 2,\n",
       " 'চাহিদা': 2,\n",
       " 'পরীক্ষা': 2,\n",
       " 'ভাবছি': 2,\n",
       " 'নিখুঁত': 2,\n",
       " 'মেয়ে': 2,\n",
       " 'আল্লাহর': 2,\n",
       " 'গুন্ডা': 1,\n",
       " 'অভিযোগ': 1,\n",
       " 'এরা': 1,\n",
       " 'ছাত্র': 1,\n",
       " 'বহিরাগত।': 1,\n",
       " 'পেরিয়ে': 1,\n",
       " 'লোকাল': 1,\n",
       " 'বাসের': 1,\n",
       " 'দল।': 1,\n",
       " 'উপভোগে': 1,\n",
       " 'শেয়ার': 1,\n",
       " 'ক্যাসিনো': 1,\n",
       " 'তোলার': 1,\n",
       " 'বাজার': 1,\n",
       " 'দুইটাই': 1,\n",
       " 'জুয়া।': 1,\n",
       " 'প্রাইভেট': 1,\n",
       " 'থাক।': 1,\n",
       " 'নাগালে।': 1,\n",
       " 'প্রবল': 1,\n",
       " '১০': 1,\n",
       " 'কোটি': 1,\n",
       " 'সফলতা।': 1,\n",
       " 'গড়ে': 1,\n",
       " 'হাতের': 1,\n",
       " 'অমরত্ব': 1,\n",
       " 'বর্তমান': 1,\n",
       " 'আউট': 1,\n",
       " 'মুলক।': 1,\n",
       " 'হলো।': 1,\n",
       " 'রিটেক': 1,\n",
       " 'বাধ্যতা': 1,\n",
       " 'ব্যবহার': 1,\n",
       " 'পাবলিক': 1,\n",
       " 'কলিজা': 1,\n",
       " 'দাদীকে': 1,\n",
       " 'ভোগায়।': 1,\n",
       " 'নির্বাচন': 1,\n",
       " 'জ্বালিয়ে': 1,\n",
       " 'খালি': 1,\n",
       " 'নাচবো': 1,\n",
       " 'খাবো।': 1,\n",
       " 'পানামা': 1,\n",
       " 'খুশি।': 1,\n",
       " 'সবসময়': 1,\n",
       " 'মোড': 1,\n",
       " 'ক্ষমতাবান।': 1,\n",
       " 'গুলো।': 1,\n",
       " 'জীবন্ত': 1,\n",
       " 'কিংবদন্তী।': 1,\n",
       " 'গুনতে': 1,\n",
       " 'জানটা': 1,\n",
       " 'ফেইসবুকের': 1,\n",
       " 'পড়েও': 1,\n",
       " 'নোয়াখালীবাসী': 1,\n",
       " 'নিয়াও': 1,\n",
       " 'আছি।': 1,\n",
       " 'উঠা': 1,\n",
       " '\"সচেতনতা\"।': 1,\n",
       " 'সাধ্য': 1,\n",
       " 'কাবাডি': 1,\n",
       " 'দে।': 1,\n",
       " 'নম্বর': 1,\n",
       " 'আনব্লক': 1,\n",
       " 'প্রাইভেটে': 1,\n",
       " 'থাকো।': 1,\n",
       " 'বিধাতা': 1,\n",
       " 'জীবন-মৃত্যুর': 1,\n",
       " 'বিরোধী।': 1,\n",
       " 'মাঝখানে': 1,\n",
       " 'যুবকেরাই': 1,\n",
       " 'আগারওয়াল': 1,\n",
       " 'জাজ': 1,\n",
       " 'যাওয়াটা': 1,\n",
       " 'বোকামী।': 1,\n",
       " 'ঘুষ': 1,\n",
       " 'পশুর': 1,\n",
       " 'চেয়েছিলো।': 1,\n",
       " 'করানো।': 1,\n",
       " 'ভর্তি': 1,\n",
       " 'বিভাগে': 1,\n",
       " 'দা\\u200cশকে': 1,\n",
       " 'শরীর': 1,\n",
       " 'মারার': 1,\n",
       " 'মজা': 1,\n",
       " 'প্রত্যেকটা': 1,\n",
       " 'আর্তনাদ।': 1,\n",
       " 'কোথাও': 1,\n",
       " 'কমের': 1,\n",
       " 'আকুতি।': 1,\n",
       " 'খেলা': 1,\n",
       " 'কমে': 1,\n",
       " 'কল্যাণ': 1,\n",
       " 'পড়লো': 1,\n",
       " 'বেরিয়ে': 1,\n",
       " 'কর': 1,\n",
       " 'রাগেরপৃষ্ঠে': 1,\n",
       " 'করিয়াছে': 1,\n",
       " 'নর': 1,\n",
       " 'রীতি।': 1,\n",
       " 'মানবিক': 1,\n",
       " 'দৃষ্টিতে': 1,\n",
       " 'ফিরে': 1,\n",
       " 'পাটকেল': 1,\n",
       " 'তি\\u200cন্নি': 1,\n",
       " \"ড্রাইভিং'\": 1,\n",
       " 'বজায়': 1,\n",
       " 'চেনা': 1,\n",
       " 'তোরা।': 1,\n",
       " 'অসৎ': 1,\n",
       " 'পেলেই': 1,\n",
       " 'আমরাা': 1,\n",
       " 'উপায়ে': 1,\n",
       " 'বুজবে|': 1,\n",
       " 'হবেন': 1,\n",
       " 'গুনীদের': 1,\n",
       " 'অনের': 1,\n",
       " 'সৌন্দর্যটাই': 1,\n",
       " 'হ': 1,\n",
       " 'ছড়াতে': 1,\n",
       " 'ভিশনারি': 1,\n",
       " 'গণরুম': 1,\n",
       " 'মেশিন': 1,\n",
       " 'মানবতাবাদী': 1,\n",
       " 'গেটসকেই': 1,\n",
       " 'লাখ': 1,\n",
       " 'জায়ান্টদের': 1,\n",
       " 'টেক': 1,\n",
       " 'পরিমান': 1,\n",
       " 'লার্নিং': 1,\n",
       " 'অসভ্য': 1,\n",
       " 'ঘুরতে': 1,\n",
       " 'সীতাকুন্ডের': 1,\n",
       " 'ভাবিয়া': 1,\n",
       " 'বাঁচতো।': 1,\n",
       " 'গুলান': 1,\n",
       " 'রে': 1,\n",
       " 'নাটক': 1,\n",
       " 'বাসায়': 1,\n",
       " 'কোয়ারেন্টাইনে': 1,\n",
       " 'হোম': 1,\n",
       " 'ডেভেলপ': 1,\n",
       " 'জন': 1,\n",
       " 'ইমিউনিটি': 1,\n",
       " 'ভাত': 1,\n",
       " ...}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "id": "jU5Y2x-yu5Ff",
    "outputId": "f7138270-6cfb-4caa-f4c7-8f312f120b97"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>না</td>\n",
       "      <td>1370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>করে</td>\n",
       "      <td>1358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>আর</td>\n",
       "      <td>1004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>আমার</td>\n",
       "      <td>977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>আমি</td>\n",
       "      <td>963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>এই</td>\n",
       "      <td>844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>তার</td>\n",
       "      <td>797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>না।</td>\n",
       "      <td>697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>একটা</td>\n",
       "      <td>692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>যে</td>\n",
       "      <td>685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>থেকে</td>\n",
       "      <td>665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>...</td>\n",
       "      <td>589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>হয়ে</td>\n",
       "      <td>588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>সে</td>\n",
       "      <td>585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>সাথে</td>\n",
       "      <td>563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>জন্য</td>\n",
       "      <td>541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>কি</td>\n",
       "      <td>527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>করতে</td>\n",
       "      <td>482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>তোমার</td>\n",
       "      <td>479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>কথা</td>\n",
       "      <td>466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Words  Count\n",
       "0      না   1370\n",
       "1     করে   1358\n",
       "2      আর   1004\n",
       "3    আমার    977\n",
       "4     আমি    963\n",
       "5      এই    844\n",
       "6     তার    797\n",
       "7     না।    697\n",
       "8    একটা    692\n",
       "9      যে    685\n",
       "10   থেকে    665\n",
       "11    ...    589\n",
       "12    হয়ে    588\n",
       "13     সে    585\n",
       "14   সাথে    563\n",
       "15   জন্য    541\n",
       "16     কি    527\n",
       "17   করতে    482\n",
       "18  তোমার    479\n",
       "19    কথা    466"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = pd.DataFrame(c1.items(), columns=['Words', 'Count'])\n",
    "d1 = d1.head(20)\n",
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "id": "mQG517rQuINx",
    "outputId": "03c3cecb-90cd-48be-cf80-88715b38d18b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>না</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>মানুষ</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>করে</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>জন্য</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ভালোবাসা</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>না।</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>করা</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>অনেক</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>মানুষের</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>আর</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ভালো</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>করতে</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>সব</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>হবে</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>হয়</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>যে</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>সাথে</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>কিছু</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>নিজের</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ভালোবাসার</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Words  Count\n",
       "0          না    138\n",
       "1       মানুষ     66\n",
       "2         করে     60\n",
       "3        জন্য     59\n",
       "4    ভালোবাসা     54\n",
       "5         না।     50\n",
       "6         করা     44\n",
       "7        অনেক     44\n",
       "8     মানুষের     42\n",
       "9          আর     41\n",
       "10       ভালো     40\n",
       "11       করতে     40\n",
       "12         সব     39\n",
       "13        হবে     39\n",
       "14         হয়     38\n",
       "15         যে     35\n",
       "16       সাথে     33\n",
       "17       কিছু     33\n",
       "18      নিজের     31\n",
       "19  ভালোবাসার     30"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2 = pd.DataFrame(c2.items(), columns=['Words', 'Count'])\n",
    "d2 = d2.head(20)\n",
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "6sXflk5xuqhL",
    "outputId": "69dfe4e0-cca7-4570-9faa-947a14d39949"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import numpy as np\n",
    "prop = fm.FontProperties(fname='kalpurush.ttf')\n",
    "plt.barh(d1.Words,d1.Count)\n",
    "plt.yticks(d1.Words,fontproperties=prop)\n",
    "plt.title('Most Frequent Word in Bengali Text')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "ad2xMkZuvJtL",
    "outputId": "c00f924b-a187-4fa1-c315-90f79066d505"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.barh(d2.Words,d2.Count)\n",
    "plt.yticks(d2.Words,fontproperties=prop)\n",
    "plt.title('Most Frequent Word in Bengali Text Summary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70qGA1wNMB8e",
    "outputId": "21ec9c6a-e83a-49a3-98f8-20969efb2f97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    <go> বাংলাদেশে কোচিং বানিজ্য বন্ধ এখন সময়ের দা...\n",
       "2    <go> যদি শিশুরা বই পড়ার অভ্যাস করে তাহলে সারা ...\n",
       "3    <go> বাংলাদেশে সব স্তরে নারীর ক্ষমতায়নের জন্য ...\n",
       "4                <go> ভালো কথা বল, নয়ত চুপ থাকো <stop>\n",
       "5    <go> সমাবর্তনের মাধ্যমে শিক্ষা জীবনের শেষ হলেও...\n",
       "Name: Summary, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = summary.apply(lambda x: '<go> ' + str(x) + ' <stop>')\n",
    "summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "8HvwpXkyME9l"
   },
   "outputs": [],
   "source": [
    "# since < and > from default tokens cannot be removed\n",
    "filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n",
    "oov_token = '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "BVV8LssUMUrY"
   },
   "outputs": [],
   "source": [
    "document_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token)\n",
    "summary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "0Sj_k9u8Mb1R"
   },
   "outputs": [],
   "source": [
    "document_tokenizer.fit_on_texts(document)\n",
    "summary_tokenizer.fit_on_texts(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "r9adUMBPMmlM"
   },
   "outputs": [],
   "source": [
    "inputs = document_tokenizer.texts_to_sequences(document)\n",
    "targets = summary_tokenizer.texts_to_sequences(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2AOW-f0HMqhc",
    "outputId": "e651aab1-4348-493b-fc06-0cff93be78af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[141, 495, 862, 340, 57, 2076, 496]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_tokenizer.texts_to_sequences([\"বাংলাদেশে কোচিং বানিজ্য বন্ধ এখন সময়ের দাবি\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NVk8QZJfMw9m",
    "outputId": "3a559590-b9b2-4db6-9e8f-6f215cd7d5ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['দরকার। অপরকে নিজ প্রাপ্য নিজেকে সম্পর্কে উঠতে']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_tokenizer.sequences_to_texts([[153, 547, 942, 379, 61, 134, 548]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pHL2dMQmM9mo",
    "outputId": "175e8f44-ef45-4254-9189-332c9abb97fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22703, 2651)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_vocab_size = len(document_tokenizer.word_index) + 1\n",
    "decoder_vocab_size = len(summary_tokenizer.word_index) + 1\n",
    "\n",
    "encoder_vocab_size, decoder_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "dfaZi7eVNBXi"
   },
   "outputs": [],
   "source": [
    "document_lengths = pd.Series([len(x) for x in document])\n",
    "summary_lengths = pd.Series([len(x) for x in summary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r7dSrFUNNDgC",
    "outputId": "96647662-cc55-49fa-df6a-1c040a01dd68"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     921.000000\n",
       "mean      822.006515\n",
       "std       913.595774\n",
       "min        42.000000\n",
       "25%       256.000000\n",
       "50%       523.000000\n",
       "75%      1030.000000\n",
       "max      8262.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jOpgw0ZGNFzV",
    "outputId": "0aa93fa1-ef11-4530-c15b-c3ab22f53418"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    921.000000\n",
       "mean      54.939197\n",
       "std       23.555083\n",
       "min       23.000000\n",
       "25%       41.000000\n",
       "50%       51.000000\n",
       "75%       64.000000\n",
       "max      462.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "8juPbc01NIr9"
   },
   "outputs": [],
   "source": [
    "encoder_maxlen = 250\n",
    "decoder_maxlen = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ItReQzKWNMFg"
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
    "targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=decoder_maxlen, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "xG0C_tklNPwy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 21:34:52.282256: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-02 21:34:55.022358: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15401 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.cast(inputs, dtype=tf.int32)\n",
    "targets = tf.cast(targets, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "89rZHD26NSgF"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 2000\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "CoWgTzpKNV94"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "F6b0UxwHNZSf"
   },
   "outputs": [],
   "source": [
    "def get_angles(position, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return position * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "HQVUljkTNbuN"
   },
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(\n",
    "        np.arange(position)[:, np.newaxis],\n",
    "        np.arange(d_model)[np.newaxis, :],\n",
    "        d_model\n",
    "    )\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "hW2wInusNeLI"
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "6Xq1sTtjNgyi"
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "c9-KePeRNjN4"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "ZA7drIKONlcK"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "        output = self.dense(concat_attention)\n",
    "            \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "G3mHNuHLNoiU"
   },
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),\n",
    "        tf.keras.layers.Dense(d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "5lrg_xJ9N4tx"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "FKaLacvSN7w7"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "dQrnlVMWOBuF"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "    \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "DGC5vXJeOHMh"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "    \n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "ZFTmtmN5OJVM"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
    "\n",
    "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "Hrcb29sJOMR1"
   },
   "outputs": [],
   "source": [
    "# hyper-params\n",
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "EPOCHS = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "GruvDrXiOPmT"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "mITWV3C8OR3S"
   },
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "lXhjR4nzOUVe"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "9rGOkAf3OWzs"
   },
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "37hPdmtAOY_o"
   },
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "uERVOad5OdOV"
   },
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers, \n",
    "    d_model, \n",
    "    num_heads, \n",
    "    dff,\n",
    "    encoder_vocab_size, \n",
    "    decoder_vocab_size, \n",
    "    pe_input=encoder_vocab_size, \n",
    "    pe_target=decoder_vocab_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "JzpgMP4yOgRE"
   },
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "W7BPPE7QOipH"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"checkpoints\"\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "YZitSb2COlBR"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(\n",
    "            inp, tar_inp, \n",
    "            True, \n",
    "            enc_padding_mask, \n",
    "            combined_mask, \n",
    "            dec_padding_mask\n",
    "        )\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y3Hz5FbaOnhr",
    "outputId": "2dea4e2c-9daf-4312-9f61-09c3bbba4b39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 7.9308\n",
      "Epoch 1 Batch 8 Loss 7.9131\n",
      "Epoch 1 Batch 16 Loss 7.9210\n",
      "Epoch 1 Batch 24 Loss 7.9034\n",
      "Epoch 1 Batch 32 Loss 7.8794\n",
      "Epoch 1 Batch 40 Loss 7.8631\n",
      "Epoch 1 Batch 48 Loss 7.8469\n",
      "Epoch 1 Batch 56 Loss 7.8230\n",
      "Epoch 1 Batch 64 Loss 7.7991\n",
      "Epoch 1 Batch 72 Loss 7.7787\n",
      "Epoch 1 Batch 80 Loss 7.7624\n",
      "Epoch 1 Batch 88 Loss 7.7456\n",
      "Epoch 1 Batch 96 Loss 7.7304\n",
      "Epoch 1 Batch 104 Loss 7.7128\n",
      "Epoch 1 Batch 112 Loss 7.6977\n",
      "Epoch 1 Loss 7.6919\n",
      "Time taken for 1 epoch: 18.732307195663452 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 7.4950\n",
      "Epoch 2 Batch 8 Loss 7.4568\n",
      "Epoch 2 Batch 16 Loss 7.4435\n",
      "Epoch 2 Batch 24 Loss 7.4352\n",
      "Epoch 2 Batch 32 Loss 7.4178\n",
      "Epoch 2 Batch 40 Loss 7.4047\n",
      "Epoch 2 Batch 48 Loss 7.3899\n",
      "Epoch 2 Batch 56 Loss 7.3752\n",
      "Epoch 2 Batch 64 Loss 7.3705\n",
      "Epoch 2 Batch 72 Loss 7.3603\n",
      "Epoch 2 Batch 80 Loss 7.3521\n",
      "Epoch 2 Batch 88 Loss 7.3484\n",
      "Epoch 2 Batch 96 Loss 7.3416\n",
      "Epoch 2 Batch 104 Loss 7.3345\n",
      "Epoch 2 Batch 112 Loss 7.3252\n",
      "Epoch 2 Loss 7.3252\n",
      "Time taken for 1 epoch: 2.8189961910247803 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 7.2010\n",
      "Epoch 3 Batch 8 Loss 7.1566\n",
      "Epoch 3 Batch 16 Loss 7.1322\n",
      "Epoch 3 Batch 24 Loss 7.1131\n",
      "Epoch 3 Batch 32 Loss 7.1145\n",
      "Epoch 3 Batch 40 Loss 7.0809\n",
      "Epoch 3 Batch 48 Loss 7.0710\n",
      "Epoch 3 Batch 56 Loss 7.0633\n",
      "Epoch 3 Batch 64 Loss 7.0540\n",
      "Epoch 3 Batch 72 Loss 7.0462\n",
      "Epoch 3 Batch 80 Loss 7.0437\n",
      "Epoch 3 Batch 88 Loss 7.0340\n",
      "Epoch 3 Batch 96 Loss 7.0245\n",
      "Epoch 3 Batch 104 Loss 7.0154\n",
      "Epoch 3 Batch 112 Loss 7.0119\n",
      "Epoch 3 Loss 7.0148\n",
      "Time taken for 1 epoch: 2.8114118576049805 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 6.9611\n",
      "Epoch 4 Batch 8 Loss 6.8143\n",
      "Epoch 4 Batch 16 Loss 6.8125\n",
      "Epoch 4 Batch 24 Loss 6.8100\n",
      "Epoch 4 Batch 32 Loss 6.7993\n",
      "Epoch 4 Batch 40 Loss 6.7730\n",
      "Epoch 4 Batch 48 Loss 6.7732\n",
      "Epoch 4 Batch 56 Loss 6.7596\n",
      "Epoch 4 Batch 64 Loss 6.7489\n",
      "Epoch 4 Batch 72 Loss 6.7451\n",
      "Epoch 4 Batch 80 Loss 6.7607\n",
      "Epoch 4 Batch 88 Loss 6.7385\n",
      "Epoch 4 Batch 96 Loss 6.7421\n",
      "Epoch 4 Batch 104 Loss 6.7465\n",
      "Epoch 4 Batch 112 Loss 6.7622\n",
      "Epoch 4 Loss 6.7591\n",
      "Time taken for 1 epoch: 2.7954375743865967 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 6.6597\n",
      "Epoch 5 Batch 8 Loss 6.6453\n",
      "Epoch 5 Batch 16 Loss 6.6517\n",
      "Epoch 5 Batch 24 Loss 6.6193\n",
      "Epoch 5 Batch 32 Loss 6.6138\n",
      "Epoch 5 Batch 40 Loss 6.6443\n",
      "Epoch 5 Batch 48 Loss 6.6425\n",
      "Epoch 5 Batch 56 Loss 6.6634\n",
      "Epoch 5 Batch 64 Loss 6.6561\n",
      "Epoch 5 Batch 72 Loss 6.6526\n",
      "Epoch 5 Batch 80 Loss 6.6365\n",
      "Epoch 5 Batch 88 Loss 6.6282\n",
      "Epoch 5 Batch 96 Loss 6.6145\n",
      "Epoch 5 Batch 104 Loss 6.6030\n",
      "Epoch 5 Batch 112 Loss 6.6120\n",
      "Saving checkpoint for epoch 5 at checkpoints/ckpt-1\n",
      "Epoch 5 Loss 6.6108\n",
      "Time taken for 1 epoch: 3.114856243133545 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 6.3082\n",
      "Epoch 6 Batch 8 Loss 6.4392\n",
      "Epoch 6 Batch 16 Loss 6.4640\n",
      "Epoch 6 Batch 24 Loss 6.4958\n",
      "Epoch 6 Batch 32 Loss 6.4741\n",
      "Epoch 6 Batch 40 Loss 6.4849\n",
      "Epoch 6 Batch 48 Loss 6.4763\n",
      "Epoch 6 Batch 56 Loss 6.4867\n",
      "Epoch 6 Batch 64 Loss 6.4869\n",
      "Epoch 6 Batch 72 Loss 6.4924\n",
      "Epoch 6 Batch 80 Loss 6.4866\n",
      "Epoch 6 Batch 88 Loss 6.4900\n",
      "Epoch 6 Batch 96 Loss 6.4779\n",
      "Epoch 6 Batch 104 Loss 6.4901\n",
      "Epoch 6 Batch 112 Loss 6.4990\n",
      "Epoch 6 Loss 6.5080\n",
      "Time taken for 1 epoch: 2.7892119884490967 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 6.3593\n",
      "Epoch 7 Batch 8 Loss 6.3838\n",
      "Epoch 7 Batch 16 Loss 6.4103\n",
      "Epoch 7 Batch 24 Loss 6.3983\n",
      "Epoch 7 Batch 32 Loss 6.3451\n",
      "Epoch 7 Batch 40 Loss 6.3460\n",
      "Epoch 7 Batch 48 Loss 6.3362\n",
      "Epoch 7 Batch 56 Loss 6.3348\n",
      "Epoch 7 Batch 64 Loss 6.3402\n",
      "Epoch 7 Batch 72 Loss 6.3436\n",
      "Epoch 7 Batch 80 Loss 6.3497\n",
      "Epoch 7 Batch 88 Loss 6.3349\n",
      "Epoch 7 Batch 96 Loss 6.3535\n",
      "Epoch 7 Batch 104 Loss 6.3471\n",
      "Epoch 7 Batch 112 Loss 6.3489\n",
      "Epoch 7 Loss 6.3506\n",
      "Time taken for 1 epoch: 2.8001561164855957 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 6.4745\n",
      "Epoch 8 Batch 8 Loss 6.3253\n",
      "Epoch 8 Batch 16 Loss 6.3437\n",
      "Epoch 8 Batch 24 Loss 6.2642\n",
      "Epoch 8 Batch 32 Loss 6.2267\n",
      "Epoch 8 Batch 40 Loss 6.2012\n",
      "Epoch 8 Batch 48 Loss 6.1841\n",
      "Epoch 8 Batch 56 Loss 6.1653\n",
      "Epoch 8 Batch 64 Loss 6.1447\n",
      "Epoch 8 Batch 72 Loss 6.1394\n",
      "Epoch 8 Batch 80 Loss 6.1389\n",
      "Epoch 8 Batch 88 Loss 6.1577\n",
      "Epoch 8 Batch 96 Loss 6.1521\n",
      "Epoch 8 Batch 104 Loss 6.1576\n",
      "Epoch 8 Batch 112 Loss 6.1576\n",
      "Epoch 8 Loss 6.1561\n",
      "Time taken for 1 epoch: 2.7929201126098633 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 6.2704\n",
      "Epoch 9 Batch 8 Loss 6.1601\n",
      "Epoch 9 Batch 16 Loss 6.1091\n",
      "Epoch 9 Batch 24 Loss 6.0612\n",
      "Epoch 9 Batch 32 Loss 6.0503\n",
      "Epoch 9 Batch 40 Loss 6.0479\n",
      "Epoch 9 Batch 48 Loss 6.0270\n",
      "Epoch 9 Batch 56 Loss 6.0090\n",
      "Epoch 9 Batch 64 Loss 6.0150\n",
      "Epoch 9 Batch 72 Loss 6.0052\n",
      "Epoch 9 Batch 80 Loss 6.0021\n",
      "Epoch 9 Batch 88 Loss 6.0037\n",
      "Epoch 9 Batch 96 Loss 5.9937\n",
      "Epoch 9 Batch 104 Loss 5.9929\n",
      "Epoch 9 Batch 112 Loss 5.9963\n",
      "Epoch 9 Loss 5.9965\n",
      "Time taken for 1 epoch: 2.8006551265716553 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 5.7704\n",
      "Epoch 10 Batch 8 Loss 5.7299\n",
      "Epoch 10 Batch 16 Loss 5.7070\n",
      "Epoch 10 Batch 24 Loss 5.7289\n",
      "Epoch 10 Batch 32 Loss 5.7547\n",
      "Epoch 10 Batch 40 Loss 5.7600\n",
      "Epoch 10 Batch 48 Loss 5.7812\n",
      "Epoch 10 Batch 56 Loss 5.7620\n",
      "Epoch 10 Batch 64 Loss 5.7451\n",
      "Epoch 10 Batch 72 Loss 5.7509\n",
      "Epoch 10 Batch 80 Loss 5.7664\n",
      "Epoch 10 Batch 88 Loss 5.7746\n",
      "Epoch 10 Batch 96 Loss 5.7779\n",
      "Epoch 10 Batch 104 Loss 5.7819\n",
      "Epoch 10 Batch 112 Loss 5.7777\n",
      "Saving checkpoint for epoch 10 at checkpoints/ckpt-2\n",
      "Epoch 10 Loss 5.7839\n",
      "Time taken for 1 epoch: 3.1189587116241455 secs\n",
      "\n",
      "Epoch 11 Batch 0 Loss 5.4051\n",
      "Epoch 11 Batch 8 Loss 5.5870\n",
      "Epoch 11 Batch 16 Loss 5.6100\n",
      "Epoch 11 Batch 24 Loss 5.6524\n",
      "Epoch 11 Batch 32 Loss 5.6508\n",
      "Epoch 11 Batch 40 Loss 5.6451\n",
      "Epoch 11 Batch 48 Loss 5.6253\n",
      "Epoch 11 Batch 56 Loss 5.6211\n",
      "Epoch 11 Batch 64 Loss 5.6132\n",
      "Epoch 11 Batch 72 Loss 5.6101\n",
      "Epoch 11 Batch 80 Loss 5.6070\n",
      "Epoch 11 Batch 88 Loss 5.6027\n",
      "Epoch 11 Batch 96 Loss 5.5932\n",
      "Epoch 11 Batch 104 Loss 5.5819\n",
      "Epoch 11 Batch 112 Loss 5.5866\n",
      "Epoch 11 Loss 5.5822\n",
      "Time taken for 1 epoch: 2.8088810443878174 secs\n",
      "\n",
      "Epoch 12 Batch 0 Loss 5.2937\n",
      "Epoch 12 Batch 8 Loss 5.4086\n",
      "Epoch 12 Batch 16 Loss 5.3990\n",
      "Epoch 12 Batch 24 Loss 5.3871\n",
      "Epoch 12 Batch 32 Loss 5.3881\n",
      "Epoch 12 Batch 40 Loss 5.3541\n",
      "Epoch 12 Batch 48 Loss 5.3404\n",
      "Epoch 12 Batch 56 Loss 5.3458\n",
      "Epoch 12 Batch 64 Loss 5.3448\n",
      "Epoch 12 Batch 72 Loss 5.3475\n",
      "Epoch 12 Batch 80 Loss 5.3473\n",
      "Epoch 12 Batch 88 Loss 5.3545\n",
      "Epoch 12 Batch 96 Loss 5.3623\n",
      "Epoch 12 Batch 104 Loss 5.3757\n",
      "Epoch 12 Batch 112 Loss 5.3800\n",
      "Epoch 12 Loss 5.4011\n",
      "Time taken for 1 epoch: 2.800140142440796 secs\n",
      "\n",
      "Epoch 13 Batch 0 Loss 5.8208\n",
      "Epoch 13 Batch 8 Loss 5.3906\n",
      "Epoch 13 Batch 16 Loss 5.2688\n",
      "Epoch 13 Batch 24 Loss 5.2824\n",
      "Epoch 13 Batch 32 Loss 5.2457\n",
      "Epoch 13 Batch 40 Loss 5.2410\n",
      "Epoch 13 Batch 48 Loss 5.2226\n",
      "Epoch 13 Batch 56 Loss 5.2147\n",
      "Epoch 13 Batch 64 Loss 5.2013\n",
      "Epoch 13 Batch 72 Loss 5.1811\n",
      "Epoch 13 Batch 80 Loss 5.1876\n",
      "Epoch 13 Batch 88 Loss 5.1688\n",
      "Epoch 13 Batch 96 Loss 5.1818\n",
      "Epoch 13 Batch 104 Loss 5.1917\n",
      "Epoch 13 Batch 112 Loss 5.1906\n",
      "Epoch 13 Loss 5.1839\n",
      "Time taken for 1 epoch: 2.8017640113830566 secs\n",
      "\n",
      "Epoch 14 Batch 0 Loss 5.0902\n",
      "Epoch 14 Batch 8 Loss 4.9877\n",
      "Epoch 14 Batch 16 Loss 5.0061\n",
      "Epoch 14 Batch 24 Loss 4.9843\n",
      "Epoch 14 Batch 32 Loss 5.0063\n",
      "Epoch 14 Batch 40 Loss 4.9844\n",
      "Epoch 14 Batch 48 Loss 4.9994\n",
      "Epoch 14 Batch 56 Loss 4.9942\n",
      "Epoch 14 Batch 64 Loss 4.9759\n",
      "Epoch 14 Batch 72 Loss 4.9618\n",
      "Epoch 14 Batch 80 Loss 4.9869\n",
      "Epoch 14 Batch 88 Loss 4.9833\n",
      "Epoch 14 Batch 96 Loss 4.9993\n",
      "Epoch 14 Batch 104 Loss 4.9970\n",
      "Epoch 14 Batch 112 Loss 4.9879\n",
      "Epoch 14 Loss 4.9851\n",
      "Time taken for 1 epoch: 2.8148186206817627 secs\n",
      "\n",
      "Epoch 15 Batch 0 Loss 4.7610\n",
      "Epoch 15 Batch 8 Loss 4.5493\n",
      "Epoch 15 Batch 16 Loss 4.7067\n",
      "Epoch 15 Batch 24 Loss 4.7276\n",
      "Epoch 15 Batch 32 Loss 4.7524\n",
      "Epoch 15 Batch 40 Loss 4.7675\n",
      "Epoch 15 Batch 48 Loss 4.7551\n",
      "Epoch 15 Batch 56 Loss 4.7629\n",
      "Epoch 15 Batch 64 Loss 4.7500\n",
      "Epoch 15 Batch 72 Loss 4.7518\n",
      "Epoch 15 Batch 80 Loss 4.7552\n",
      "Epoch 15 Batch 88 Loss 4.7472\n",
      "Epoch 15 Batch 96 Loss 4.7369\n",
      "Epoch 15 Batch 104 Loss 4.7373\n",
      "Epoch 15 Batch 112 Loss 4.7463\n",
      "Saving checkpoint for epoch 15 at checkpoints/ckpt-3\n",
      "Epoch 15 Loss 4.7512\n",
      "Time taken for 1 epoch: 3.1070690155029297 secs\n",
      "\n",
      "Epoch 16 Batch 0 Loss 5.3896\n",
      "Epoch 16 Batch 8 Loss 4.4715\n",
      "Epoch 16 Batch 16 Loss 4.5142\n",
      "Epoch 16 Batch 24 Loss 4.4751\n",
      "Epoch 16 Batch 32 Loss 4.5005\n",
      "Epoch 16 Batch 40 Loss 4.5161\n",
      "Epoch 16 Batch 48 Loss 4.5200\n",
      "Epoch 16 Batch 56 Loss 4.5131\n",
      "Epoch 16 Batch 64 Loss 4.5164\n",
      "Epoch 16 Batch 72 Loss 4.5078\n",
      "Epoch 16 Batch 80 Loss 4.4949\n",
      "Epoch 16 Batch 88 Loss 4.5115\n",
      "Epoch 16 Batch 96 Loss 4.5046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Batch 104 Loss 4.5212\n",
      "Epoch 16 Batch 112 Loss 4.5283\n",
      "Epoch 16 Loss 4.5320\n",
      "Time taken for 1 epoch: 2.808283567428589 secs\n",
      "\n",
      "Epoch 17 Batch 0 Loss 4.1561\n",
      "Epoch 17 Batch 8 Loss 4.2520\n",
      "Epoch 17 Batch 16 Loss 4.2909\n",
      "Epoch 17 Batch 24 Loss 4.2552\n",
      "Epoch 17 Batch 32 Loss 4.2328\n",
      "Epoch 17 Batch 40 Loss 4.2193\n",
      "Epoch 17 Batch 48 Loss 4.2358\n",
      "Epoch 17 Batch 56 Loss 4.2254\n",
      "Epoch 17 Batch 64 Loss 4.2418\n",
      "Epoch 17 Batch 72 Loss 4.2624\n",
      "Epoch 17 Batch 80 Loss 4.2713\n",
      "Epoch 17 Batch 88 Loss 4.2900\n",
      "Epoch 17 Batch 96 Loss 4.3002\n",
      "Epoch 17 Batch 104 Loss 4.3018\n",
      "Epoch 17 Batch 112 Loss 4.2843\n",
      "Epoch 17 Loss 4.2781\n",
      "Time taken for 1 epoch: 2.7981269359588623 secs\n",
      "\n",
      "Epoch 18 Batch 0 Loss 3.6145\n",
      "Epoch 18 Batch 8 Loss 3.9465\n",
      "Epoch 18 Batch 16 Loss 3.9629\n",
      "Epoch 18 Batch 24 Loss 4.0255\n",
      "Epoch 18 Batch 32 Loss 4.0237\n",
      "Epoch 18 Batch 40 Loss 4.0588\n",
      "Epoch 18 Batch 48 Loss 4.0272\n",
      "Epoch 18 Batch 56 Loss 4.0246\n",
      "Epoch 18 Batch 64 Loss 4.0066\n",
      "Epoch 18 Batch 72 Loss 4.0307\n",
      "Epoch 18 Batch 80 Loss 4.0341\n",
      "Epoch 18 Batch 88 Loss 4.0428\n",
      "Epoch 18 Batch 96 Loss 4.0482\n",
      "Epoch 18 Batch 104 Loss 4.0523\n",
      "Epoch 18 Batch 112 Loss 4.0619\n",
      "Epoch 18 Loss 4.0523\n",
      "Time taken for 1 epoch: 2.7993392944335938 secs\n",
      "\n",
      "Epoch 19 Batch 0 Loss 3.5646\n",
      "Epoch 19 Batch 8 Loss 3.6090\n",
      "Epoch 19 Batch 16 Loss 3.6966\n",
      "Epoch 19 Batch 24 Loss 3.7404\n",
      "Epoch 19 Batch 32 Loss 3.7595\n",
      "Epoch 19 Batch 40 Loss 3.7562\n",
      "Epoch 19 Batch 48 Loss 3.7618\n",
      "Epoch 19 Batch 56 Loss 3.7384\n",
      "Epoch 19 Batch 64 Loss 3.7538\n",
      "Epoch 19 Batch 72 Loss 3.7542\n",
      "Epoch 19 Batch 80 Loss 3.7398\n",
      "Epoch 19 Batch 88 Loss 3.7640\n",
      "Epoch 19 Batch 96 Loss 3.7772\n",
      "Epoch 19 Batch 104 Loss 3.7800\n",
      "Epoch 19 Batch 112 Loss 3.7895\n",
      "Epoch 19 Loss 3.7852\n",
      "Time taken for 1 epoch: 2.8007798194885254 secs\n",
      "\n",
      "Epoch 20 Batch 0 Loss 3.5365\n",
      "Epoch 20 Batch 8 Loss 3.4414\n",
      "Epoch 20 Batch 16 Loss 3.4882\n",
      "Epoch 20 Batch 24 Loss 3.4867\n",
      "Epoch 20 Batch 32 Loss 3.5067\n",
      "Epoch 20 Batch 40 Loss 3.5105\n",
      "Epoch 20 Batch 48 Loss 3.5389\n",
      "Epoch 20 Batch 56 Loss 3.5059\n",
      "Epoch 20 Batch 64 Loss 3.5189\n",
      "Epoch 20 Batch 72 Loss 3.5139\n",
      "Epoch 20 Batch 80 Loss 3.5133\n",
      "Epoch 20 Batch 88 Loss 3.5025\n",
      "Epoch 20 Batch 96 Loss 3.5115\n",
      "Epoch 20 Batch 104 Loss 3.5301\n",
      "Epoch 20 Batch 112 Loss 3.5272\n",
      "Saving checkpoint for epoch 20 at checkpoints/ckpt-4\n",
      "Epoch 20 Loss 3.5224\n",
      "Time taken for 1 epoch: 3.085935115814209 secs\n",
      "\n",
      "Epoch 21 Batch 0 Loss 3.4038\n",
      "Epoch 21 Batch 8 Loss 3.0371\n",
      "Epoch 21 Batch 16 Loss 3.0532\n",
      "Epoch 21 Batch 24 Loss 3.0464\n",
      "Epoch 21 Batch 32 Loss 3.1226\n",
      "Epoch 21 Batch 40 Loss 3.1515\n",
      "Epoch 21 Batch 48 Loss 3.1686\n",
      "Epoch 21 Batch 56 Loss 3.1780\n",
      "Epoch 21 Batch 64 Loss 3.1837\n",
      "Epoch 21 Batch 72 Loss 3.1804\n",
      "Epoch 21 Batch 80 Loss 3.2035\n",
      "Epoch 21 Batch 88 Loss 3.2192\n",
      "Epoch 21 Batch 96 Loss 3.2361\n",
      "Epoch 21 Batch 104 Loss 3.2446\n",
      "Epoch 21 Batch 112 Loss 3.2580\n",
      "Epoch 21 Loss 3.2581\n",
      "Time taken for 1 epoch: 2.803213119506836 secs\n",
      "\n",
      "Epoch 22 Batch 0 Loss 2.5492\n",
      "Epoch 22 Batch 8 Loss 2.8919\n",
      "Epoch 22 Batch 16 Loss 2.9536\n",
      "Epoch 22 Batch 24 Loss 2.9208\n",
      "Epoch 22 Batch 32 Loss 2.9310\n",
      "Epoch 22 Batch 40 Loss 2.9170\n",
      "Epoch 22 Batch 48 Loss 2.9246\n",
      "Epoch 22 Batch 56 Loss 2.9342\n",
      "Epoch 22 Batch 64 Loss 2.9411\n",
      "Epoch 22 Batch 72 Loss 2.9462\n",
      "Epoch 22 Batch 80 Loss 2.9515\n",
      "Epoch 22 Batch 88 Loss 2.9551\n",
      "Epoch 22 Batch 96 Loss 2.9467\n",
      "Epoch 22 Batch 104 Loss 2.9501\n",
      "Epoch 22 Batch 112 Loss 2.9683\n",
      "Epoch 22 Loss 2.9724\n",
      "Time taken for 1 epoch: 2.8164474964141846 secs\n",
      "\n",
      "Epoch 23 Batch 0 Loss 2.6451\n",
      "Epoch 23 Batch 8 Loss 2.6530\n",
      "Epoch 23 Batch 16 Loss 2.6024\n",
      "Epoch 23 Batch 24 Loss 2.6233\n",
      "Epoch 23 Batch 32 Loss 2.6598\n",
      "Epoch 23 Batch 40 Loss 2.6865\n",
      "Epoch 23 Batch 48 Loss 2.6689\n",
      "Epoch 23 Batch 56 Loss 2.6633\n",
      "Epoch 23 Batch 64 Loss 2.6698\n",
      "Epoch 23 Batch 72 Loss 2.6687\n",
      "Epoch 23 Batch 80 Loss 2.6775\n",
      "Epoch 23 Batch 88 Loss 2.6936\n",
      "Epoch 23 Batch 96 Loss 2.6953\n",
      "Epoch 23 Batch 104 Loss 2.6987\n",
      "Epoch 23 Batch 112 Loss 2.7046\n",
      "Epoch 23 Loss 2.6977\n",
      "Time taken for 1 epoch: 2.7999773025512695 secs\n",
      "\n",
      "Epoch 24 Batch 0 Loss 1.9323\n",
      "Epoch 24 Batch 8 Loss 2.2313\n",
      "Epoch 24 Batch 16 Loss 2.2177\n",
      "Epoch 24 Batch 24 Loss 2.2372\n",
      "Epoch 24 Batch 32 Loss 2.2290\n",
      "Epoch 24 Batch 40 Loss 2.2814\n",
      "Epoch 24 Batch 48 Loss 2.2814\n",
      "Epoch 24 Batch 56 Loss 2.2931\n",
      "Epoch 24 Batch 64 Loss 2.2950\n",
      "Epoch 24 Batch 72 Loss 2.3057\n",
      "Epoch 24 Batch 80 Loss 2.3196\n",
      "Epoch 24 Batch 88 Loss 2.3339\n",
      "Epoch 24 Batch 96 Loss 2.3666\n",
      "Epoch 24 Batch 104 Loss 2.3776\n",
      "Epoch 24 Batch 112 Loss 2.3953\n",
      "Epoch 24 Loss 2.4115\n",
      "Time taken for 1 epoch: 2.7954766750335693 secs\n",
      "\n",
      "Epoch 25 Batch 0 Loss 1.4432\n",
      "Epoch 25 Batch 8 Loss 1.8816\n",
      "Epoch 25 Batch 16 Loss 1.9388\n",
      "Epoch 25 Batch 24 Loss 1.9918\n",
      "Epoch 25 Batch 32 Loss 2.0359\n",
      "Epoch 25 Batch 40 Loss 2.0147\n",
      "Epoch 25 Batch 48 Loss 2.0464\n",
      "Epoch 25 Batch 56 Loss 2.0865\n",
      "Epoch 25 Batch 64 Loss 2.0991\n",
      "Epoch 25 Batch 72 Loss 2.1199\n",
      "Epoch 25 Batch 80 Loss 2.1093\n",
      "Epoch 25 Batch 88 Loss 2.1096\n",
      "Epoch 25 Batch 96 Loss 2.1272\n",
      "Epoch 25 Batch 104 Loss 2.1283\n",
      "Epoch 25 Batch 112 Loss 2.1361\n",
      "Saving checkpoint for epoch 25 at checkpoints/ckpt-5\n",
      "Epoch 25 Loss 2.1222\n",
      "Time taken for 1 epoch: 3.1105239391326904 secs\n",
      "\n",
      "Epoch 26 Batch 0 Loss 1.5778\n",
      "Epoch 26 Batch 8 Loss 1.6895\n",
      "Epoch 26 Batch 16 Loss 1.8032\n",
      "Epoch 26 Batch 24 Loss 1.8079\n",
      "Epoch 26 Batch 32 Loss 1.8111\n",
      "Epoch 26 Batch 40 Loss 1.8067\n",
      "Epoch 26 Batch 48 Loss 1.7838\n",
      "Epoch 26 Batch 56 Loss 1.7956\n",
      "Epoch 26 Batch 64 Loss 1.8088\n",
      "Epoch 26 Batch 72 Loss 1.8161\n",
      "Epoch 26 Batch 80 Loss 1.8411\n",
      "Epoch 26 Batch 88 Loss 1.8622\n",
      "Epoch 26 Batch 96 Loss 1.8790\n",
      "Epoch 26 Batch 104 Loss 1.8852\n",
      "Epoch 26 Batch 112 Loss 1.8915\n",
      "Epoch 26 Loss 1.9134\n",
      "Time taken for 1 epoch: 2.8052775859832764 secs\n",
      "\n",
      "Epoch 27 Batch 0 Loss 1.5467\n",
      "Epoch 27 Batch 8 Loss 1.7718\n",
      "Epoch 27 Batch 16 Loss 1.6908\n",
      "Epoch 27 Batch 24 Loss 1.6346\n",
      "Epoch 27 Batch 32 Loss 1.6043\n",
      "Epoch 27 Batch 40 Loss 1.5959\n",
      "Epoch 27 Batch 48 Loss 1.5875\n",
      "Epoch 27 Batch 56 Loss 1.6043\n",
      "Epoch 27 Batch 64 Loss 1.5955\n",
      "Epoch 27 Batch 72 Loss 1.6034\n",
      "Epoch 27 Batch 80 Loss 1.6083\n",
      "Epoch 27 Batch 88 Loss 1.6197\n",
      "Epoch 27 Batch 96 Loss 1.6325\n",
      "Epoch 27 Batch 104 Loss 1.6309\n",
      "Epoch 27 Batch 112 Loss 1.6560\n",
      "Epoch 27 Loss 1.6511\n",
      "Time taken for 1 epoch: 2.799325704574585 secs\n",
      "\n",
      "Epoch 28 Batch 0 Loss 1.3926\n",
      "Epoch 28 Batch 8 Loss 1.3922\n",
      "Epoch 28 Batch 16 Loss 1.3673\n",
      "Epoch 28 Batch 24 Loss 1.3934\n",
      "Epoch 28 Batch 32 Loss 1.3714\n",
      "Epoch 28 Batch 40 Loss 1.3416\n",
      "Epoch 28 Batch 48 Loss 1.3365\n",
      "Epoch 28 Batch 56 Loss 1.3351\n",
      "Epoch 28 Batch 64 Loss 1.3393\n",
      "Epoch 28 Batch 72 Loss 1.3691\n",
      "Epoch 28 Batch 80 Loss 1.3858\n",
      "Epoch 28 Batch 88 Loss 1.4002\n",
      "Epoch 28 Batch 96 Loss 1.4141\n",
      "Epoch 28 Batch 104 Loss 1.4168\n",
      "Epoch 28 Batch 112 Loss 1.4375\n",
      "Epoch 28 Loss 1.4436\n",
      "Time taken for 1 epoch: 2.7969095706939697 secs\n",
      "\n",
      "Epoch 29 Batch 0 Loss 1.0238\n",
      "Epoch 29 Batch 8 Loss 1.0925\n",
      "Epoch 29 Batch 16 Loss 1.1644\n",
      "Epoch 29 Batch 24 Loss 1.1120\n",
      "Epoch 29 Batch 32 Loss 1.1264\n",
      "Epoch 29 Batch 40 Loss 1.1418\n",
      "Epoch 29 Batch 48 Loss 1.1593\n",
      "Epoch 29 Batch 56 Loss 1.1718\n",
      "Epoch 29 Batch 64 Loss 1.1771\n",
      "Epoch 29 Batch 72 Loss 1.1828\n",
      "Epoch 29 Batch 80 Loss 1.1889\n",
      "Epoch 29 Batch 88 Loss 1.1967\n",
      "Epoch 29 Batch 96 Loss 1.2056\n",
      "Epoch 29 Batch 104 Loss 1.2208\n",
      "Epoch 29 Batch 112 Loss 1.2336\n",
      "Epoch 29 Loss 1.2364\n",
      "Time taken for 1 epoch: 2.8116445541381836 secs\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.7240\n",
      "Epoch 30 Batch 8 Loss 0.8084\n",
      "Epoch 30 Batch 16 Loss 0.8497\n",
      "Epoch 30 Batch 24 Loss 0.8745\n",
      "Epoch 30 Batch 32 Loss 0.8801\n",
      "Epoch 30 Batch 40 Loss 0.9121\n",
      "Epoch 30 Batch 48 Loss 0.9544\n",
      "Epoch 30 Batch 56 Loss 0.9718\n",
      "Epoch 30 Batch 64 Loss 0.9828\n",
      "Epoch 30 Batch 72 Loss 0.9935\n",
      "Epoch 30 Batch 80 Loss 1.0047\n",
      "Epoch 30 Batch 88 Loss 1.0219\n",
      "Epoch 30 Batch 96 Loss 1.0401\n",
      "Epoch 30 Batch 104 Loss 1.0526\n",
      "Epoch 30 Batch 112 Loss 1.0572\n",
      "Saving checkpoint for epoch 30 at checkpoints/ckpt-6\n",
      "Epoch 30 Loss 1.0551\n",
      "Time taken for 1 epoch: 3.1083264350891113 secs\n",
      "\n",
      "Epoch 31 Batch 0 Loss 0.9202\n",
      "Epoch 31 Batch 8 Loss 0.8127\n",
      "Epoch 31 Batch 16 Loss 0.9098\n",
      "Epoch 31 Batch 24 Loss 0.8645\n",
      "Epoch 31 Batch 32 Loss 0.8600\n",
      "Epoch 31 Batch 40 Loss 0.8294\n",
      "Epoch 31 Batch 48 Loss 0.8427\n",
      "Epoch 31 Batch 56 Loss 0.8532\n",
      "Epoch 31 Batch 64 Loss 0.8684\n",
      "Epoch 31 Batch 72 Loss 0.8728\n",
      "Epoch 31 Batch 80 Loss 0.8937\n",
      "Epoch 31 Batch 88 Loss 0.9122\n",
      "Epoch 31 Batch 96 Loss 0.9201\n",
      "Epoch 31 Batch 104 Loss 0.9336\n",
      "Epoch 31 Batch 112 Loss 0.9417\n",
      "Epoch 31 Loss 0.9530\n",
      "Time taken for 1 epoch: 2.8035426139831543 secs\n",
      "\n",
      "Epoch 32 Batch 0 Loss 0.7731\n",
      "Epoch 32 Batch 8 Loss 0.7128\n",
      "Epoch 32 Batch 16 Loss 0.7278\n",
      "Epoch 32 Batch 24 Loss 0.7505\n",
      "Epoch 32 Batch 32 Loss 0.7616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 Batch 40 Loss 0.7701\n",
      "Epoch 32 Batch 48 Loss 0.7629\n",
      "Epoch 32 Batch 56 Loss 0.7738\n",
      "Epoch 32 Batch 64 Loss 0.8021\n",
      "Epoch 32 Batch 72 Loss 0.8011\n",
      "Epoch 32 Batch 80 Loss 0.8077\n",
      "Epoch 32 Batch 88 Loss 0.8084\n",
      "Epoch 32 Batch 96 Loss 0.8207\n",
      "Epoch 32 Batch 104 Loss 0.8260\n",
      "Epoch 32 Batch 112 Loss 0.8305\n",
      "Epoch 32 Loss 0.8281\n",
      "Time taken for 1 epoch: 2.8024230003356934 secs\n",
      "\n",
      "Epoch 33 Batch 0 Loss 0.4954\n",
      "Epoch 33 Batch 8 Loss 0.6181\n",
      "Epoch 33 Batch 16 Loss 0.6597\n",
      "Epoch 33 Batch 24 Loss 0.6794\n",
      "Epoch 33 Batch 32 Loss 0.6667\n",
      "Epoch 33 Batch 40 Loss 0.6832\n",
      "Epoch 33 Batch 48 Loss 0.6775\n",
      "Epoch 33 Batch 56 Loss 0.6886\n",
      "Epoch 33 Batch 64 Loss 0.6949\n",
      "Epoch 33 Batch 72 Loss 0.7070\n",
      "Epoch 33 Batch 80 Loss 0.7220\n",
      "Epoch 33 Batch 88 Loss 0.7321\n",
      "Epoch 33 Batch 96 Loss 0.7437\n",
      "Epoch 33 Batch 104 Loss 0.7497\n",
      "Epoch 33 Batch 112 Loss 0.7579\n",
      "Epoch 33 Loss 0.7547\n",
      "Time taken for 1 epoch: 2.8117244243621826 secs\n",
      "\n",
      "Epoch 34 Batch 0 Loss 0.4995\n",
      "Epoch 34 Batch 8 Loss 0.7373\n",
      "Epoch 34 Batch 16 Loss 0.6920\n",
      "Epoch 34 Batch 24 Loss 0.6617\n",
      "Epoch 34 Batch 32 Loss 0.6325\n",
      "Epoch 34 Batch 40 Loss 0.6130\n",
      "Epoch 34 Batch 48 Loss 0.5953\n",
      "Epoch 34 Batch 56 Loss 0.6087\n",
      "Epoch 34 Batch 64 Loss 0.6018\n",
      "Epoch 34 Batch 72 Loss 0.6080\n",
      "Epoch 34 Batch 80 Loss 0.6175\n",
      "Epoch 34 Batch 88 Loss 0.6293\n",
      "Epoch 34 Batch 96 Loss 0.6403\n",
      "Epoch 34 Batch 104 Loss 0.6581\n",
      "Epoch 34 Batch 112 Loss 0.6723\n",
      "Epoch 34 Loss 0.6753\n",
      "Time taken for 1 epoch: 2.808295965194702 secs\n",
      "\n",
      "Epoch 35 Batch 0 Loss 0.5768\n",
      "Epoch 35 Batch 8 Loss 0.7116\n",
      "Epoch 35 Batch 16 Loss 0.7364\n",
      "Epoch 35 Batch 24 Loss 0.7156\n",
      "Epoch 35 Batch 32 Loss 0.7103\n",
      "Epoch 35 Batch 40 Loss 0.6964\n",
      "Epoch 35 Batch 48 Loss 0.6881\n",
      "Epoch 35 Batch 56 Loss 0.6712\n",
      "Epoch 35 Batch 64 Loss 0.6586\n",
      "Epoch 35 Batch 72 Loss 0.6636\n",
      "Epoch 35 Batch 80 Loss 0.6597\n",
      "Epoch 35 Batch 88 Loss 0.6550\n",
      "Epoch 35 Batch 96 Loss 0.6596\n",
      "Epoch 35 Batch 104 Loss 0.6628\n",
      "Epoch 35 Batch 112 Loss 0.6658\n",
      "Saving checkpoint for epoch 35 at checkpoints/ckpt-7\n",
      "Epoch 35 Loss 0.6704\n",
      "Time taken for 1 epoch: 3.1136698722839355 secs\n",
      "\n",
      "Epoch 36 Batch 0 Loss 0.5970\n",
      "Epoch 36 Batch 8 Loss 0.5687\n",
      "Epoch 36 Batch 16 Loss 0.5713\n",
      "Epoch 36 Batch 24 Loss 0.5997\n",
      "Epoch 36 Batch 32 Loss 0.5887\n",
      "Epoch 36 Batch 40 Loss 0.6082\n",
      "Epoch 36 Batch 48 Loss 0.6145\n",
      "Epoch 36 Batch 56 Loss 0.6194\n",
      "Epoch 36 Batch 64 Loss 0.6178\n",
      "Epoch 36 Batch 72 Loss 0.6120\n",
      "Epoch 36 Batch 80 Loss 0.6097\n",
      "Epoch 36 Batch 88 Loss 0.6065\n",
      "Epoch 36 Batch 96 Loss 0.6048\n",
      "Epoch 36 Batch 104 Loss 0.6047\n",
      "Epoch 36 Batch 112 Loss 0.6048\n",
      "Epoch 36 Loss 0.6049\n",
      "Time taken for 1 epoch: 2.8235883712768555 secs\n",
      "\n",
      "Epoch 37 Batch 0 Loss 0.4134\n",
      "Epoch 37 Batch 8 Loss 0.4859\n",
      "Epoch 37 Batch 16 Loss 0.4525\n",
      "Epoch 37 Batch 24 Loss 0.4400\n",
      "Epoch 37 Batch 32 Loss 0.4234\n",
      "Epoch 37 Batch 40 Loss 0.4172\n",
      "Epoch 37 Batch 48 Loss 0.4326\n",
      "Epoch 37 Batch 56 Loss 0.4347\n",
      "Epoch 37 Batch 64 Loss 0.4358\n",
      "Epoch 37 Batch 72 Loss 0.4450\n",
      "Epoch 37 Batch 80 Loss 0.4625\n",
      "Epoch 37 Batch 88 Loss 0.4642\n",
      "Epoch 37 Batch 96 Loss 0.4732\n",
      "Epoch 37 Batch 104 Loss 0.4762\n",
      "Epoch 37 Batch 112 Loss 0.4802\n",
      "Epoch 37 Loss 0.4833\n",
      "Time taken for 1 epoch: 2.8006370067596436 secs\n",
      "\n",
      "Epoch 38 Batch 0 Loss 0.4994\n",
      "Epoch 38 Batch 8 Loss 0.4653\n",
      "Epoch 38 Batch 16 Loss 0.4457\n",
      "Epoch 38 Batch 24 Loss 0.4543\n",
      "Epoch 38 Batch 32 Loss 0.4447\n",
      "Epoch 38 Batch 40 Loss 0.4388\n",
      "Epoch 38 Batch 48 Loss 0.4508\n",
      "Epoch 38 Batch 56 Loss 0.4572\n",
      "Epoch 38 Batch 64 Loss 0.4599\n",
      "Epoch 38 Batch 72 Loss 0.4690\n",
      "Epoch 38 Batch 80 Loss 0.4692\n",
      "Epoch 38 Batch 88 Loss 0.4688\n",
      "Epoch 38 Batch 96 Loss 0.4720\n",
      "Epoch 38 Batch 104 Loss 0.4775\n",
      "Epoch 38 Batch 112 Loss 0.4780\n",
      "Epoch 38 Loss 0.4807\n",
      "Time taken for 1 epoch: 2.8035078048706055 secs\n",
      "\n",
      "Epoch 39 Batch 0 Loss 0.2266\n",
      "Epoch 39 Batch 8 Loss 0.3909\n",
      "Epoch 39 Batch 16 Loss 0.3963\n",
      "Epoch 39 Batch 24 Loss 0.3803\n",
      "Epoch 39 Batch 32 Loss 0.3718\n",
      "Epoch 39 Batch 40 Loss 0.3649\n",
      "Epoch 39 Batch 48 Loss 0.3663\n",
      "Epoch 39 Batch 56 Loss 0.3707\n",
      "Epoch 39 Batch 64 Loss 0.3698\n",
      "Epoch 39 Batch 72 Loss 0.3725\n",
      "Epoch 39 Batch 80 Loss 0.3806\n",
      "Epoch 39 Batch 88 Loss 0.3792\n",
      "Epoch 39 Batch 96 Loss 0.3766\n",
      "Epoch 39 Batch 104 Loss 0.3815\n",
      "Epoch 39 Batch 112 Loss 0.3882\n",
      "Epoch 39 Loss 0.3936\n",
      "Time taken for 1 epoch: 2.7961618900299072 secs\n",
      "\n",
      "Epoch 40 Batch 0 Loss 0.4196\n",
      "Epoch 40 Batch 8 Loss 0.4129\n",
      "Epoch 40 Batch 16 Loss 0.3962\n",
      "Epoch 40 Batch 24 Loss 0.4041\n",
      "Epoch 40 Batch 32 Loss 0.3966\n",
      "Epoch 40 Batch 40 Loss 0.3893\n",
      "Epoch 40 Batch 48 Loss 0.3872\n",
      "Epoch 40 Batch 56 Loss 0.3770\n",
      "Epoch 40 Batch 64 Loss 0.3759\n",
      "Epoch 40 Batch 72 Loss 0.3773\n",
      "Epoch 40 Batch 80 Loss 0.3822\n",
      "Epoch 40 Batch 88 Loss 0.3789\n",
      "Epoch 40 Batch 96 Loss 0.3885\n",
      "Epoch 40 Batch 104 Loss 0.3897\n",
      "Epoch 40 Batch 112 Loss 0.3993\n",
      "Saving checkpoint for epoch 40 at checkpoints/ckpt-8\n",
      "Epoch 40 Loss 0.3982\n",
      "Time taken for 1 epoch: 3.846691370010376 secs\n",
      "\n",
      "Epoch 41 Batch 0 Loss 0.2827\n",
      "Epoch 41 Batch 8 Loss 0.3083\n",
      "Epoch 41 Batch 16 Loss 0.3361\n",
      "Epoch 41 Batch 24 Loss 0.3523\n",
      "Epoch 41 Batch 32 Loss 0.3539\n",
      "Epoch 41 Batch 40 Loss 0.3366\n",
      "Epoch 41 Batch 48 Loss 0.3426\n",
      "Epoch 41 Batch 56 Loss 0.3340\n",
      "Epoch 41 Batch 64 Loss 0.3368\n",
      "Epoch 41 Batch 72 Loss 0.3415\n",
      "Epoch 41 Batch 80 Loss 0.3490\n",
      "Epoch 41 Batch 88 Loss 0.3577\n",
      "Epoch 41 Batch 96 Loss 0.3509\n",
      "Epoch 41 Batch 104 Loss 0.3504\n",
      "Epoch 41 Batch 112 Loss 0.3549\n",
      "Epoch 41 Loss 0.3564\n",
      "Time taken for 1 epoch: 2.807053804397583 secs\n",
      "\n",
      "Epoch 42 Batch 0 Loss 0.4639\n",
      "Epoch 42 Batch 8 Loss 0.3261\n",
      "Epoch 42 Batch 16 Loss 0.2751\n",
      "Epoch 42 Batch 24 Loss 0.2719\n",
      "Epoch 42 Batch 32 Loss 0.2738\n",
      "Epoch 42 Batch 40 Loss 0.2884\n",
      "Epoch 42 Batch 48 Loss 0.2861\n",
      "Epoch 42 Batch 56 Loss 0.2900\n",
      "Epoch 42 Batch 64 Loss 0.2849\n",
      "Epoch 42 Batch 72 Loss 0.2883\n",
      "Epoch 42 Batch 80 Loss 0.2880\n",
      "Epoch 42 Batch 88 Loss 0.2960\n",
      "Epoch 42 Batch 96 Loss 0.3019\n",
      "Epoch 42 Batch 104 Loss 0.3074\n",
      "Epoch 42 Batch 112 Loss 0.3119\n",
      "Epoch 42 Loss 0.3141\n",
      "Time taken for 1 epoch: 2.807802200317383 secs\n",
      "\n",
      "Epoch 43 Batch 0 Loss 0.3141\n",
      "Epoch 43 Batch 8 Loss 0.2440\n",
      "Epoch 43 Batch 16 Loss 0.2503\n",
      "Epoch 43 Batch 24 Loss 0.2591\n",
      "Epoch 43 Batch 32 Loss 0.2668\n",
      "Epoch 43 Batch 40 Loss 0.2681\n",
      "Epoch 43 Batch 48 Loss 0.2592\n",
      "Epoch 43 Batch 56 Loss 0.2581\n",
      "Epoch 43 Batch 64 Loss 0.2751\n",
      "Epoch 43 Batch 72 Loss 0.2808\n",
      "Epoch 43 Batch 80 Loss 0.2825\n",
      "Epoch 43 Batch 88 Loss 0.2917\n",
      "Epoch 43 Batch 96 Loss 0.2931\n",
      "Epoch 43 Batch 104 Loss 0.3015\n",
      "Epoch 43 Batch 112 Loss 0.3078\n",
      "Epoch 43 Loss 0.3113\n",
      "Time taken for 1 epoch: 2.7985401153564453 secs\n",
      "\n",
      "Epoch 44 Batch 0 Loss 0.1211\n",
      "Epoch 44 Batch 8 Loss 0.2629\n",
      "Epoch 44 Batch 16 Loss 0.2509\n",
      "Epoch 44 Batch 24 Loss 0.2569\n",
      "Epoch 44 Batch 32 Loss 0.2525\n",
      "Epoch 44 Batch 40 Loss 0.2536\n",
      "Epoch 44 Batch 48 Loss 0.2563\n",
      "Epoch 44 Batch 56 Loss 0.2599\n",
      "Epoch 44 Batch 64 Loss 0.2624\n",
      "Epoch 44 Batch 72 Loss 0.2667\n",
      "Epoch 44 Batch 80 Loss 0.2690\n",
      "Epoch 44 Batch 88 Loss 0.2688\n",
      "Epoch 44 Batch 96 Loss 0.2713\n",
      "Epoch 44 Batch 104 Loss 0.2792\n",
      "Epoch 44 Batch 112 Loss 0.2871\n",
      "Epoch 44 Loss 0.2901\n",
      "Time taken for 1 epoch: 2.8230881690979004 secs\n",
      "\n",
      "Epoch 45 Batch 0 Loss 0.2470\n",
      "Epoch 45 Batch 8 Loss 0.3377\n",
      "Epoch 45 Batch 16 Loss 0.2998\n",
      "Epoch 45 Batch 24 Loss 0.2738\n",
      "Epoch 45 Batch 32 Loss 0.2561\n",
      "Epoch 45 Batch 40 Loss 0.2484\n",
      "Epoch 45 Batch 48 Loss 0.2346\n",
      "Epoch 45 Batch 56 Loss 0.2379\n",
      "Epoch 45 Batch 64 Loss 0.2413\n",
      "Epoch 45 Batch 72 Loss 0.2425\n",
      "Epoch 45 Batch 80 Loss 0.2463\n",
      "Epoch 45 Batch 88 Loss 0.2492\n",
      "Epoch 45 Batch 96 Loss 0.2562\n",
      "Epoch 45 Batch 104 Loss 0.2536\n",
      "Epoch 45 Batch 112 Loss 0.2569\n",
      "Saving checkpoint for epoch 45 at checkpoints/ckpt-9\n",
      "Epoch 45 Loss 0.2573\n",
      "Time taken for 1 epoch: 3.123054265975952 secs\n",
      "\n",
      "Epoch 46 Batch 0 Loss 0.0930\n",
      "Epoch 46 Batch 8 Loss 0.1504\n",
      "Epoch 46 Batch 16 Loss 0.1718\n",
      "Epoch 46 Batch 24 Loss 0.1768\n",
      "Epoch 46 Batch 32 Loss 0.1799\n",
      "Epoch 46 Batch 40 Loss 0.1925\n",
      "Epoch 46 Batch 48 Loss 0.1963\n",
      "Epoch 46 Batch 56 Loss 0.2030\n",
      "Epoch 46 Batch 64 Loss 0.2011\n",
      "Epoch 46 Batch 72 Loss 0.2147\n",
      "Epoch 46 Batch 80 Loss 0.2177\n",
      "Epoch 46 Batch 88 Loss 0.2200\n",
      "Epoch 46 Batch 96 Loss 0.2268\n",
      "Epoch 46 Batch 104 Loss 0.2265\n",
      "Epoch 46 Batch 112 Loss 0.2242\n",
      "Epoch 46 Loss 0.2239\n",
      "Time taken for 1 epoch: 2.8205699920654297 secs\n",
      "\n",
      "Epoch 47 Batch 0 Loss 0.2773\n",
      "Epoch 47 Batch 8 Loss 0.1663\n",
      "Epoch 47 Batch 16 Loss 0.1955\n",
      "Epoch 47 Batch 24 Loss 0.1888\n",
      "Epoch 47 Batch 32 Loss 0.1846\n",
      "Epoch 47 Batch 40 Loss 0.1939\n",
      "Epoch 47 Batch 48 Loss 0.1924\n",
      "Epoch 47 Batch 56 Loss 0.2041\n",
      "Epoch 47 Batch 64 Loss 0.2022\n",
      "Epoch 47 Batch 72 Loss 0.2011\n",
      "Epoch 47 Batch 80 Loss 0.2060\n",
      "Epoch 47 Batch 88 Loss 0.2080\n",
      "Epoch 47 Batch 96 Loss 0.2134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 Batch 104 Loss 0.2173\n",
      "Epoch 47 Batch 112 Loss 0.2187\n",
      "Epoch 47 Loss 0.2255\n",
      "Time taken for 1 epoch: 2.8078114986419678 secs\n",
      "\n",
      "Epoch 48 Batch 0 Loss 0.2711\n",
      "Epoch 48 Batch 8 Loss 0.3207\n",
      "Epoch 48 Batch 16 Loss 0.3068\n",
      "Epoch 48 Batch 24 Loss 0.2944\n",
      "Epoch 48 Batch 32 Loss 0.2778\n",
      "Epoch 48 Batch 40 Loss 0.2666\n",
      "Epoch 48 Batch 48 Loss 0.2586\n",
      "Epoch 48 Batch 56 Loss 0.2464\n",
      "Epoch 48 Batch 64 Loss 0.2389\n",
      "Epoch 48 Batch 72 Loss 0.2411\n",
      "Epoch 48 Batch 80 Loss 0.2378\n",
      "Epoch 48 Batch 88 Loss 0.2405\n",
      "Epoch 48 Batch 96 Loss 0.2355\n",
      "Epoch 48 Batch 104 Loss 0.2360\n",
      "Epoch 48 Batch 112 Loss 0.2384\n",
      "Epoch 48 Loss 0.2377\n",
      "Time taken for 1 epoch: 2.809091091156006 secs\n",
      "\n",
      "Epoch 49 Batch 0 Loss 0.0955\n",
      "Epoch 49 Batch 8 Loss 0.1711\n",
      "Epoch 49 Batch 16 Loss 0.1797\n",
      "Epoch 49 Batch 24 Loss 0.1719\n",
      "Epoch 49 Batch 32 Loss 0.1745\n",
      "Epoch 49 Batch 40 Loss 0.1744\n",
      "Epoch 49 Batch 48 Loss 0.1767\n",
      "Epoch 49 Batch 56 Loss 0.1779\n",
      "Epoch 49 Batch 64 Loss 0.1795\n",
      "Epoch 49 Batch 72 Loss 0.1811\n",
      "Epoch 49 Batch 80 Loss 0.1834\n",
      "Epoch 49 Batch 88 Loss 0.1827\n",
      "Epoch 49 Batch 96 Loss 0.1835\n",
      "Epoch 49 Batch 104 Loss 0.1880\n",
      "Epoch 49 Batch 112 Loss 0.1883\n",
      "Epoch 49 Loss 0.1876\n",
      "Time taken for 1 epoch: 2.8158562183380127 secs\n",
      "\n",
      "Epoch 50 Batch 0 Loss 0.1961\n",
      "Epoch 50 Batch 8 Loss 0.0961\n",
      "Epoch 50 Batch 16 Loss 0.1513\n",
      "Epoch 50 Batch 24 Loss 0.1655\n",
      "Epoch 50 Batch 32 Loss 0.1707\n",
      "Epoch 50 Batch 40 Loss 0.1764\n",
      "Epoch 50 Batch 48 Loss 0.1721\n",
      "Epoch 50 Batch 56 Loss 0.1800\n",
      "Epoch 50 Batch 64 Loss 0.1885\n",
      "Epoch 50 Batch 72 Loss 0.1919\n",
      "Epoch 50 Batch 80 Loss 0.1906\n",
      "Epoch 50 Batch 88 Loss 0.1910\n",
      "Epoch 50 Batch 96 Loss 0.1945\n",
      "Epoch 50 Batch 104 Loss 0.1952\n",
      "Epoch 50 Batch 112 Loss 0.1960\n",
      "Saving checkpoint for epoch 50 at checkpoints/ckpt-10\n",
      "Epoch 50 Loss 0.1993\n",
      "Time taken for 1 epoch: 3.0937249660491943 secs\n",
      "\n",
      "Epoch 51 Batch 0 Loss 0.0597\n",
      "Epoch 51 Batch 8 Loss 0.1989\n",
      "Epoch 51 Batch 16 Loss 0.2028\n",
      "Epoch 51 Batch 24 Loss 0.2092\n",
      "Epoch 51 Batch 32 Loss 0.2098\n",
      "Epoch 51 Batch 40 Loss 0.2079\n",
      "Epoch 51 Batch 48 Loss 0.2038\n",
      "Epoch 51 Batch 56 Loss 0.1971\n",
      "Epoch 51 Batch 64 Loss 0.1913\n",
      "Epoch 51 Batch 72 Loss 0.1890\n",
      "Epoch 51 Batch 80 Loss 0.1880\n",
      "Epoch 51 Batch 88 Loss 0.1887\n",
      "Epoch 51 Batch 96 Loss 0.1902\n",
      "Epoch 51 Batch 104 Loss 0.1876\n",
      "Epoch 51 Batch 112 Loss 0.1866\n",
      "Epoch 51 Loss 0.1857\n",
      "Time taken for 1 epoch: 2.8057003021240234 secs\n",
      "\n",
      "Epoch 52 Batch 0 Loss 0.0517\n",
      "Epoch 52 Batch 8 Loss 0.1032\n",
      "Epoch 52 Batch 16 Loss 0.1305\n",
      "Epoch 52 Batch 24 Loss 0.1378\n",
      "Epoch 52 Batch 32 Loss 0.1307\n",
      "Epoch 52 Batch 40 Loss 0.1255\n",
      "Epoch 52 Batch 48 Loss 0.1278\n",
      "Epoch 52 Batch 56 Loss 0.1292\n",
      "Epoch 52 Batch 64 Loss 0.1301\n",
      "Epoch 52 Batch 72 Loss 0.1356\n",
      "Epoch 52 Batch 80 Loss 0.1353\n",
      "Epoch 52 Batch 88 Loss 0.1375\n",
      "Epoch 52 Batch 96 Loss 0.1393\n",
      "Epoch 52 Batch 104 Loss 0.1421\n",
      "Epoch 52 Batch 112 Loss 0.1504\n",
      "Epoch 52 Loss 0.1499\n",
      "Time taken for 1 epoch: 2.8110904693603516 secs\n",
      "\n",
      "Epoch 53 Batch 0 Loss 0.3564\n",
      "Epoch 53 Batch 8 Loss 0.1461\n",
      "Epoch 53 Batch 16 Loss 0.1461\n",
      "Epoch 53 Batch 24 Loss 0.1339\n",
      "Epoch 53 Batch 32 Loss 0.1429\n",
      "Epoch 53 Batch 40 Loss 0.1453\n",
      "Epoch 53 Batch 48 Loss 0.1401\n",
      "Epoch 53 Batch 56 Loss 0.1433\n",
      "Epoch 53 Batch 64 Loss 0.1496\n",
      "Epoch 53 Batch 72 Loss 0.1506\n",
      "Epoch 53 Batch 80 Loss 0.1487\n",
      "Epoch 53 Batch 88 Loss 0.1532\n",
      "Epoch 53 Batch 96 Loss 0.1549\n",
      "Epoch 53 Batch 104 Loss 0.1567\n",
      "Epoch 53 Batch 112 Loss 0.1576\n",
      "Epoch 53 Loss 0.1575\n",
      "Time taken for 1 epoch: 2.8143069744110107 secs\n",
      "\n",
      "Epoch 54 Batch 0 Loss 0.1599\n",
      "Epoch 54 Batch 8 Loss 0.1559\n",
      "Epoch 54 Batch 16 Loss 0.1566\n",
      "Epoch 54 Batch 24 Loss 0.1560\n",
      "Epoch 54 Batch 32 Loss 0.1446\n",
      "Epoch 54 Batch 40 Loss 0.1409\n",
      "Epoch 54 Batch 48 Loss 0.1493\n",
      "Epoch 54 Batch 56 Loss 0.1549\n",
      "Epoch 54 Batch 64 Loss 0.1503\n",
      "Epoch 54 Batch 72 Loss 0.1570\n",
      "Epoch 54 Batch 80 Loss 0.1561\n",
      "Epoch 54 Batch 88 Loss 0.1546\n",
      "Epoch 54 Batch 96 Loss 0.1562\n",
      "Epoch 54 Batch 104 Loss 0.1626\n",
      "Epoch 54 Batch 112 Loss 0.1622\n",
      "Epoch 54 Loss 0.1619\n",
      "Time taken for 1 epoch: 2.800596237182617 secs\n",
      "\n",
      "Epoch 55 Batch 0 Loss 0.0822\n",
      "Epoch 55 Batch 8 Loss 0.0895\n",
      "Epoch 55 Batch 16 Loss 0.1115\n",
      "Epoch 55 Batch 24 Loss 0.1145\n",
      "Epoch 55 Batch 32 Loss 0.1332\n",
      "Epoch 55 Batch 40 Loss 0.1355\n",
      "Epoch 55 Batch 48 Loss 0.1358\n",
      "Epoch 55 Batch 56 Loss 0.1455\n",
      "Epoch 55 Batch 64 Loss 0.1477\n",
      "Epoch 55 Batch 72 Loss 0.1509\n",
      "Epoch 55 Batch 80 Loss 0.1540\n",
      "Epoch 55 Batch 88 Loss 0.1548\n",
      "Epoch 55 Batch 96 Loss 0.1560\n",
      "Epoch 55 Batch 104 Loss 0.1559\n",
      "Epoch 55 Batch 112 Loss 0.1557\n",
      "Saving checkpoint for epoch 55 at checkpoints/ckpt-11\n",
      "Epoch 55 Loss 0.1568\n",
      "Time taken for 1 epoch: 3.3139283657073975 secs\n",
      "\n",
      "Epoch 56 Batch 0 Loss 0.0525\n",
      "Epoch 56 Batch 8 Loss 0.1389\n",
      "Epoch 56 Batch 16 Loss 0.1283\n",
      "Epoch 56 Batch 24 Loss 0.1264\n",
      "Epoch 56 Batch 32 Loss 0.1232\n",
      "Epoch 56 Batch 40 Loss 0.1220\n",
      "Epoch 56 Batch 48 Loss 0.1325\n",
      "Epoch 56 Batch 56 Loss 0.1320\n",
      "Epoch 56 Batch 64 Loss 0.1351\n",
      "Epoch 56 Batch 72 Loss 0.1352\n",
      "Epoch 56 Batch 80 Loss 0.1359\n",
      "Epoch 56 Batch 88 Loss 0.1381\n",
      "Epoch 56 Batch 96 Loss 0.1369\n",
      "Epoch 56 Batch 104 Loss 0.1375\n",
      "Epoch 56 Batch 112 Loss 0.1387\n",
      "Epoch 56 Loss 0.1405\n",
      "Time taken for 1 epoch: 2.812098741531372 secs\n",
      "\n",
      "Epoch 57 Batch 0 Loss 0.1282\n",
      "Epoch 57 Batch 8 Loss 0.0974\n",
      "Epoch 57 Batch 16 Loss 0.1294\n",
      "Epoch 57 Batch 24 Loss 0.1110\n",
      "Epoch 57 Batch 32 Loss 0.1110\n",
      "Epoch 57 Batch 40 Loss 0.1178\n",
      "Epoch 57 Batch 48 Loss 0.1222\n",
      "Epoch 57 Batch 56 Loss 0.1245\n",
      "Epoch 57 Batch 64 Loss 0.1281\n",
      "Epoch 57 Batch 72 Loss 0.1290\n",
      "Epoch 57 Batch 80 Loss 0.1301\n",
      "Epoch 57 Batch 88 Loss 0.1327\n",
      "Epoch 57 Batch 96 Loss 0.1353\n",
      "Epoch 57 Batch 104 Loss 0.1386\n",
      "Epoch 57 Batch 112 Loss 0.1385\n",
      "Epoch 57 Loss 0.1387\n",
      "Time taken for 1 epoch: 2.8034582138061523 secs\n",
      "\n",
      "Epoch 58 Batch 0 Loss 0.0259\n",
      "Epoch 58 Batch 8 Loss 0.1100\n",
      "Epoch 58 Batch 16 Loss 0.1167\n",
      "Epoch 58 Batch 24 Loss 0.1290\n",
      "Epoch 58 Batch 32 Loss 0.1297\n",
      "Epoch 58 Batch 40 Loss 0.1277\n",
      "Epoch 58 Batch 48 Loss 0.1294\n",
      "Epoch 58 Batch 56 Loss 0.1359\n",
      "Epoch 58 Batch 64 Loss 0.1349\n",
      "Epoch 58 Batch 72 Loss 0.1364\n",
      "Epoch 58 Batch 80 Loss 0.1335\n",
      "Epoch 58 Batch 88 Loss 0.1364\n",
      "Epoch 58 Batch 96 Loss 0.1355\n",
      "Epoch 58 Batch 104 Loss 0.1370\n",
      "Epoch 58 Batch 112 Loss 0.1351\n",
      "Epoch 58 Loss 0.1352\n",
      "Time taken for 1 epoch: 2.8038785457611084 secs\n",
      "\n",
      "Epoch 59 Batch 0 Loss 0.2582\n",
      "Epoch 59 Batch 8 Loss 0.1245\n",
      "Epoch 59 Batch 16 Loss 0.1480\n",
      "Epoch 59 Batch 24 Loss 0.1349\n",
      "Epoch 59 Batch 32 Loss 0.1382\n",
      "Epoch 59 Batch 40 Loss 0.1383\n",
      "Epoch 59 Batch 48 Loss 0.1350\n",
      "Epoch 59 Batch 56 Loss 0.1308\n",
      "Epoch 59 Batch 64 Loss 0.1356\n",
      "Epoch 59 Batch 72 Loss 0.1317\n",
      "Epoch 59 Batch 80 Loss 0.1273\n",
      "Epoch 59 Batch 88 Loss 0.1275\n",
      "Epoch 59 Batch 96 Loss 0.1223\n",
      "Epoch 59 Batch 104 Loss 0.1251\n",
      "Epoch 59 Batch 112 Loss 0.1262\n",
      "Epoch 59 Loss 0.1255\n",
      "Time taken for 1 epoch: 2.817075490951538 secs\n",
      "\n",
      "Epoch 60 Batch 0 Loss 0.1422\n",
      "Epoch 60 Batch 8 Loss 0.0833\n",
      "Epoch 60 Batch 16 Loss 0.0859\n",
      "Epoch 60 Batch 24 Loss 0.1006\n",
      "Epoch 60 Batch 32 Loss 0.1100\n",
      "Epoch 60 Batch 40 Loss 0.1127\n",
      "Epoch 60 Batch 48 Loss 0.1086\n",
      "Epoch 60 Batch 56 Loss 0.1197\n",
      "Epoch 60 Batch 64 Loss 0.1142\n",
      "Epoch 60 Batch 72 Loss 0.1143\n",
      "Epoch 60 Batch 80 Loss 0.1123\n",
      "Epoch 60 Batch 88 Loss 0.1106\n",
      "Epoch 60 Batch 96 Loss 0.1152\n",
      "Epoch 60 Batch 104 Loss 0.1217\n",
      "Epoch 60 Batch 112 Loss 0.1204\n",
      "Saving checkpoint for epoch 60 at checkpoints/ckpt-12\n",
      "Epoch 60 Loss 0.1191\n",
      "Time taken for 1 epoch: 3.1335694789886475 secs\n",
      "\n",
      "Epoch 61 Batch 0 Loss 0.1442\n",
      "Epoch 61 Batch 8 Loss 0.1150\n",
      "Epoch 61 Batch 16 Loss 0.0886\n",
      "Epoch 61 Batch 24 Loss 0.0840\n",
      "Epoch 61 Batch 32 Loss 0.0880\n",
      "Epoch 61 Batch 40 Loss 0.0864\n",
      "Epoch 61 Batch 48 Loss 0.0936\n",
      "Epoch 61 Batch 56 Loss 0.0979\n",
      "Epoch 61 Batch 64 Loss 0.0948\n",
      "Epoch 61 Batch 72 Loss 0.0975\n",
      "Epoch 61 Batch 80 Loss 0.0978\n",
      "Epoch 61 Batch 88 Loss 0.1042\n",
      "Epoch 61 Batch 96 Loss 0.1051\n",
      "Epoch 61 Batch 104 Loss 0.1066\n",
      "Epoch 61 Batch 112 Loss 0.1080\n",
      "Epoch 61 Loss 0.1092\n",
      "Time taken for 1 epoch: 2.799372911453247 secs\n",
      "\n",
      "Epoch 62 Batch 0 Loss 0.0141\n",
      "Epoch 62 Batch 8 Loss 0.0936\n",
      "Epoch 62 Batch 16 Loss 0.0853\n",
      "Epoch 62 Batch 24 Loss 0.0899\n",
      "Epoch 62 Batch 32 Loss 0.0863\n",
      "Epoch 62 Batch 40 Loss 0.0817\n",
      "Epoch 62 Batch 48 Loss 0.0841\n",
      "Epoch 62 Batch 56 Loss 0.0840\n",
      "Epoch 62 Batch 64 Loss 0.0854\n",
      "Epoch 62 Batch 72 Loss 0.0889\n",
      "Epoch 62 Batch 80 Loss 0.0911\n",
      "Epoch 62 Batch 88 Loss 0.0931\n",
      "Epoch 62 Batch 96 Loss 0.0941\n",
      "Epoch 62 Batch 104 Loss 0.0971\n",
      "Epoch 62 Batch 112 Loss 0.1004\n",
      "Epoch 62 Loss 0.1006\n",
      "Time taken for 1 epoch: 2.8059303760528564 secs\n",
      "\n",
      "Epoch 63 Batch 0 Loss 0.0310\n",
      "Epoch 63 Batch 8 Loss 0.0772\n",
      "Epoch 63 Batch 16 Loss 0.0984\n",
      "Epoch 63 Batch 24 Loss 0.1039\n",
      "Epoch 63 Batch 32 Loss 0.1033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63 Batch 40 Loss 0.1104\n",
      "Epoch 63 Batch 48 Loss 0.1118\n",
      "Epoch 63 Batch 56 Loss 0.1104\n",
      "Epoch 63 Batch 64 Loss 0.1095\n",
      "Epoch 63 Batch 72 Loss 0.1103\n",
      "Epoch 63 Batch 80 Loss 0.1101\n",
      "Epoch 63 Batch 88 Loss 0.1091\n",
      "Epoch 63 Batch 96 Loss 0.1107\n",
      "Epoch 63 Batch 104 Loss 0.1106\n",
      "Epoch 63 Batch 112 Loss 0.1133\n",
      "Epoch 63 Loss 0.1123\n",
      "Time taken for 1 epoch: 2.808931827545166 secs\n",
      "\n",
      "Epoch 64 Batch 0 Loss 0.0080\n",
      "Epoch 64 Batch 8 Loss 0.0525\n",
      "Epoch 64 Batch 16 Loss 0.0769\n",
      "Epoch 64 Batch 24 Loss 0.0965\n",
      "Epoch 64 Batch 32 Loss 0.0979\n",
      "Epoch 64 Batch 40 Loss 0.1022\n",
      "Epoch 64 Batch 48 Loss 0.0985\n",
      "Epoch 64 Batch 56 Loss 0.0978\n",
      "Epoch 64 Batch 64 Loss 0.1051\n",
      "Epoch 64 Batch 72 Loss 0.1116\n",
      "Epoch 64 Batch 80 Loss 0.1128\n",
      "Epoch 64 Batch 88 Loss 0.1130\n",
      "Epoch 64 Batch 96 Loss 0.1114\n",
      "Epoch 64 Batch 104 Loss 0.1134\n",
      "Epoch 64 Batch 112 Loss 0.1131\n",
      "Epoch 64 Loss 0.1123\n",
      "Time taken for 1 epoch: 2.8217320442199707 secs\n",
      "\n",
      "Epoch 65 Batch 0 Loss 0.0716\n",
      "Epoch 65 Batch 8 Loss 0.0680\n",
      "Epoch 65 Batch 16 Loss 0.0668\n",
      "Epoch 65 Batch 24 Loss 0.0733\n",
      "Epoch 65 Batch 32 Loss 0.0678\n",
      "Epoch 65 Batch 40 Loss 0.0726\n",
      "Epoch 65 Batch 48 Loss 0.0717\n",
      "Epoch 65 Batch 56 Loss 0.0763\n",
      "Epoch 65 Batch 64 Loss 0.0822\n",
      "Epoch 65 Batch 72 Loss 0.0830\n",
      "Epoch 65 Batch 80 Loss 0.0824\n",
      "Epoch 65 Batch 88 Loss 0.0861\n",
      "Epoch 65 Batch 96 Loss 0.0901\n",
      "Epoch 65 Batch 104 Loss 0.0936\n",
      "Epoch 65 Batch 112 Loss 0.0941\n",
      "Saving checkpoint for epoch 65 at checkpoints/ckpt-13\n",
      "Epoch 65 Loss 0.0970\n",
      "Time taken for 1 epoch: 3.1214232444763184 secs\n",
      "\n",
      "Epoch 66 Batch 0 Loss 0.0310\n",
      "Epoch 66 Batch 8 Loss 0.1406\n",
      "Epoch 66 Batch 16 Loss 0.1094\n",
      "Epoch 66 Batch 24 Loss 0.1258\n",
      "Epoch 66 Batch 32 Loss 0.1165\n",
      "Epoch 66 Batch 40 Loss 0.1100\n",
      "Epoch 66 Batch 48 Loss 0.1082\n",
      "Epoch 66 Batch 56 Loss 0.1116\n",
      "Epoch 66 Batch 64 Loss 0.1119\n",
      "Epoch 66 Batch 72 Loss 0.1097\n",
      "Epoch 66 Batch 80 Loss 0.1123\n",
      "Epoch 66 Batch 88 Loss 0.1121\n",
      "Epoch 66 Batch 96 Loss 0.1131\n",
      "Epoch 66 Batch 104 Loss 0.1131\n",
      "Epoch 66 Batch 112 Loss 0.1176\n",
      "Epoch 66 Loss 0.1171\n",
      "Time taken for 1 epoch: 2.7979938983917236 secs\n",
      "\n",
      "Epoch 67 Batch 0 Loss 0.0764\n",
      "Epoch 67 Batch 8 Loss 0.0695\n",
      "Epoch 67 Batch 16 Loss 0.0860\n",
      "Epoch 67 Batch 24 Loss 0.0836\n",
      "Epoch 67 Batch 32 Loss 0.0787\n",
      "Epoch 67 Batch 40 Loss 0.0788\n",
      "Epoch 67 Batch 48 Loss 0.0865\n",
      "Epoch 67 Batch 56 Loss 0.0882\n",
      "Epoch 67 Batch 64 Loss 0.0845\n",
      "Epoch 67 Batch 72 Loss 0.0886\n",
      "Epoch 67 Batch 80 Loss 0.0901\n",
      "Epoch 67 Batch 88 Loss 0.0891\n",
      "Epoch 67 Batch 96 Loss 0.0906\n",
      "Epoch 67 Batch 104 Loss 0.0960\n",
      "Epoch 67 Batch 112 Loss 0.0960\n",
      "Epoch 67 Loss 0.0969\n",
      "Time taken for 1 epoch: 2.7958333492279053 secs\n",
      "\n",
      "Epoch 68 Batch 0 Loss 0.1958\n",
      "Epoch 68 Batch 8 Loss 0.0899\n",
      "Epoch 68 Batch 16 Loss 0.1037\n",
      "Epoch 68 Batch 24 Loss 0.1059\n",
      "Epoch 68 Batch 32 Loss 0.1049\n",
      "Epoch 68 Batch 40 Loss 0.1000\n",
      "Epoch 68 Batch 48 Loss 0.0934\n",
      "Epoch 68 Batch 56 Loss 0.0966\n",
      "Epoch 68 Batch 64 Loss 0.0969\n",
      "Epoch 68 Batch 72 Loss 0.0953\n",
      "Epoch 68 Batch 80 Loss 0.0962\n",
      "Epoch 68 Batch 88 Loss 0.0955\n",
      "Epoch 68 Batch 96 Loss 0.0924\n",
      "Epoch 68 Batch 104 Loss 0.0905\n",
      "Epoch 68 Batch 112 Loss 0.0912\n",
      "Epoch 68 Loss 0.0927\n",
      "Time taken for 1 epoch: 2.8085896968841553 secs\n",
      "\n",
      "Epoch 69 Batch 0 Loss 0.0315\n",
      "Epoch 69 Batch 8 Loss 0.0663\n",
      "Epoch 69 Batch 16 Loss 0.0680\n",
      "Epoch 69 Batch 24 Loss 0.0706\n",
      "Epoch 69 Batch 32 Loss 0.0773\n",
      "Epoch 69 Batch 40 Loss 0.0705\n",
      "Epoch 69 Batch 48 Loss 0.0709\n",
      "Epoch 69 Batch 56 Loss 0.0729\n",
      "Epoch 69 Batch 64 Loss 0.0724\n",
      "Epoch 69 Batch 72 Loss 0.0756\n",
      "Epoch 69 Batch 80 Loss 0.0770\n",
      "Epoch 69 Batch 88 Loss 0.0765\n",
      "Epoch 69 Batch 96 Loss 0.0771\n",
      "Epoch 69 Batch 104 Loss 0.0798\n",
      "Epoch 69 Batch 112 Loss 0.0801\n",
      "Epoch 69 Loss 0.0799\n",
      "Time taken for 1 epoch: 2.7967798709869385 secs\n",
      "\n",
      "Epoch 70 Batch 0 Loss 0.0080\n",
      "Epoch 70 Batch 8 Loss 0.0694\n",
      "Epoch 70 Batch 16 Loss 0.0657\n",
      "Epoch 70 Batch 24 Loss 0.0726\n",
      "Epoch 70 Batch 32 Loss 0.0716\n",
      "Epoch 70 Batch 40 Loss 0.0768\n",
      "Epoch 70 Batch 48 Loss 0.0871\n",
      "Epoch 70 Batch 56 Loss 0.0822\n",
      "Epoch 70 Batch 64 Loss 0.0843\n",
      "Epoch 70 Batch 72 Loss 0.0882\n",
      "Epoch 70 Batch 80 Loss 0.0959\n",
      "Epoch 70 Batch 88 Loss 0.0986\n",
      "Epoch 70 Batch 96 Loss 0.0981\n",
      "Epoch 70 Batch 104 Loss 0.0977\n",
      "Epoch 70 Batch 112 Loss 0.1027\n",
      "Saving checkpoint for epoch 70 at checkpoints/ckpt-14\n",
      "Epoch 70 Loss 0.1024\n",
      "Time taken for 1 epoch: 3.148777484893799 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "  \n",
    "    for (batch, (inp, tar)) in enumerate(dataset):\n",
    "        train_step(inp, tar)\n",
    "    \n",
    "        # 55k samples\n",
    "        # we display 3 batch results -- 0th, middle and last one (approx)\n",
    "        # 55k / 64 ~ 858; 858 / 2 = 429\n",
    "        if batch % 8 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, train_loss.result()))\n",
    "      \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
    "    \n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch + 1, train_loss.result()))\n",
    "\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "5MmWB9wpOp6x"
   },
   "outputs": [],
   "source": [
    "def evaluate(input_document):\n",
    "    input_document = document_tokenizer.texts_to_sequences([input_document])\n",
    "    input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
    "\n",
    "    encoder_input = tf.expand_dims(input_document[0], 0)\n",
    "\n",
    "    decoder_input = [summary_tokenizer.word_index[\"<go>\"]]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "    \n",
    "    for i in range(decoder_maxlen):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
    "\n",
    "        predictions, attention_weights = transformer(\n",
    "            encoder_input, \n",
    "            output,\n",
    "            False,\n",
    "            enc_padding_mask,\n",
    "            combined_mask,\n",
    "            dec_padding_mask\n",
    "        )\n",
    "\n",
    "        predictions = predictions[: ,-1:, :]\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        if predicted_id == summary_tokenizer.word_index[\"<stop>\"]:\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "g308MId-PCmh"
   },
   "outputs": [],
   "source": [
    "def summarize(input_document):\n",
    "    summarized = evaluate(input_document=input_document)[0].numpy()\n",
    "    summarized = np.expand_dims(summarized[1:], 0)  # not printing <go> token\n",
    "    return summary_tokenizer.sequences_to_texts(summarized)[0]  # since there is just one translated document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "pD6a2l8YPFge",
    "outputId": "731e3cab-9852-4a62-8556-0985611fb1df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ভালো ছিল সেই সোনালি অতীতটা।'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(\"মানুষের মুখ খুব শ ক্তিশালী এক জিনিস। মানুষ যেটা বলে সেটার একটা প্রভাব আছে।অভিজ্ঞতা থেকে দেখেছি একটা ছেলেকে ক্ষেপানোর জন্য বলা হতো, অমুক মেয়ের সাথে তুই প্রেম করিস। কদিন পরে সত্যি সত্যি তারা প্রেম করা শুরু করে দিয়েছিল।স্বামী-স্ত্রীর মনোমালিন্যর সময় হয়ত স্ত্রী আফসোস করে বলল, আমি পুরোনো হয়ে গেছি - এখন তো আর আমাকে ভালো লাগবে না।সত্যি সত্যি দেখা যাবে কদিন পরে স্বামীর ঠিক ওই জিনিসটাই মনে হতে থাকবে। অথচ হয়ত সে এ ব্যাপারে আগে ভাবেইনি।একটা ছেলেকে পরিবারের সবাই বলে, তুই কোনো কাজের না - দেখা যাবে ছেলেটা আসলেই কিছু করতে পারছে না।এজন্য রসুল সাল্লাল্লাহু আলাইহি ওয়া সাল্লাম বলেছেন, হয় ভালো কথা বলো নয়ত চুপ থাক।আমাদের জীবনের বহু ভালো পরিস্থিতি খারাপ থেকে খারাপ হয়েছে শুধুমাত্র আমাদের কথার কারণে।জিহবা সাবধান ভাইয়েরা। মুখ সাবধান বোনেরা।রসুল সাল্লাল্লাহু আলাইহি ওয়া সাল্লামের কথাটাকে দাম দিই - সংসারে শান্তি আসবে, আয়ে বারাকাহ আসবে।\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "ERj84JjDP_x-",
    "outputId": "20c03aa5-5a61-48b9-f80d-870fba81b61b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'সমাবর্তনের মাধ্যমে শিক্ষা জীবনের শেষ হলেও সফলতা ও ব্যর্থজীবনের হিসাব গণনা শুরু হয়ে এখান থেকেই'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(\"ক্লাসে সবচেয়ে দূর্বল ছেলেটি কাল সমাবর্তনে এসেছিল সবার চেয়ে হাই পজিশনের জব নিয়ে। বারবার প্রেমে ব্যর্থ হওয়া মেয়েটি এসেছিল একটি সুন্দর ছোট্ট পরিবার নিয়ে। কারো কাছে পাত্তা না পাওয়া, তোকে দিয়ে কিছু হবে না বলা ছেলেটিই সবচেয়ে সুন্দর বউ নিয়ে এসেছে। পড়াশোনার খরচ যোগাতে টিউশন করে হাত খরচ চালানো মেয়েটি কাল গাড়ি দিয়ে ক্যাম্পাসে এসেছিল। ক্লাসের সবচেয়ে সাক্সেস্ফুল ছেলেটি ডিপ্রেশনে ভুগছে জব না পাওয়ায়। ডিপার্টমেন্টের হার্টথ্রোব মেয়েটির চোখে নিচে কালি বিয়ে হচ্ছে না বয়স হয়ে গেছে।এভাবেই সময়ের সাথে বদলে যায় মানুষের জীবনে ইকুয়েশন। আসলে সমাবর্তনের মাধ্যমে শিক্ষা জীবনের শেষ হলেও সফলতা ও ব্যর্থজীবনের হিসাব গণনা শুরু হয়ে এখান থেকেই।তাই ঘৃণা, হিংসা, কম্পিটিশন বাদ দিয়ে জীবনটাকে বাচা উচিত সম্পূর্ণ স্বাদ ও ভালবাসা নিয়ে। কখন জীবনের কোন মোড় দেখায় কোন নিশ্চয়তা নেই, তাই কোন মুহূর্তের জন্য যাতে আফসোস না থাকে।\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "5YKHMymdSugA",
    "outputId": "768ed7ca-892e-44a9-8cce-7e4f47e50c30"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'লেখক হতেে হলে হতে হবে যোগ্যতাসম্পন্ন লেখক'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(\"ইতালির প্রধানমন্ত্রী জুসেপ্পে কন্তে বলেছেন, একেকজন বাংলাদেশি একেকটা ভাইরাস বোমা। অনেক বাংলাদেশি ভাই সেটাকে শেয়ার করে দেশকে পরোক্ষভাবে তিরস্কার করছেন। অথচ কন্তে সাহেবকে বলা দরকার, আমাদের অসচেতনতা নিয়ে এমন ঢালাও মন্তব্য করার আগে আপনার অগ্রজ শাসকদের দিকে তাকান। নিরো সাহেবের দিকে তাকান।আপনি কী জানেন না? রোম যখন পুড়ছিলো, নিরো তখন সাহেব বাঁশি বাজাচ্ছিলেন।আর আমরা একটু বাঁশি বাজালেই দোষ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "Swe6Dl93DEfY",
    "outputId": "58a3bd29-90d1-4da8-fc26-16679247d065"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'বইয়ের সাদা পাতার আবেগ গুলো খুব প্রাণময়'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(\"মুক্তির সারথি বাংলাদেশ সাধারণ ছাত্র অধিকার সংরক্ষণ পরিষদ আমাদের দেশে স্বাধীনতার পর থেকে চলমান অসুস্থ ধারার রাজনীতি এখনো বহাল তবিয়তে চলছে। এই অবস্থা থেকে বের হয়ে আসতে না পারলে দেশ ও জাতি আরো গভীর অন্ধকারে নিমজ্জিত হবে। একটি দেশের প্রকৃত উন্নয়ন নির্ভর করে সেদেশের রাজনৈতিক স্থিতিশীলতার উপর। দেশে দৃশ্যমান উন্নয়ন অনেক কিন্তু প্রকৃত উন্নয়ন কতটা তা যতেষ্ঠ প্রশ্নের মুখোমুখি আজ। উল্টো দেশের স্তম্ভ গুলো দিনকে দিন দুর্বল থেকে দুর্বলতর করা হচ্ছে। আইন বিচার এবং শাসন বিভাগের অবস্থা বড্ড নাজুক। এখন অনেক সময় দেখি মহান জাতীয় সংসদ কোরাম সংকটে ভুগে। সাংবিধানিক প্রতিষ্ঠান গুলো তাদের স্বকীয়তা হারাচ্ছে অনবরত। বাংলাদেশ নির্বাচন কমিশন, দুর্নীতি দমন কমিশন সহ সাংবিধানিক প্রতিষ্ঠানগুলোকে এখন আর কার্যকর তেমন কোন পদক্ষেপ নিতে দেখি না আর আমরা।রাষ্ট্রের চতুর্থ স্তম্ভ গণমাধ্যম, এই গণমাধ্যমের অবস্থা যে খুব একটা ভাল তাও কিন্তু নয়। তবুও বলবো সব মিলিয়ে এগিয়ে যাচ্ছে প্রিয় স্বদেশ। আগামীতে গণমানুষের প্রত্যাশা পূরণে কাজ করে যাবে ছাত্রসমাজের প্রাণের স্পন্দন বাংলাদেশ সাধারণ ছাত্র অধিকার সংরক্ষণ পরিষদ দেশ ব্যাপি কমিটি হালনাগাদের কার্যক্রম চলমান রয়েছে, আপনি আছেন তো আপনার জেলার কমিটিতে....??যুক্ত না থাকলে এখনি সময় যুক্ত হওয়ার। আপনাদের হাত ধরেই পরিবর্তন আসবে ইনশাআল্লাহ।।\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HUvYIrLxWSAZ",
    "outputId": "43852f7b-7857-4a1a-87ca-cd867e5c1dec"
   },
   "outputs": [],
   "source": [
    "# !pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "qL_JfyFyXgEP"
   },
   "outputs": [],
   "source": [
    "from rouge import Rouge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NSoXj0WkaQw-",
    "outputId": "1741e374-62f1-47bd-93ad-4263e18e88e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'rouge-1': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-2': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}, 'rouge-l': {'r': 1.0, 'p': 1.0, 'f': 0.999999995}}]\n"
     ]
    }
   ],
   "source": [
    "hypothesis = \"স্বাধীনতার পর থেকে চলমান অসুস্থ ধারার রাজনীতি এখনো বহাল।\"\n",
    "reference = \"স্বাধীনতার পর থেকে চলমান অসুস্থ ধারার রাজনীতি এখনো বহাল।\"\n",
    "\n",
    "\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hypothesis, reference)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96B3LVoV0DjP",
    "outputId": "ed80f4fe-77a1-478b-db23-51400d6b08ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [01:28<00:00,  1.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.48665166764754936"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BLEU score on test set\n",
    "import nltk\n",
    "blu = []\n",
    "for i in tqdm.tqdm(range(len(test_document))):\n",
    "    summery_1 = summarize(test_document.values[i])\n",
    "    BLEUscore = nltk.translate.bleu_score.corpus_bleu([test_summary.values[i]], [summery_1], weights = [1])\n",
    "    blu.append(BLEUscore)\n",
    "# print(BLEUscore)\n",
    "np.average(blu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 921/921 [16:25<00:00,  1.07s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.47423674344539163"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BLEU score on train set\n",
    "\n",
    "import nltk\n",
    "blu = []\n",
    "for i in tqdm.tqdm(range(len(document))):\n",
    "    summery_1 = summarize(document.values[i])\n",
    "    BLEUscore = nltk.translate.bleu_score.corpus_bleu([summary.values[i]], [summery_1], weights = [1])\n",
    "    blu.append(BLEUscore)\n",
    "np.average(blu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [01:32<00:00,  1.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48.14705882352941"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Levenshtein score on testß set\n",
    "\n",
    "\n",
    "import Levenshtein\n",
    "lv = []\n",
    "for i in tqdm.tqdm(range(len(test_document))):\n",
    "    summery_1 = summarize(test_document.values[i])\n",
    "    summery_1 = \" \".join(summery_1.split())\n",
    "    summery_2 = summary.values[i]\n",
    "    summery_2 = \" \".join(summery_2.split())\n",
    "                         \n",
    "    lvscore =  Levenshtein.distance(summery_2, summery_1)\n",
    "    lv.append(lvscore)\n",
    "np.average(lv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python [conda env:.conda-tf]",
   "language": "python",
   "name": "conda-env-.conda-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
